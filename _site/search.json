[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "ADA Bayern\n\n\n\n\n\nIn der öffentlichen Verwaltung liegen vielfältige Daten vor, die potentiell genutzt werden können um Fragen zu beantworten und Probleme zu lösen. Wir können Sie unterstützen dieses Potential zu entfachen.\nIn Workshop-Serien, bei denen wir in Teamarbeit voneinander lernen und Lösungen entwickeln. Denn Data Science ist ein Teamsport, der nicht ohne Sie als Inhaltsexpert:innen funktioniert. Wir stellen eine sichere Cloud-Umgebung bereit, damit wir gemeinsam und effektiv mit den Daten arbeiten können. Dazu nutzen wir Open Source Software und stellen die entstandenen Produkte und Materialien unter offenen Lizenzen zur Verfügung.\n\n\n\n\n\nIn der Praxis müssen von der Idee bis zur Umsetzung von Datenprojekten viele Hürden genommen werden. Unser Ziel ist, dass Daten in der öffentlichen Verwaltung effektiver genutzt werden. Wir wollen\n\nBeschäftigte im öffentlichen Sektor zum (besseren) Umgang mit Daten befähigen,\nDatenkompetenz stärken,\nund so zur Umsetzung von sinnvollen Datenprojekten beitragen.\n\nAnhand von konkreten Beispielen aus der Verwaltungspraxis möchten wir Digitalisierung greifbar und interaktiv erlebbar machen.\nHaben Sie ein Problem, das Sie mich uns gemeinsam lösen möchten? Kontaktieren Sie uns!"
  },
  {
    "objectID": "index.html#ada-bayern",
    "href": "index.html#ada-bayern",
    "title": "ADA Bayern",
    "section": "",
    "text": "In der öffentlichen Verwaltung liegen vielfältige Daten vor, die potentiell genutzt werden können um Fragen zu beantworten und Probleme zu lösen. Wir können Sie unterstützen dieses Potential zu entfachen.\nIn Workshop-Serien, bei denen wir in Teamarbeit voneinander lernen und Lösungen entwickeln. Denn Data Science ist ein Teamsport, der nicht ohne Sie als Inhaltsexpert:innen funktioniert. Wir stellen eine sichere Cloud-Umgebung bereit, damit wir gemeinsam und effektiv mit den Daten arbeiten können. Dazu nutzen wir Open Source Software und stellen die entstandenen Produkte und Materialien unter offenen Lizenzen zur Verfügung.\n\n\n\n\n\nIn der Praxis müssen von der Idee bis zur Umsetzung von Datenprojekten viele Hürden genommen werden. Unser Ziel ist, dass Daten in der öffentlichen Verwaltung effektiver genutzt werden. Wir wollen\n\nBeschäftigte im öffentlichen Sektor zum (besseren) Umgang mit Daten befähigen,\nDatenkompetenz stärken,\nund so zur Umsetzung von sinnvollen Datenprojekten beitragen.\n\nAnhand von konkreten Beispielen aus der Verwaltungspraxis möchten wir Digitalisierung greifbar und interaktiv erlebbar machen.\nHaben Sie ein Problem, das Sie mich uns gemeinsam lösen möchten? Kontaktieren Sie uns!"
  },
  {
    "objectID": "impressum.html",
    "href": "impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "Angaben gemäß § 5 TMG"
  },
  {
    "objectID": "impressum.html#kontakt",
    "href": "impressum.html#kontakt",
    "title": "Impressum",
    "section": "Kontakt",
    "text": "Kontakt\nFrauke Kreuter  Institut für Statistik Ludwigstr. 33 80539 München\nTelefon: +49 89 2180 2814 E-Mail: data-analytics@stat.uni-muenchen.de"
  },
  {
    "objectID": "impressum.html#haftungsausschluss",
    "href": "impressum.html#haftungsausschluss",
    "title": "Impressum",
    "section": "Haftungsausschluss",
    "text": "Haftungsausschluss\n\nHaftung für Links\nUnser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf mögliche Rechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.\n\n\nUrheberrecht\nDie durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.\n\n\nDatenschutz\nDie Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten möglich. Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift oder eMail-Adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger Basis. Diese Daten werden ohne Ihre ausdrückliche Zustimmung nicht an Dritte weitergegeben. Wir weisen darauf hin, dass die Datenübertragung im Internet (z.B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich. Der Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten durch Dritte zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit ausdrücklich widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-Mails, vor.\nWebsite Impressum erstellt durch impressum-generator.de von der Kanzlei Hasselbach"
  },
  {
    "objectID": "kontakt.html",
    "href": "kontakt.html",
    "title": "Kontakt",
    "section": "",
    "text": "Sie erreichen uns unter data-analytics@stat.uni-muenchen.de.\nGerne sprechen wir mit Ihnen auch über zukünftige Workshops."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Wir sind ein Team aus Data Scientists und Statistiker:innen. Unser Ziel ist es, dazu beizutragen, dass Daten in der öffentlichen Verwaltung effektiver genutzt werden.\nWir wollen Beschäftigte im öffentlichen Sektor zum (besseren) Umgang mit Daten befähigen, die Datenkompetenz stärken, und so zur verstärkten Umsetzung von Datenprojekten beitragen. Anhand von konkreten Beispielen aus der Verwaltungspraxis möchten wir Digitalisierung greifbar und interaktiv erlebbar machen."
  },
  {
    "objectID": "team.html#prof.-dr.-frauke-kreuter",
    "href": "team.html#prof.-dr.-frauke-kreuter",
    "title": "Team",
    "section": "Prof. Dr. Frauke Kreuter",
    "text": "Prof. Dr. Frauke Kreuter\n\n\n\nFrauke Kreuter ist Inhaberin des Lehrstuhls für Statistik und Datenwissenschaft an der LMU München, und an der University of Maryland, USA, ist sie Co-Direktorin des Social Data Science Center (SoDa) und Fakultätsmitglied im Joint Program in Survey Methodology (JPSM). Sie ist eine gewählte Fellow der American Statistical Association und erhielt 2020 den Warren Mitofsky Innovators Award der American Association for Public Opinion Research. Neben ihrer akademischen Arbeit ist Frauke Kreuter Gründerin des International Program for Survey and Data Science (IPSDS), das als Reaktion auf die steigende Nachfrage von Forscher:innen und Parktiker:innen nach geeigneten Methoden und Instrumenten für ein sich veränderndes Datenumfeld entwickelt wurde. Sie ist außerdem Mitbegründerin der Coleridge Initiative, deren Ziel es ist, datengestützte Forschung und Politik rund um den Menschen und seine Interaktionen für Programmmanagement, Politikentwicklung und wissenschaftliche Zwecke zu beschleunigen, indem ein effizienter, effektiver und sicherer Zugang zu sensiblen Daten über Gesellschaft und Wirtschaft ermöglicht wird; und Mitbegründerin des deutschsprachigen Podcasts Dig Deep."
  },
  {
    "objectID": "WS3.html",
    "href": "WS3.html",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "21.03. - 30.04.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides\n\n\n\n\n\n\n\nBitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "WS3.html#ablauf",
    "href": "WS3.html#ablauf",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides",
    "crumbs": [
      "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "WS3.html#anmeldung",
    "href": "WS3.html#anmeldung",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Bitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "WS1.html",
    "href": "WS1.html",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "19.09. - 13.10.2023\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\nHier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "WS1.html#ablauf",
    "href": "WS1.html#ablauf",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "In dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen."
  },
  {
    "objectID": "WS1.html#anmeldung",
    "href": "WS1.html#anmeldung",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Hier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "WS4.html",
    "href": "WS4.html",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "11.10. - 08.11.2024\nIn dieser Workshop-Serie beschäftigen wir uns mit dem Thema Einzelbaumerkennung und insbesondere mit der Frage, wie Infrastruktur für die Datenanalyse aussehen könnte.\n\n\nBitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen.",
    "crumbs": [
      "Workshop Serie 4: Einzelbaumerkennung"
    ]
  },
  {
    "objectID": "WS4.html#anmeldung",
    "href": "WS4.html#anmeldung",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "Bitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 4: Einzelbaumerkennung"
    ]
  },
  {
    "objectID": "WS4.html#ablauf",
    "href": "WS4.html#ablauf",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen.",
    "crumbs": [
      "Workshop Serie 4: Einzelbaumerkennung"
    ]
  },
  {
    "objectID": "WS2.html",
    "href": "WS2.html",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "08.02. - 13.03.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt\n\n\n\n\n\n\nSie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "WS2.html#ablauf",
    "href": "WS2.html#ablauf",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "crumbs": [
      "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "WS2.html#anmeldung",
    "href": "WS2.html#anmeldung",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "Sie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "WS4_Modul_1.html",
    "href": "WS4_Modul_1.html",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "WS4_Modul_1.html#themen",
    "href": "WS4_Modul_1.html#themen",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "WS4_Modul_1.html#material",
    "href": "WS4_Modul_1.html#material",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "WS4_Modul_2.html",
    "href": "WS4_Modul_2.html",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte\n\n\n\n\nSlides",
    "crumbs": [
      "Tage",
      "2. Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "WS4_Modul_2.html#themen",
    "href": "WS4_Modul_2.html#themen",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte",
    "crumbs": [
      "Tage",
      "2. Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "WS4_Modul_2.html#material",
    "href": "WS4_Modul_2.html#material",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Tage",
      "2. Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "WS4_Modul_3.html",
    "href": "WS4_Modul_3.html",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "WS4_Modul_3.html#themen",
    "href": "WS4_Modul_3.html#themen",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "WS4_Modul_3.html#material",
    "href": "WS4_Modul_3.html#material",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "WS2 copy.html",
    "href": "WS2 copy.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "08.02. - 13.03.2024"
  },
  {
    "objectID": "WS2 copy.html#workshop-serie-2-implementation-daten-basierter-archivierung-von-gerichtsakten",
    "href": "WS2 copy.html#workshop-serie-2-implementation-daten-basierter-archivierung-von-gerichtsakten",
    "title": "ADA Bayern",
    "section": "",
    "text": "08.02. - 13.03.2024"
  },
  {
    "objectID": "WS2 copy.html#ablauf",
    "href": "WS2 copy.html#ablauf",
    "title": "ADA Bayern",
    "section": "Ablauf",
    "text": "Ablauf\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\nOrte\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\nInhalte\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\nKick-Off\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\nModule\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt"
  },
  {
    "objectID": "WS2 copy.html#anmeldung",
    "href": "WS2 copy.html#anmeldung",
    "title": "ADA Bayern",
    "section": "Anmeldung",
    "text": "Anmeldung\nSie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de."
  },
  {
    "objectID": "WS2_Modul_4.html",
    "href": "WS2_Modul_4.html",
    "title": "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… einen nachhaltigen Prozess etablieren, wie die Daten übertragen, die Analysen durchgeführt und Archivierungsentscheidungen dokumentiert werden.\n… ihr Wissen an andere weitergeben.\n… einschätzen, wie die benötigte Infrastruktur erhalten werden kann.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt"
    ]
  },
  {
    "objectID": "WS2_Modul_4.html#material",
    "href": "WS2_Modul_4.html#material",
    "title": "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt"
    ]
  },
  {
    "objectID": "WS2_Modul_2.html",
    "href": "WS2_Modul_2.html",
    "title": "2. Umsetzung: Implementierung der Stichprobenziehung",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n… die Stichprobe analysieren und visuell darstellen.\n… Schwächen der Strategie einschätzen und verteidigen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "2. Umsetzung: Implementierung der Stichprobenziehung"
    ]
  },
  {
    "objectID": "WS2_Modul_2.html#material",
    "href": "WS2_Modul_2.html#material",
    "title": "2. Umsetzung: Implementierung der Stichprobenziehung",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "2. Umsetzung: Implementierung der Stichprobenziehung"
    ]
  },
  {
    "objectID": "WS3_Modul_3.html",
    "href": "WS3_Modul_3.html",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "3. Visualisierung und Reporting"
    ]
  },
  {
    "objectID": "WS3_Modul_3.html#themen",
    "href": "WS3_Modul_3.html#themen",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.",
    "crumbs": [
      "Module",
      "3. Visualisierung und Reporting"
    ]
  },
  {
    "objectID": "WS3_Modul_3.html#material",
    "href": "WS3_Modul_3.html#material",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "3. Visualisierung und Reporting"
    ]
  },
  {
    "objectID": "WS3_Modul_1.html",
    "href": "WS3_Modul_1.html",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Problemerfassung und Daten als Lösung"
    ]
  },
  {
    "objectID": "WS3_Modul_1.html#themen",
    "href": "WS3_Modul_1.html#themen",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten",
    "crumbs": [
      "Module",
      "1. Problemerfassung und Daten als Lösung"
    ]
  },
  {
    "objectID": "WS3_Modul_1.html#material",
    "href": "WS3_Modul_1.html#material",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "1. Problemerfassung und Daten als Lösung"
    ]
  },
  {
    "objectID": "WS1 copy.html",
    "href": "WS1 copy.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "19.09. - 13.10.2023"
  },
  {
    "objectID": "WS1 copy.html#workshop-serie-1-daten-basierte-archivierung-von-gerichtsakten",
    "href": "WS1 copy.html#workshop-serie-1-daten-basierte-archivierung-von-gerichtsakten",
    "title": "ADA Bayern",
    "section": "",
    "text": "19.09. - 13.10.2023"
  },
  {
    "objectID": "WS1 copy.html#ablauf",
    "href": "WS1 copy.html#ablauf",
    "title": "ADA Bayern",
    "section": "Ablauf",
    "text": "Ablauf\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen."
  },
  {
    "objectID": "WS1 copy.html#anmeldung",
    "href": "WS1 copy.html#anmeldung",
    "title": "ADA Bayern",
    "section": "Anmeldung",
    "text": "Anmeldung\nHier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "WS3_Modul_2.html",
    "href": "WS3_Modul_2.html",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "In Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "2. Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "WS3_Modul_2.html#themen",
    "href": "WS3_Modul_2.html#themen",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "In Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.",
    "crumbs": [
      "Module",
      "2. Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "WS3_Modul_2.html#material",
    "href": "WS3_Modul_2.html#material",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "2. Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "WS2_Modul_3.html",
    "href": "WS2_Modul_3.html",
    "title": "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… die gewählte Strategie für Datensätze aus verschiedenen Jahren umsetzen.\n… Lücken erkennen, die im Projekt zu schließen sind, um die Analysen in das Tagesgeschäft aufzunehmen.\n… die Datenanalysen dokumentieren.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs"
    ]
  },
  {
    "objectID": "WS2_Modul_3.html#material",
    "href": "WS2_Modul_3.html#material",
    "title": "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs"
    ]
  },
  {
    "objectID": "WS2_Modul_1.html",
    "href": "WS2_Modul_1.html",
    "title": "1. Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… Kontext und Vorarbeiten des Projekts nennen.\n… verstehen, was das Ziel dieses Projekts ist.\n… die Datenanalyse als Teil eines größeren Kontexts beschreiben.\n… die einzelnen Schritte von der Vereinbarung der Datenübertragung bis hin zur Archivierung der einzelnen Akten beschreiben.\n… mit den relevanten Personen in Kontakt treten und einen reibungsarmen Ablauf ermöglichen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Arbeitsabläufe"
    ]
  },
  {
    "objectID": "WS2_Modul_1.html#material",
    "href": "WS2_Modul_1.html#material",
    "title": "1. Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Arbeitsabläufe"
    ]
  },
  {
    "objectID": "WS3 copy.html",
    "href": "WS3 copy.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "21.03. - 30.04.2024"
  },
  {
    "objectID": "WS3 copy.html#workshop-serie-3-daten-basierte-archivierung-von-gerichtsakten",
    "href": "WS3 copy.html#workshop-serie-3-daten-basierte-archivierung-von-gerichtsakten",
    "title": "ADA Bayern",
    "section": "",
    "text": "21.03. - 30.04.2024"
  },
  {
    "objectID": "WS3 copy.html#ablauf",
    "href": "WS3 copy.html#ablauf",
    "title": "ADA Bayern",
    "section": "Ablauf",
    "text": "Ablauf\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\nOrte\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\nInhalte\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\nKick-Off\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides"
  },
  {
    "objectID": "WS3 copy.html#anmeldung",
    "href": "WS3 copy.html#anmeldung",
    "title": "ADA Bayern",
    "section": "Anmeldung",
    "text": "Anmeldung\nBitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de."
  },
  {
    "objectID": "workshops_past/WS4_Modul_1.html",
    "href": "workshops_past/WS4_Modul_1.html",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS4_Modul_1.html#themen",
    "href": "workshops_past/WS4_Modul_1.html#themen",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur"
  },
  {
    "objectID": "workshops_past/WS4_Modul_1.html#material",
    "href": "workshops_past/WS4_Modul_1.html#material",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS4_Modul_2.html",
    "href": "workshops_past/WS4_Modul_2.html",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS4_Modul_2.html#themen",
    "href": "workshops_past/WS4_Modul_2.html#themen",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte"
  },
  {
    "objectID": "workshops_past/WS4_Modul_2.html#material",
    "href": "workshops_past/WS4_Modul_2.html#material",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_1.html",
    "href": "workshops_past/WS2_Modul_1.html",
    "title": "1. Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… Kontext und Vorarbeiten des Projekts nennen.\n… verstehen, was das Ziel dieses Projekts ist.\n… die Datenanalyse als Teil eines größeren Kontexts beschreiben.\n… die einzelnen Schritte von der Vereinbarung der Datenübertragung bis hin zur Archivierung der einzelnen Akten beschreiben.\n… mit den relevanten Personen in Kontakt treten und einen reibungsarmen Ablauf ermöglichen.\n\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_1.html#material",
    "href": "workshops_past/WS2_Modul_1.html#material",
    "title": "1. Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_3.html",
    "href": "workshops_past/WS2_Modul_3.html",
    "title": "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… die gewählte Strategie für Datensätze aus verschiedenen Jahren umsetzen.\n… Lücken erkennen, die im Projekt zu schließen sind, um die Analysen in das Tagesgeschäft aufzunehmen.\n… die Datenanalysen dokumentieren.\n\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_3.html#material",
    "href": "workshops_past/WS2_Modul_3.html#material",
    "title": "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS3.html",
    "href": "workshops_past/WS3.html",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "21.03. - 30.04.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides\n\n\n\n\n\n\n\nBitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de."
  },
  {
    "objectID": "workshops_past/WS3.html#ablauf",
    "href": "workshops_past/WS3.html#ablauf",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS3.html#anmeldung",
    "href": "workshops_past/WS3.html#anmeldung",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Bitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de."
  },
  {
    "objectID": "workshops_past/WS3_Modul_2.html",
    "href": "workshops_past/WS3_Modul_2.html",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "In Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS3_Modul_2.html#themen",
    "href": "workshops_past/WS3_Modul_2.html#themen",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "In Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen."
  },
  {
    "objectID": "workshops_past/WS3_Modul_2.html#material",
    "href": "workshops_past/WS3_Modul_2.html#material",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS3_Modul_1.html",
    "href": "workshops_past/WS3_Modul_1.html",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten\n\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS3_Modul_1.html#themen",
    "href": "workshops_past/WS3_Modul_1.html#themen",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten"
  },
  {
    "objectID": "workshops_past/WS3_Modul_1.html#material",
    "href": "workshops_past/WS3_Modul_1.html#material",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS4.html",
    "href": "workshops_past/WS4.html",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "11.10. - 08.11.2024\nIn dieser Workshop-Serie beschäftigen wir uns mit dem Thema Einzelbaumerkennung und insbesondere mit der Frage, wie Infrastruktur für die Datenanalyse aussehen könnte.\n\n\nBitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen."
  },
  {
    "objectID": "workshops_past/WS4.html#anmeldung",
    "href": "workshops_past/WS4.html#anmeldung",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "Bitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de."
  },
  {
    "objectID": "workshops_past/WS4.html#ablauf",
    "href": "workshops_past/WS4.html#ablauf",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen."
  },
  {
    "objectID": "workshops_past/WS1.html",
    "href": "workshops_past/WS1.html",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "19.09. - 13.10.2023\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\nHier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "workshops_past/WS1.html#ablauf",
    "href": "workshops_past/WS1.html#ablauf",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "In dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen."
  },
  {
    "objectID": "workshops_past/WS1.html#anmeldung",
    "href": "workshops_past/WS1.html#anmeldung",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Hier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "workshops_past/WS2.html",
    "href": "workshops_past/WS2.html",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "08.02. - 13.03.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt\n\n\n\n\n\n\nSie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de."
  },
  {
    "objectID": "workshops_past/WS2.html#ablauf",
    "href": "workshops_past/WS2.html#ablauf",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt"
  },
  {
    "objectID": "workshops_past/WS2.html#anmeldung",
    "href": "workshops_past/WS2.html#anmeldung",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "Sie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de."
  },
  {
    "objectID": "workshops_past/WS3_Modul_3.html",
    "href": "workshops_past/WS3_Modul_3.html",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.\n\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS3_Modul_3.html#themen",
    "href": "workshops_past/WS3_Modul_3.html#themen",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen."
  },
  {
    "objectID": "workshops_past/WS3_Modul_3.html#material",
    "href": "workshops_past/WS3_Modul_3.html#material",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_2.html",
    "href": "workshops_past/WS2_Modul_2.html",
    "title": "2. Umsetzung: Implementierung der Stichprobenziehung",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n… die Stichprobe analysieren und visuell darstellen.\n… Schwächen der Strategie einschätzen und verteidigen.\n\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_2.html#material",
    "href": "workshops_past/WS2_Modul_2.html#material",
    "title": "2. Umsetzung: Implementierung der Stichprobenziehung",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_4.html",
    "href": "workshops_past/WS2_Modul_4.html",
    "title": "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… einen nachhaltigen Prozess etablieren, wie die Daten übertragen, die Analysen durchgeführt und Archivierungsentscheidungen dokumentiert werden.\n… ihr Wissen an andere weitergeben.\n… einschätzen, wie die benötigte Infrastruktur erhalten werden kann.\n\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS2_Modul_4.html#material",
    "href": "workshops_past/WS2_Modul_4.html#material",
    "title": "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops_past/WS4_Modul_3.html",
    "href": "workshops_past/WS4_Modul_3.html",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops\n\n\n\n\nSlides"
  },
  {
    "objectID": "workshops_past/WS4_Modul_3.html#themen",
    "href": "workshops_past/WS4_Modul_3.html#themen",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops"
  },
  {
    "objectID": "workshops_past/WS4_Modul_3.html#material",
    "href": "workshops_past/WS4_Modul_3.html#material",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "workshops/WS4_Modul_1.html",
    "href": "workshops/WS4_Modul_1.html",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_1.html#themen",
    "href": "workshops/WS4_Modul_1.html#themen",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_1.html#material",
    "href": "workshops/WS4_Modul_1.html#material",
    "title": "1. Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_2.html",
    "href": "workshops/WS4_Modul_2.html",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "2. Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_2.html#themen",
    "href": "workshops/WS4_Modul_2.html#themen",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte",
    "crumbs": [
      "Module",
      "2. Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_2.html#material",
    "href": "workshops/WS4_Modul_2.html#material",
    "title": "2. Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "2. Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_1.html",
    "href": "workshops/WS2_Modul_1.html",
    "title": "1. Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… Kontext und Vorarbeiten des Projekts nennen.\n… verstehen, was das Ziel dieses Projekts ist.\n… die Datenanalyse als Teil eines größeren Kontexts beschreiben.\n… die einzelnen Schritte von der Vereinbarung der Datenübertragung bis hin zur Archivierung der einzelnen Akten beschreiben.\n… mit den relevanten Personen in Kontakt treten und einen reibungsarmen Ablauf ermöglichen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Arbeitsabläufe"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_1.html#material",
    "href": "workshops/WS2_Modul_1.html#material",
    "title": "1. Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Arbeitsabläufe"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_3.html",
    "href": "workshops/WS2_Modul_3.html",
    "title": "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… die gewählte Strategie für Datensätze aus verschiedenen Jahren umsetzen.\n… Lücken erkennen, die im Projekt zu schließen sind, um die Analysen in das Tagesgeschäft aufzunehmen.\n… die Datenanalysen dokumentieren.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_3.html#material",
    "href": "workshops/WS2_Modul_3.html#material",
    "title": "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "3. Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs"
    ]
  },
  {
    "objectID": "workshops/WS3.html",
    "href": "workshops/WS3.html",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "21.03. - 30.04.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides\n\n\n\n\n\n\n\nBitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "workshops/WS3.html#ablauf",
    "href": "workshops/WS3.html#ablauf",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides",
    "crumbs": [
      "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "workshops/WS3.html#anmeldung",
    "href": "workshops/WS3.html#anmeldung",
    "title": "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Bitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 3: Daten-basierte Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_2.html",
    "href": "workshops/WS3_Modul_2.html",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "In Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "2. Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_2.html#themen",
    "href": "workshops/WS3_Modul_2.html#themen",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "In Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.",
    "crumbs": [
      "Module",
      "2. Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_2.html#material",
    "href": "workshops/WS3_Modul_2.html#material",
    "title": "2. Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "2. Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_1.html",
    "href": "workshops/WS3_Modul_1.html",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Problemerfassung und Daten als Lösung"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_1.html#themen",
    "href": "workshops/WS3_Modul_1.html#themen",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten",
    "crumbs": [
      "Module",
      "1. Problemerfassung und Daten als Lösung"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_1.html#material",
    "href": "workshops/WS3_Modul_1.html#material",
    "title": "1. Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "1. Problemerfassung und Daten als Lösung"
    ]
  },
  {
    "objectID": "workshops/WS4.html",
    "href": "workshops/WS4.html",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "11.10. - 08.11.2024\nIn dieser Workshop-Serie beschäftigen wir uns mit dem Thema Einzelbaumerkennung und insbesondere mit der Frage, wie Infrastruktur für die Datenanalyse aussehen könnte.\n\n\nBitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen.",
    "crumbs": [
      "Workshop Serie 4: Einzelbaumerkennung"
    ]
  },
  {
    "objectID": "workshops/WS4.html#anmeldung",
    "href": "workshops/WS4.html#anmeldung",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "Bitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 4: Einzelbaumerkennung"
    ]
  },
  {
    "objectID": "workshops/WS4.html#ablauf",
    "href": "workshops/WS4.html#ablauf",
    "title": "Workshop Serie 4: Einzelbaumerkennung",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen.",
    "crumbs": [
      "Workshop Serie 4: Einzelbaumerkennung"
    ]
  },
  {
    "objectID": "workshops/WS1.html",
    "href": "workshops/WS1.html",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "19.09. - 13.10.2023\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\nHier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "workshops/WS1.html#ablauf",
    "href": "workshops/WS1.html#ablauf",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "In dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen."
  },
  {
    "objectID": "workshops/WS1.html#anmeldung",
    "href": "workshops/WS1.html#anmeldung",
    "title": "Workshop Serie 1: Daten-basierte Archivierung von Gerichtsakten",
    "section": "",
    "text": "Hier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "workshops/WS2.html",
    "href": "workshops/WS2.html",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "08.02. - 13.03.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt\n\n\n\n\n\n\nSie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "workshops/WS2.html#ablauf",
    "href": "workshops/WS2.html#ablauf",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "crumbs": [
      "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "workshops/WS2.html#anmeldung",
    "href": "workshops/WS2.html#anmeldung",
    "title": "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten",
    "section": "",
    "text": "Sie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 2: Implementation Daten-basierter Archivierung von Gerichtsakten"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_3.html",
    "href": "workshops/WS3_Modul_3.html",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "3. Visualisierung und Reporting"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_3.html#themen",
    "href": "workshops/WS3_Modul_3.html#themen",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.",
    "crumbs": [
      "Module",
      "3. Visualisierung und Reporting"
    ]
  },
  {
    "objectID": "workshops/WS3_Modul_3.html#material",
    "href": "workshops/WS3_Modul_3.html#material",
    "title": "3. Visualisierung und Reporting",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "3. Visualisierung und Reporting"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_2.html",
    "href": "workshops/WS2_Modul_2.html",
    "title": "2. Umsetzung: Implementierung der Stichprobenziehung",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n… die Stichprobe analysieren und visuell darstellen.\n… Schwächen der Strategie einschätzen und verteidigen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "2. Umsetzung: Implementierung der Stichprobenziehung"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_2.html#material",
    "href": "workshops/WS2_Modul_2.html#material",
    "title": "2. Umsetzung: Implementierung der Stichprobenziehung",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "2. Umsetzung: Implementierung der Stichprobenziehung"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_4.html",
    "href": "workshops/WS2_Modul_4.html",
    "title": "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… einen nachhaltigen Prozess etablieren, wie die Daten übertragen, die Analysen durchgeführt und Archivierungsentscheidungen dokumentiert werden.\n… ihr Wissen an andere weitergeben.\n… einschätzen, wie die benötigte Infrastruktur erhalten werden kann.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt"
    ]
  },
  {
    "objectID": "workshops/WS2_Modul_4.html#material",
    "href": "workshops/WS2_Modul_4.html#material",
    "title": "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "4. Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_3.html",
    "href": "workshops/WS4_Modul_3.html",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_3.html#themen",
    "href": "workshops/WS4_Modul_3.html#themen",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "workshops/WS4_Modul_3.html#material",
    "href": "workshops/WS4_Modul_3.html#material",
    "title": "3. Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_1.html",
    "href": "workshops/ws4/WS4_Modul_1.html",
    "title": "Modul 1: Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "Themen\nIn Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur\n\n\n\nMaterial\nSlides",
    "crumbs": [
      "Module",
      "Modul 1: Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_1.html#themen",
    "href": "workshops/ws4/WS4_Modul_1.html#themen",
    "title": "Modul 1: Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur",
    "crumbs": [
      "Module",
      "Modul 1: Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_1.html#material",
    "href": "workshops/ws4/WS4_Modul_1.html#material",
    "title": "Modul 1: Einführung, Problemerfassung und Daten",
    "section": "Material",
    "text": "Material\nSlides",
    "crumbs": [
      "Module",
      "Modul 1: Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_2.html",
    "href": "workshops/ws4/WS4_Modul_2.html",
    "title": "Modul 2: Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "Themen\nIn Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte\n\n\n\nMaterial\nSlides",
    "crumbs": [
      "Module",
      "Modul 2: Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_2.html#themen",
    "href": "workshops/ws4/WS4_Modul_2.html#themen",
    "title": "Modul 2: Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte",
    "crumbs": [
      "Module",
      "Modul 2: Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_2.html#material",
    "href": "workshops/ws4/WS4_Modul_2.html#material",
    "title": "Modul 2: Konzeption eines Datenprodukts, Infrastruktur und Software",
    "section": "Material",
    "text": "Material\nSlides",
    "crumbs": [
      "Module",
      "Modul 2: Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_3.html",
    "href": "workshops/ws4/WS4_Modul_3.html",
    "title": "Modul 3: Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "Themen\nIn Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops\n\n\n\nMaterial\nSlides",
    "crumbs": [
      "Module",
      "Modul 3: Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_3.html#themen",
    "href": "workshops/ws4/WS4_Modul_3.html#themen",
    "title": "Modul 3: Konzeption einer skalierbaren Cloud-Lösung",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_3.html#material",
    "href": "workshops/ws4/WS4_Modul_3.html#material",
    "title": "Modul 3: Konzeption einer skalierbaren Cloud-Lösung",
    "section": "Material",
    "text": "Material\nSlides",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2.html",
    "href": "workshops/ws2/WS2.html",
    "title": "Workshop Serie 2",
    "section": "",
    "text": "08.02. - 13.03.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "crumbs": [
      "Workshop Serie 2"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2.html#ablauf",
    "href": "workshops/ws2/WS2.html#ablauf",
    "title": "Workshop Serie 2",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "crumbs": [
      "Workshop Serie 2"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2.html#anmeldung",
    "href": "workshops/ws2/WS2.html#anmeldung",
    "title": "Workshop Serie 2",
    "section": "Anmeldung",
    "text": "Anmeldung\nSie haben Interesse an der Teilnahme in dieser Weiterbildiung? Bitte schreiben sie uns: data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 2"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_2.html",
    "href": "workshops/ws3/WS3_Modul_2.html",
    "title": "Modul 2: Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "Themen\nIn Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n\n\n\n\n\nMaterial\nSlides",
    "crumbs": [
      "Module",
      "Modul 2: Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_2.html#themen",
    "href": "workshops/ws3/WS3_Modul_2.html#themen",
    "title": "Workshop Serie 3, Modul 2",
    "section": "",
    "text": "In Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.",
    "crumbs": [
      "Module",
      "Workshop Serie 3, Modul 2"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_2.html#material",
    "href": "workshops/ws3/WS3_Modul_2.html#material",
    "title": "Workshop Serie 3, Modul 2",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "Workshop Serie 3, Modul 2"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3.html",
    "href": "workshops/ws3/WS3.html",
    "title": "Workshop Serie 3",
    "section": "",
    "text": "21.03. - 30.04.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides",
    "crumbs": [
      "Workshop Serie 3"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3.html#ablauf",
    "href": "workshops/ws3/WS3.html#ablauf",
    "title": "Workshop Serie 3",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides",
    "crumbs": [
      "Workshop Serie 3"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3.html#anmeldung",
    "href": "workshops/ws3/WS3.html#anmeldung",
    "title": "Workshop Serie 3",
    "section": "Anmeldung",
    "text": "Anmeldung\nBitte melden Sie sich über Pretix an.\nFragen gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 3"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_1.html",
    "href": "workshops/ws3/WS3_Modul_1.html",
    "title": "Modul 1: Problemerfassung und Daten als Lösung",
    "section": "",
    "text": "Themen\nIn Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten\n\n\n\n\n\nMaterial\nSlides",
    "crumbs": [
      "Module",
      "Modul 1: Problemerfassung und Daten als Lösung"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_1.html#themen",
    "href": "workshops/ws3/WS3_Modul_1.html#themen",
    "title": "Workshop Serie 3, Modul 1",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten zu verstehen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nDatenarbeit als Teamarbeit\nEvidenzbasierte Verwaltung\nBeantwortung von Fragen mit Daten\nDaten: Was ist das eigentlich?\nDatenquellen und Entstehung von Daten\nSicher mit Daten arbeiten: Cloud-Computing als Möglichkeit\n\n \n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… beschreiben, was Evidenzbasierte Verwaltung ist.\n… einschätzen, wann Evidenzbasierte Verwaltung genutzt werden kann.\n… einige Fragen benennen, die man mit Daten beantworten kann.\n… Probleme schildern, die in Ihrem Umfeld mit Daten gelöst werden könnten",
    "crumbs": [
      "Module",
      "Workshop Serie 3, Modul 1"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_1.html#material",
    "href": "workshops/ws3/WS3_Modul_1.html#material",
    "title": "Workshop Serie 3, Modul 1",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "Workshop Serie 3, Modul 1"
    ]
  },
  {
    "objectID": "workshops/ws1/WS1.html",
    "href": "workshops/ws1/WS1.html",
    "title": "Workshop Serie 1",
    "section": "",
    "text": "19.09. - 13.10.2023\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\nHier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an.",
    "crumbs": [
      "Workshop Serie 1"
    ]
  },
  {
    "objectID": "workshops/ws1/WS1.html#ablauf",
    "href": "workshops/ws1/WS1.html#ablauf",
    "title": "Workshop Serie 1",
    "section": "",
    "text": "In dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen."
  },
  {
    "objectID": "workshops/ws1/WS1.html#anmeldung",
    "href": "workshops/ws1/WS1.html#anmeldung",
    "title": "Workshop Serie 1",
    "section": "",
    "text": "Hier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an."
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_1.html",
    "href": "workshops/ws2/WS2_Modul_1.html",
    "title": "Modul 1: Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… Kontext und Vorarbeiten des Projekts nennen.\n… verstehen, was das Ziel dieses Projekts ist.\n… die Datenanalyse als Teil eines größeren Kontexts beschreiben.\n… die einzelnen Schritte von der Vereinbarung der Datenübertragung bis hin zur Archivierung der einzelnen Akten beschreiben.\n… mit den relevanten Personen in Kontakt treten und einen reibungsarmen Ablauf ermöglichen.",
    "crumbs": [
      "Module",
      "Modul 1: Einführung, Problemerfassung und Arbeitsabläufe"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_1.html#material",
    "href": "workshops/ws2/WS2_Modul_1.html#material",
    "title": "Modul 1: Einführung, Problemerfassung und Arbeitsabläufe",
    "section": "Material",
    "text": "Material\n\nSlides",
    "crumbs": [
      "Module",
      "Modul 1: Einführung, Problemerfassung und Arbeitsabläufe"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_3.html",
    "href": "workshops/ws3/WS3_Modul_3.html",
    "title": "Workshop Serie 3, Modul 3",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "Workshop Serie 3, Modul 3"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_3.html#themen",
    "href": "workshops/ws3/WS3_Modul_3.html#themen",
    "title": "Workshop Serie 3, Modul 3",
    "section": "",
    "text": "In Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.",
    "crumbs": [
      "Module",
      "Workshop Serie 3, Modul 3"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3_Modul_3.html#material",
    "href": "workshops/ws3/WS3_Modul_3.html#material",
    "title": "Workshop Serie 3, Modul 3",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "Workshop Serie 3, Modul 3"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_4.html",
    "href": "workshops/ws2/WS2_Modul_4.html",
    "title": "Modul 4: Nachhaltige Implementierung, Wissensweitergabe und Infrastrukturerhalt",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… einen nachhaltigen Prozess etablieren, wie die Daten übertragen, die Analysen durchgeführt und Archivierungsentscheidungen dokumentiert werden.\n… ihr Wissen an andere weitergeben.\n… einschätzen, wie die benötigte Infrastruktur erhalten werden kann.\n\n\n\n\nMaterial\n\nSlides",
    "crumbs": [
      "Module",
      "Modul 4: Nachhaltige Implementierung, Wissensweitergabe und Infrastrukturerhalt"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_4.html#material",
    "href": "workshops/ws2/WS2_Modul_4.html#material",
    "title": "Workshop Serie 2, Modul 4",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "Workshop Serie 2, Modul 4"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_3.html",
    "href": "workshops/ws2/WS2_Modul_3.html",
    "title": "Modul 3: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… die gewählte Strategie für Datensätze aus verschiedenen Jahren umsetzen.\n… Lücken erkennen, die im Projekt zu schließen sind, um die Analysen in das Tagesgeschäft aufzunehmen.\n… die Datenanalysen dokumentieren.\n\n\n\n\nMaterial\n\nSlides",
    "crumbs": [
      "Module",
      "Modul 3: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_3.html#material",
    "href": "workshops/ws2/WS2_Modul_3.html#material",
    "title": "Workshop Serie 2, Modul 3",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "Workshop Serie 2, Modul 3"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_2.html",
    "href": "workshops/ws2/WS2_Modul_2.html",
    "title": "Workshop Serie 2, Modul 2",
    "section": "",
    "text": "Lernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n… die Stichprobe analysieren und visuell darstellen.\n… Schwächen der Strategie einschätzen und verteidigen.\n\n\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "Workshop Serie 2, Modul 2"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2_Modul_2.html#material",
    "href": "workshops/ws2/WS2_Modul_2.html#material",
    "title": "Workshop Serie 2, Modul 2",
    "section": "",
    "text": "Slides",
    "crumbs": [
      "Module",
      "Workshop Serie 2, Modul 2"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4.html",
    "href": "workshops/ws4/WS4.html",
    "title": "Workshop Serie 4",
    "section": "",
    "text": "11.10. - 08.11.2024\nIn dieser Workshop-Serie beschäftigen wir uns mit dem Thema Einzelbaumerkennung und insbesondere mit der Frage, wie Infrastruktur für die Datenanalyse aussehen könnte.\n\n\nBitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen.",
    "crumbs": [
      "Workshop Serie 4"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4.html#anmeldung",
    "href": "workshops/ws4/WS4.html#anmeldung",
    "title": "Workshop Serie 4",
    "section": "",
    "text": "Bitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Workshop Serie 4"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4.html#ablauf",
    "href": "workshops/ws4/WS4.html#ablauf",
    "title": "Workshop Serie 4",
    "section": "",
    "text": "Veranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen.",
    "crumbs": [
      "Workshop Serie 4"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_1.html#einführung-problemerfassung-und-daten",
    "href": "workshops/ws4/WS4_Modul_1.html#einführung-problemerfassung-und-daten",
    "title": "Workshop Serie 4 - Modul 1",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_2.html#konzeption-eines-datenprodukts-infrastruktur-und-software",
    "href": "workshops/ws4/WS4_Modul_2.html#konzeption-eines-datenprodukts-infrastruktur-und-software",
    "title": "Workshop Serie 4 - Modul 2",
    "section": "",
    "text": "In Modul 2 gehen wir auf Infrastrukturbedürfnisse und -möglichkeiten ein und diskutieren dabei insbesondere die Nutzung von Cloud-Anbietern. Sie können sich auf folgende Themen freuen:\n\nEinführung in Cloud-Computing und Cloud-Anbieter\nErfahrungen von ADA Bayern im Cloud-Bereich\nInfrastrukturanforderungen von Einzelbaumerkennungsdaten und -systemen\nAllgemeine aktuelle Herausforderungen der IT-Infrastruktur\nMögliche Anwendungen/Produkte\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "2. Konzeption eines Datenprodukts, Infrastruktur und Software"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_3.html#konzeption-einer-skalierbaren-cloud-lösung",
    "href": "workshops/ws4/WS4_Modul_3.html#konzeption-einer-skalierbaren-cloud-lösung",
    "title": "Workshop Serie 4 - Modul 3",
    "section": "",
    "text": "In Modul 3 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nErarbeiten einer sinnvollen Infrastrukturlösung\nSammlung relevanter Stakeholder und ihrer Bedürfnisse\nSammlung und Priorisierung von Requirements und Features einer Lösung\nSammlung von möglichen Risiken\nVorbereitung der nächsten Schritte im Nachgang des Workshops\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "3. Konzeption einer skalierbaren Cloud-Lösung"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4_Modul_1.html#section",
    "href": "workshops/ws4/WS4_Modul_1.html#section",
    "title": "Modul 1 Einführung, Problemerfassung und Daten",
    "section": "",
    "text": "In Modul 1 setzen wir den Rahmen, um Probleme zu erfassen und vorliegende Daten und Arbeitsabläufe zu verstehen. Sie können sich auf folgende Themen freuen:\n\nEinführung in die Workshop-Serie\nVorstellung der Problemstellung einzelner Behörden\nTypische Arbeitsabläufe im Kontext fernerkundungsdatengetriebener Waldinventur\n\n\n\n\nSlides",
    "crumbs": [
      "Module",
      "1. Einführung, Problemerfassung und Daten"
    ]
  },
  {
    "objectID": "workshops/ws2/WS2.html#implementation-daten-basierter-archivierung-von-gerichtsakten",
    "href": "workshops/ws2/WS2.html#implementation-daten-basierter-archivierung-von-gerichtsakten",
    "title": "Workshop Serie 2",
    "section": "",
    "text": "08.02. - 13.03.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off\n08.02.\n11:00 - 13:00\nOnline (Zoom)\n\n\nModul 1\n20.02.\n10:00 - 15:30\nLMU\n\n\nModul 2\n21.02.\n10:00 - 15:30\nLMU\n\n\nModul 3\n22.02.\n10:00 - 15:30\nLMU\n\n\nModul 4\n23.02.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n28.02.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n06.03.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 3\n13.03.\n10:00 - 12:00\nLMU\n\n\nAbschlussvorstellungen\n13.03.\n12:00 - 15:30\nLMU\n\n\n\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Die Termine an der LMU findet im Institut für Statistik, Ludwigstr. 33, 4. Stock statt.\n\n\n\n\nIn dieser Workshop-Serie bauen wir auf die Inhalte der Workshop-Serie 1 auf und setzen das gelernte in die Praxis um. Gemeinsam erarbeiten wir Arbeitsabläufe, die sowohl aus Datenanlyseperspektive als auch aus Perspektive der Staatsarchive und der Justiz IT Sinn ergeben. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… Stichprobenziehung für die Archivierung von Gerichtsakten dürchführen.\n… Die Cloud-Infrastruktur für die jährliche Stichprobenziehung nutzen.\n… Die Datenanalyse-Aufgaben für die Archivierung von Gerichtsakten auf neue Daten anpassen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\n\n\nKick-Off Slides\n\n\n\nDie Inhalte der Module finden sich auf den kommenden Seiten.\n\nModul 1 - Einführung, Problemerfassung und Arbeitsabläufe\nModul 2 - Umsetzung: Implementierung der Stichprobenziehung\nModul 3 - Umsetzung: Implementierung der Stichprobenziehung im Kontext des Gesamtablaufs\nModul 4 - Nachhaltige Implementierung: Wissensweitergabe und Infrastrukturerhalt",
    "crumbs": [
      "Workshop Serie 2"
    ]
  },
  {
    "objectID": "workshops/ws4/WS4.html#einzelbaumerkennung",
    "href": "workshops/ws4/WS4.html#einzelbaumerkennung",
    "title": "Workshop Serie 4",
    "section": "",
    "text": "11.10. - 08.11.2024\nIn dieser Workshop-Serie beschäftigen wir uns mit dem Thema Einzelbaumerkennung und insbesondere mit der Frage, wie Infrastruktur für die Datenanalyse aussehen könnte.\n\n\nBitte melden Sie sich über Pretix an.\nBei Fragen wenden Sie sich gerne an data-analytics@stat.uni-muenchen.de.\n\n\n\n\n\n\n\n\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\nBeschreibung\n\n\n\n\nKick-Off\n11.10.\n10:30-12:00\nonline\nWillkommen und Einführung\n\n\nTeil 1, Tag 1\n15.10.\n13:00-16:30\nLMU\n(1) Einführung(2) Problemerfassung und Daten: Nutzung von Fernerkundungsdaten für das Baum- und Forstmanagement\n\n\nTeil 1, Tag 2\n16.10.\n09:30-15:30\nLMU\n(1) Funktionsweisen von Algorithmen zur Nutzung von Fernerkundungsdaten(2) Herausforderungen an die IT-Infrastruktur(3) Brainstorming zu möglichen Anwendungen & Datenprodukten(4) Welche Möglichkeiten bietet die Cloud?(optional: Parkspaziergang im Anschluss)\n\n\nTeil 2\n17.10.\n09:30-15:30\nLMU\nNur für IT-Interessierte: (1) Konzeption einer Cloud-Umgebung (Prototyp) zur vereinfachten Arbeit mit Fernerkundungsdaten (2) Feststellung technischer Anforderungen (3) Fortführung der Diskussion vom Vormittag (4) Arbeitsabläufe und nächste Schritte\n\n\nFollow-Up 1\n23.10.\n12:30-14:00\nonline\n\n\n\nFollow-Up 2\n30.10.\n12:30-14:00\nonline\n\n\n\nAbschluss\n08.11.\n13:00-15:00\nhybrid\nZwischenstand und Ausblick (in Abhängigkeit von den Ergebnissen des Workshops)\n\n\n\nDer Workshop vor Ort besteht aus zwei Teilen.\nDer erste Teil findet am 15.10. und am 16.10. statt. Dabei soll das Thema “Baum- und Waldmanagement mit Fernerkundungsdaten” in einer Gesamtschau ausgeleuchtet werden, insbesondere in Hinblick auf Anwendungen in der öffentlichen Verwaltung. Ziel des ersten Teiles ist es, anwenderorientiert und allgemeinverständlich zu besprechen, welche Art von Luft- und Satellitenbildern für welche Zwecke nutzbar sind, mithilfe welcher Technologien aus dem Bereich des Cloud-Computing und der Künstlichen Intelligenz diese Bilder automatisch ausgewertet werden können und welchen Mehrwert das Baum- und Forstmanagement bereits aus der Nutzung solcher Datenprodukte zieht und mögliche Verbesserungen in diesem Rahmen.\nDer zweite Teil findet am 17.10. statt. Er richtet sich an alle, die sich intensiv und tiefergehend mit Infrastruktur- und IT-Herausforderungen bei der Nutzung von Fernerkundungsdaten beschäftigen wollen. Wesentliche Anwendungsfälle sind bislang nur schwer realisierbar, weil die bestehende technische Infrastruktur an ihre Grenzen stößt, insbesondere wenn mehrere Terabyte an Bilddaten vorliegen. Ziel des zweiten Teiles ist es, eine skalierbare und für unterschiedliche Anwendungsfälle nutzbare Cloud-Lösung konzeptionell zu durchdenken und technische Anforderungen zu besprechen. Aufbauend auf den Ergebnissen aus der Diskussion soll im Idealfall im Nachgang ein einsatzbereiter Prototyp in der Cloud umgesetzt werden.\nFür die Teilnahme am zweiten Teil (17.10.) wird die Teilnahme am ersten Teil (15.10. - 16.10.) vorausgesetzt. Bitte melden Sie sich explizit für die einzelnen Tage an.\n\n\n\n\nOnline: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\n\n\nDas Thema Einzelbaumerkennung inklusive Baumeigenschaften (Art, Maße, Totholz ja/nein, Borkenkäferbefall-Status, etc.) ist für viele Behörden, öffentliche Einrichtungen, Kommunen, etc. relevant. Bild-Daten von Drohnen, Flugzeugen und Satelliten (Sentinel) werden gesammelt oder bereits genutzt, um die Einzelbaumerkennung (z.B. mit Hilfe vortrainerter Modelle) durchzuführen. Für bestimmte Flächen werden weitere Daten - wie Laserscanner-Messungen oder “manuelle” Messungen von Koordinaten, Höhe, Art usw. - erhoben, die für das Training von Einzelbaumerkennungs-Modellen genutzt werden können (z.B. für Deep Learning Modelle).\nDie Nutzungsmöglichkeiten von Einzelbaumerkennung sind vielfältig und umfassen neben verschiedenen Forschungsansätzen und der statistischen Baumerfassung für Naturschutz- und Wald-Politik auch die Möglichkeit, jedem Baum eine E-Mail-Adresse zu geben, damit Bürger:innen ihnen schreiben können ;). Nach derzeitigem Planungsstand sollen aber Stadtplaner:innen, staatliche Forstmitarbeitende und andere Verwaltungsmitarbeitende die von mehr Wissen über Standort, Art, und Eigenschaften der Bäume profitieren können, im Mittelpunkt unseres Workshops stehen. Entsprechend sollen praxisnahe Zielsetzungen wie Waldmanagement, Baumpflege oder die Mortalität von Stadtbäumen genauer beleuchtet werden.\n\n\n\nIn dieser ersten Workshop-Serie zum Thema Baum- und Waldmanagement möchten wir relevante Stakeholder zusammenbringen und zentrale Fragen wie “Welche Bäume gibt es wo?”, “Wie viele Bäume gibt es im Gebiet XY?”, “Wie groß ist die Biomasse?”, “Welche Eigenschaften hat jeder Baum (z.B. Baumgesundheit)?”, “Kann man die auf einer Karte (ggf. mit Pop-Ups für Eigenschaften) anzeigen?”, und insbesondere “Welche Infrastruktur und Daten werden benötigt, um dies umzusetzen?”, usw. gemeinsam angehen.\n\n\n\n\nStudierende im Masterstudiengang sind ebenfalls herzlich eingeladen, sich anzumelden. Wir hoffen, dass die Diskussionen und Gespräche im Rahmen des Workshops als Ausgungspunkt für spannende Seminar- und Masterarbeiten dienen. Statistikstudierende können nach aktiver Teilnahme durch eine abschließende mündliche Prüfung 3 ECTS erwerben, die im Rahmen der Master-Wahlpflichtmodule “Selected Topics in …” angerechnet werden können. Wegen Platzmangel können wir leider nur sehr wenigen Studierenden eine Teilnahme ermöglichen.",
    "crumbs": [
      "Workshop Serie 4"
    ]
  },
  {
    "objectID": "workshops/ws1/WS1_Modul_3.html",
    "href": "workshops/ws1/WS1_Modul_3.html",
    "title": "Modul 3: Visualisierung und Reporting",
    "section": "",
    "text": "02.10.2023, 10:00 - 15:00 Uhr\n\nThemen\nIn Modul 3 werden die Guidelines für nachvollziehbares Reporting und gute Visualisierungen besprochen. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nVisualisierung der vorliegenden Daten\nVisualisierung der Analyse (Stichprobenziehung)\nBest practices für Visualisierungen\nÜberblick über Reporting und Dashboards für Datenanalyse-Projekte\n\n\n\n\n\nAnzahl der Fälle pro Amtsgerichtbezirk\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nGrafische Datenanalyse\n\n… Ergebnisse über Grafiken kommunizieren (Balkendiagramme, Karten und co.).\n… gute Beispiele für Datenvisualisierungen erkennen.\n\nErgebnisse kommunizieren\n\n… Notebooks für Reporting verwenden.\n… einschätzen, wann Dashboards nützlich sind für die Kommunikation von Datenanalysen.\n\n\n\n\n\nMaterial\n\nSlides\nNotebook 3 - Best of Stichprobenziehung",
    "crumbs": [
      "Module",
      "Modul 3: Visualisierung und Reporting"
    ]
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine stratifizierte oder auch geschichtete Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen. Wir wollen interessante Spalten für die Stratifizierung identifizieren und den Code in diesem Notebook für das Projekt anpassen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziel-des-notebooks",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziel-des-notebooks",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine stratifizierte oder auch geschichtete Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen. Wir wollen interessante Spalten für die Stratifizierung identifizieren und den Code in diesem Notebook für das Projekt anpassen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#software-pakete-laden",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#software-pakete-laden",
    "title": "ADA Bayern",
    "section": "2 Software-Pakete laden",
    "text": "2 Software-Pakete laden\nWir laden erneut die gleichen Pakete, die wir auch schon zum Ziehen der einfachen Zufallstichprobe benötigt haben.\n\nlibrary(\"tidyverse\")\nlibrary(\"survey\")\nlibrary(\"PracTools\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#einlesen-der-daten",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#einlesen-der-daten",
    "title": "ADA Bayern",
    "section": "3 Einlesen der Daten",
    "text": "3 Einlesen der Daten\nIn diesem Schritt lesen wir die Daten in R ein. Der Code ist genau wie zuvor.\n\npfad &lt;- file.path(\".\")\ncol_types &lt;- c(\"ccccnccccccccccccccccccTTcnTccnTcTcccc\")\n\nforumstar_daten &lt;- read_csv(file.path(pfad, \"230817_Abfrage_Januar-Dezember.csv\"), col_types = col_types)\nkarte_amtsgerichte &lt;- readRDS(file.path(pfad, \"230911_Karte_Amtsgerichtsbezirke.RDS\"))"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "title": "ADA Bayern",
    "section": "4 Daten für die Stichprobenziehung vorbereiten",
    "text": "4 Daten für die Stichprobenziehung vorbereiten\nFür die Stichprobenziehung benötigen wir nicht alle Spalten des Datensatzes. Daher bereiten wir die Daten zunächst vor. Für eine einfache Stichprobenziehung reicht ein Eintrag pro Akte aus. Die beteiligten Personen interessieren uns hier zunächst nicht. Mit dem folgenden Code aggregieren wir die Daten auf einen Eintrag pro Aktenzeichen1.\n\nakten &lt;- forumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`,\n    `Streitwert in EURO`,\n    `Gesamtstreitgegenstand`,\n    `Erledigungsgrund`,\n    `Dauer des Verfahrens in Tagen`,\n    `Archivstatus`,\n    `Anbietungsgrund (manuell erfasst)`,\n    `Anbietungsgrund`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nUm unsere visuelle Darstellung auf Karten zu ermöglichen generieren wir außerdem die Spalte Bezirk und fügen sie unserem Datensatz hinzu.\n\ntmp_Gericht &lt;- sub(\"Amtsgericht \", \"\", akten$Gericht)\ntmp_Gericht &lt;- sub(\" Zweigstelle.*\", \"\", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.d. \", \" i. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" a.d. \", \" a. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" am \", \" a. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.OB\", \" i. OB\", tmp_Gericht)\n\nakten &lt;- akten %&gt;%\n  mutate(`Bezirk` = tmp_Gericht)"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-zufallsstichprobe-stratifiziert-nach-amtsgerichtsbezirken",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-zufallsstichprobe-stratifiziert-nach-amtsgerichtsbezirken",
    "title": "ADA Bayern",
    "section": "5 Ziehung einer stratifizierten Zufallsstichprobe, stratifiziert nach Amtsgerichtsbezirken",
    "text": "5 Ziehung einer stratifizierten Zufallsstichprobe, stratifiziert nach Amtsgerichtsbezirken\nBei einer stratifizierten Stichprobe werden zusätzlich verfügbare Information aus der Grundgesamtheit zur Stichprobenziehung verwendet.\nVielleicht wollen wir sicherstellen, dass Akten aus allen Amtsgerichtsbezirken archiviert werden? Dann könnten Historiker in Zukunft verschiedenen Regionen Bayerns untereinander vergleichen. Mit diesem Gedanken sollen nun zufällig zehn Akten pro Bezirk gezogen werden.\nVon jedem Amtsgerichtsbezirk (jeder Bezirk bildet hier ein sogenanntes Stratum) benötigen wir die Gesamtzahl der zur Verfügung stehenden Akten.\n\nbeschreibung_stichprobe &lt;- akten %&gt;% \n  group_by(`Bezirk`) %&gt;%\n  summarise(`Anzahl Akten` = n()) %&gt;%\n  arrange(`Bezirk`)\n\nNun sollen aus jedem Bezirk zehn Akten zufällig ausgewählt werden.\n\nbeschreibung_stichprobe &lt;- beschreibung_stichprobe %&gt;% \n  mutate(`Stichprobengröße` = 10)\n\nZur Vorbereitung der Stichprobenziehung werden die Infos aus beschreibung_stichprobe an die Grundgesamtheit herangespielt.\n\nakten_fuer_stratifizierung_nach_gericht &lt;- akten %&gt;%\n  left_join(beschreibung_stichprobe, by = \"Bezirk\")\n\nNun erfolgt die Stichprobenziehung2.\n\nset.seed(20230919)\nstratifizierte_stichprobe_nach_gericht &lt;- akten_fuer_stratifizierung_nach_gericht %&gt;%\n  group_by(Bezirk) %&gt;%\n  mutate(samp = sample(n())) %&gt;%\n  filter(samp &lt;= `Stichprobengröße`) %&gt;%\n  ungroup()\n\nstratifizierte_stichprobe_nach_gericht$samp &lt;- NULL\n\nAlle nötigen Infos zur Beschreibung der Stichprobe sind in der folgenden Tabelle vorhanden:\n\nbeschreibung_stichprobe\n\nKurze Prüfung. Haben wir tatsächlich zehn Akten pro Gericht gezogen?\n\nstratifizierte_stichprobe_nach_gericht %&gt;%\n  group_by(`Bezirk`) %&gt;%\n  summarise(n())\n\nWie viele Akten liegen insgesamt in der Stichprobe vor?\n\nnrow(stratifizierte_stichprobe_nach_gericht)\n\n\n5.1 Ist die stratifizierte Stichprobe repräsentativ?\nAkten aus kleinen Amtsgerichtsbezirken hatten in dieser Stichprobe eine deutlich höhere Wahrscheinlichkeit, in die Stichprobe zu gelangen, als zum Beispiel aus dem Amtsgericht München. Wenn man dies unberücksichtigt lässt, können keine statistisch validen Aussagen über die Grundgesamtheit getroffen werden.\nFür repräsentative statistische Aussagen müssen Gewichte verwendet werden. Akten mit geringer Auswahlwahrscheinlichkeit stehen für eine größere Anzahl von Akten und erhalten damit ein größeres Gewicht.\nDie Auswahlwahrscheinlichkeit berechnet sich in jedem Stratum getrennt als\n\nAuswahlwahrscheinlichkeit = Anzahl (Stichprobe) / Anzahl (Grundgesamtheit)\n\nDas Gewicht (= Hochrechnungsfaktor) ergibt sich als\n\nHochrechnungsfaktor = 1 / Auswahlwahrscheinlichkeit\n\nEine Akte mit einem Hochrechnungsfaktor von z.B. 20 wurde also mit einer Wahrscheinlichkeit von 5% in die Stichprobe gezogen und steht nun repräsentativ für 20 Akten, die alternativ hätten gezogen werden können.\nAuswahlwahrscheinlichkeit und Hochrechnungsfaktor lassen sich allein aus den Angaben zur Beschreibung der Stichprobe wie folgt berechnen.\n\nstratifizierte_stichprobe_nach_gericht &lt;- stratifizierte_stichprobe_nach_gericht %&gt;%\n  mutate(Auswahlwahrscheinlichkeit = `Stichprobengröße` / `Anzahl Akten`) %&gt;%\n  mutate(Hochrechnungsfaktor = 1 / Auswahlwahrscheinlichkeit)\n\nHier zeigt sich also, warum diese Dokumentation der Stichprobe so wichtig ist.\nAls Beispiel soll nun die mittlere Anzahl der Prozessbeteiligten geschätzt werden.\nIn der Grundgesamtheit können wir dies berechnen als\n\\[\n\\bar{y} = \\frac{\\sum_{grundgesamtheit} 1 \\cdot y_i}{N}\n\\]\n\nmean(akten$`Anzahl Beteiligte`)\n\nAnhand der Stichprobe können wir mittels folgender Formel ungefähr den selben Wert erhalten\n\\[\n\\hat{\\bar{y}}=\\frac{\\sum_{stichprobe} hrf_i \\cdot y_i}{\\sum_{stichprobe} hrf_i}\n\\]\n\nsum(stratifizierte_stichprobe_nach_gericht$Hochrechnungsfaktor * stratifizierte_stichprobe_nach_gericht$`Anzahl Beteiligte`) / sum(stratifizierte_stichprobe_nach_gericht$Hochrechnungsfaktor)\n\nZum selben Ergebnis führt auch der folgende Code3.\n\nmy_stratified_design &lt;- svydesign(id = ~0, strata = ~Bezirk, data = stratifizierte_stichprobe_nach_gericht, fpc = ~Auswahlwahrscheinlichkeit)\n\nsvymean(~`Anzahl Beteiligte`, my_stratified_design, na.rm = TRUE)\n\nZusätzlich lässt sich noch das Konfidenzintervall berechnen, welches mit 95%-Wahrscheinlichkeit den wahren Wert aus der Grundgesamtheit überdeckt. Damit kann die Genauigkeit der Schätzung beurteilt werden.\n\nconfint(svymean(~`Anzahl Beteiligte`, my_stratified_design, na.rm = TRUE))\n\nEine naive Schätzung der mittleren Anzahl Prozessbeteiligter berechnet einfach nur den Mittelwert der Stichprobe und lässt dabei die unterschiedlichen Auswahlwahrscheinlichkeiten außer Acht.\n\nmean(stratifizierte_stichprobe_nach_gericht$`Anzahl Beteiligte`)\n\nAnders als die Schätzung oben ist dieser Wert nicht zur Beschreibung der Grundgesamtheit geeignet. Nur wenn Gewichte (=Hochrechnungsfaktoren) verwendet werden, können aus der Stichprobe Ergebnisse abgeleitet werden, die repräsentativ für die Grundgesamtheit sind."
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-stichprobe-stratifiziert-nach-verfahrensdauer",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-stichprobe-stratifiziert-nach-verfahrensdauer",
    "title": "ADA Bayern",
    "section": "6 Ziehung einer stratifizierten Stichprobe, stratifiziert nach Verfahrensdauer",
    "text": "6 Ziehung einer stratifizierten Stichprobe, stratifiziert nach Verfahrensdauer\nEine stratifizierte Stichprobe kann noch aus einem anderen Grund sinnvoll sein: In diesem Beispiel sollen Akten mit einer langen Verfahrensdauer besonders häufig archiviert werden, da diese Akten besonders umfangreich und informativ sein könnten.\nZunächst betrachten wir einen kurze Zusammenfassung der Verfahrensdauer.\n\nsummary(akten$`Dauer des Verfahrens in Tagen`)\n\nDie stetige Variable Dauer des Verfahrens in Tagen diskretisieren wir:\n\nkurzes Verfahren, wenn die Verfahrensdauer weniger als 100 Tage betrug\nmittleres Verfahren bei 100-1500 Tagen\nlanges Verfahren ab 1500 Tagen Verfahrensdauer\n\nBei fehlenden Werten wird von einem kurzen Verfahren ausgegangen.\n\nakten &lt;- akten %&gt;% mutate(\n    verfahrensdauer = case_when(\n      `Dauer des Verfahrens in Tagen` &lt; 100 ~ \"kurzes Verfahren\",\n      `Dauer des Verfahrens in Tagen` &gt;= 100 & `Dauer des Verfahrens in Tagen` &lt; 1500 ~ \"mittleres Verfahren\",\n      `Dauer des Verfahrens in Tagen` &gt;= 1500 ~ \"langes Verfahren\",\n      is.na(`Dauer des Verfahrens in Tagen`) ~ \"kurzes Verfahren\",\n      TRUE ~ as.character(`Dauer des Verfahrens in Tagen`)\n    )\n)\n\nDie Variable verfahrensdauer dient zur Stratifizierung. Zur Beschreibung der Stichprobe wird zunächst gezählt, wie viele Akten eine kurze/mittlere/lange Verfahrensdauer haben.\n\nbeschreibung_stichprobe &lt;- akten %&gt;% \n  group_by(verfahrensdauer) %&gt;%\n  summarise(akten_pro_stratum = n()) %&gt;%\n  arrange(verfahrensdauer)\n\nbeschreibung_stichprobe\n\nWir setzen die Stichprobengröße für jedes einzelne Stratum händisch. Dabei sollen alle 178 langen Verfahren archiviert werden.\n\nbeschreibung_stichprobe &lt;- beschreibung_stichprobe %&gt;% mutate(\n     anzahl_akten_pro_schicht = case_when(\n      verfahrensdauer == \"kurzes Verfahren\" ~ 50,\n      verfahrensdauer == \"mittleres Verfahren\" ~ 100,\n      verfahrensdauer == \"langes Verfahren\" ~ 178)\n)\n\nbeschreibung_stichprobe\n\nMit dem bereits oben beschriebenen Code kann hier wieder die Stichprobe gezogen werden.\n\nakten_fuer_stratifizierung_nach_verfahrensdauer &lt;- akten %&gt;%\n  left_join(beschreibung_stichprobe, by = \"verfahrensdauer\")\n\nset.seed(46)\nstratifizierte_stichprobe_nach_verfahrensdauer &lt;- akten_fuer_stratifizierung_nach_verfahrensdauer %&gt;%\n  group_by(verfahrensdauer) %&gt;%\n  mutate(samp = sample(n())) %&gt;%\n  filter(samp &lt;= anzahl_akten_pro_schicht) %&gt;%\n  ungroup()\n\nstratifizierte_stichprobe_nach_gericht$samp &lt;- NULL\n\nEin kurzer Check, ob die Stichprobenziehung erfolgreich war.\n\ntable(stratifizierte_stichprobe_nach_verfahrensdauer$verfahrensdauer)\n\n\n6.1 Wie kann die Stichprobengröße pro Schicht bestimmt werden? (optional)\nHier wurde händisch vorgegeben, wie viele Akten aus jeder Schicht archiviert werden sollen. Alternativ wäre auch möglich:\n\nProportionale Allokation: Proportional zum Umfang der jeweiligen Schicht, stellt Repräsentativität bezüglich des Schichtungsmerkmals sicher\nEqual (gleiche) Allokation: gleiches \\(n\\) in jedem Stratum, so wie es oben bei der Stratifizierung nach Gerichten erfolgt ist\nOptimale Allokation: Optimiere die Genauigkeit der Schätzung für eine Variable. Die Varianz dieser Variablen muss dafür bekannt sein.\n\nDie Stichprobengröße pro Schicht soll nun mit jeder der drei genannten Methoden bestimmt werden. Als Schichtungsvariable verwenden wir erneut die verfahrensdauer (kurz/mittel/lang). Anhand der Grundgesamtheit berechnen wir, wie viele Akten \\(N_h\\) in jeder Schicht \\(h\\) vorhanden sind. Für die optimale Allokation benötigen wir zusätzlich noch die Varianz derjenigen Variablen, für die eine möglichst genaue Schätzung erzielt werden soll.\n\nbeschreibung_stichprobe_allokation &lt;- akten %&gt;% \n  group_by(verfahrensdauer) %&gt;%\n  summarise(akten_pro_stratum = n(),\n            variance_pro_stratum = var(`Dauer des Verfahrens in Tagen`, na.rm = TRUE)) %&gt;%\n  arrange(verfahrensdauer)\n\nZusätzlich geben wir an, dass die Stichprobe insgesamt \\(n = 200\\) Akten enthalten soll. Mithilfe der folgenden Formeln lässt sich nun berechnen, wie viele Akten aus jeder Schicht \\(h\\) gezogen werden:\n\\[n_{h, proportional} = Runden(n \\cdot \\frac{N_h}{N}) = Runden(n \\cdot \\frac{N_h}{\\sum_{h' = 1}^M N_{h'}})\\]\n\\[n_{h, optimal} = Runden(n \\cdot \\frac{N_h \\cdot S_h}{\\sum_{h' = 1}^M N_{h'} \\cdot S_{h'}})\\]\n\\[n_{h, equal} = Runden(n/M)\\]\n\nN &lt;- nrow(akten)\nn &lt;- 200\n\ndenom &lt;- sum(beschreibung_stichprobe_allokation$akten_pro_stratum * beschreibung_stichprobe_allokation$variance_pro_stratum)\n\nbeschreibung_stichprobe_allokation &lt;- beschreibung_stichprobe_allokation %&gt;%\n  mutate(proportional_allocation = round((n / N) * akten_pro_stratum),\n         optimal_allocation = round(n * akten_pro_stratum * variance_pro_stratum / denom),\n         equal_allocation = round(n / nrow(beschreibung_stichprobe_allokation)))\n\nbeschreibung_stichprobe_allokation\n\nDa es nur sehr wenige lange Verfahren gibt, gelangt kein einziges davon bei einer Zuordnung proportional zur Anzahl (proportional allocation) in die Stichprobe.\nBei der optimal allocation wird versucht, möglichst viel über die Dauer des Verfahrens in Tagen (oder ein damit korrelierendes Merkmal) zu erfahren. Bei den kurzen Verfahren wissen wir bereits, dass alle Verfahren weniger als 50 Tage dauern (kleine Varianz). Entsprechend gering ist der Informationsgewinn, wenn ein solches Verfahren in die Stichprobe gelangt. Umgekehrt kann die Länge eines langen Verfahrens (&gt;1500 Tage) stark streuen. Um dennoch möglichst akkurat die mittlere Dauer der langen Verfahren berechnen zu können, müssten hier sehr viele Akten gezogen werden.\nBei der equal allocation gelangt jede Schicht gleich häufig in die Stichprobe."
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-stratifizierten-stichprobe",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-stratifizierten-stichprobe",
    "title": "ADA Bayern",
    "section": "7 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer stratifizierten Stichprobe?",
    "text": "7 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer stratifizierten Stichprobe?\n\nIst die stratifizierte Stichprobe zur Auswahl von Akten geeignet? Welche Vor- und Nachteile hat das Verfahren?\nAnhand welcher Variablen definiert man einzelne Strata? Welche Schwellwerte verwendet man bei kontinuierlichen Variablen?\nWie viele Akten werden aus jedem Stratum gezogen? (Proportionale Allokation, Equal Allokation, Optimal Allokation, oder noch was anderes?)\n\nVorteile:\n\nUnterschiedliche Auswahlwahrscheinlichkeiten in jedem Stratum\nKonfidenzintervalle werden schmaler wenn Selektionswahrscheinlichkeiten und Zielvariable korreliert sind.\nGarantierte Mindestzahl an Fällen in jedem Stratum (zum Beispiel zehn Akten pro Gericht)\nJedes Stratum einer stratifizierten Zufallsstichprobe lässt sich auch einzeln auswerten (einfach wie eine einfache Zufallsstichprobe, jedoch ist die Grundgesamtheit nun beschränkt auf das Stratum).\nEinfach verständlich und gut dokumentierbar\n\nNachteile:\n\nUnterschiedliche Auswahlwahrscheinlichkeiten in jedem Stratum\nKein Effizienzgewinn (unter Umstand. sogar -verlust), wenn Selektionswahrscheinlichkeiten und Zielvariablen nicht (bzw. negativ) korreliert sind\nMehr Dokumentationsaufwand"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-wie-wollen-wir-als-gruppe-akten-für-die-archivierung-auswählen",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-wie-wollen-wir-als-gruppe-akten-für-die-archivierung-auswählen",
    "title": "ADA Bayern",
    "section": "8 Diskussion: Wie wollen wir als Gruppe Akten für die Archivierung auswählen?",
    "text": "8 Diskussion: Wie wollen wir als Gruppe Akten für die Archivierung auswählen?\n\nEinfache Zufallsstichprobe vs. stratifiziert?\nWenn stratifiziert, wie wählen wir die Strata aus?\nOder gar keine Zufallsstichprobe (zum Beispiel zehn Akten je Gericht mit den meisten Beteiligten?)\nWie viele Akten wählen wir?\nWelche Informationen zum Auswahlprozess müssen wir dokumentieren und kommunizieren?"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#footnotes",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#footnotes",
    "title": "ADA Bayern",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDazu verwenden wir die group_by Funktion. Zusätzlich erstellen wir mithilfe der Funktion summarise()eine neue Spalte Anzahl Beteiligte. Anschließend fügen wir noch eine eindeutige Index Spalte Index für jede Akte hinzu.↩︎\nMit set.seed wird zunächst ein Seed gesetzt, so dass immer dieselbe Stichprobe gezogen wird.\nBezirk soll unsere Stratumsvariable sein. Daher werden mittels group_by die nachfolgenden Schritte für jedes Gericht einzeln getrennt voneinander durchgeführt.\nAlle Akten eines jeden Gerichts werden durchnummeriert von 1 bis zur Gesamtzahl der Akten vom jeweiligen Gericht n(), die Nummern werden anschließend mittels sample gemischt und jede Akte bekommt in der Variablen samp eine Nummer zugewiesen. Akten mit einer Nummer kleiner oder gleich 10 (vorgegeben in der Variablen anzahl_akten_pro_gericht) werden herausgefiltert und bleiben in der Stichprobe erhalten.\nAbschließend wird die Hilfsvariable samp gelöscht, indem ihr der Wert NULL zugewiesen wird.↩︎\nDie Funktionen svydesign, svymean und confint kommen aus dem survey-package. Mit svydesign lässt sich das Design der Stichprobe beschreiben. Die anderen Funktionen nutzen dieses Design zur Berechnung vom gewichteten Mittelwert bzw. zur Berechnung des Konfidenzintervalls.↩︎"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine einfache Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziel-des-notebooks",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziel-des-notebooks",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine einfache Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#software-pakete-laden",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#software-pakete-laden",
    "title": "ADA Bayern",
    "section": "2 Software-Pakete laden",
    "text": "2 Software-Pakete laden\nZunächst laden wir einige R-Pakete, die uns bei der Analyse helfen.\nR-Pakete sind Erweiterungen, Sammlungen von nützlichem Code die im CRAN (Comprehensive R Archive Network) zur Verfügung gestellt werden.\nIn diesem Modul nutzen wir das sogenannte tidyverse1, das viele nützliche und einfach verständliche (tidy!) Funktionen zum Arbeiten mit Datensätzen bereitstellt.\nDas survey2 Paket hilft uns später bei der Stichprobenziehung. Das Paket stellt viele nützliche Funktionen für die Analyse von Daten aus Stichproben bereit.\nDas PracTools3 Paket beeinhaltet weitere nützliche Funktionen zur Analyse komplexerer Stichprobendesigns.\nWir haben die Pakete bereits alle in der Arbeitsumgebung installiert4. Installierte Pakete können dann mit der Funktion library() für unser Notebook verfügbar gemacht werden.\n\nlibrary(\"tidyverse\")\nlibrary(\"survey\")\nlibrary(\"PracTools\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#einlesen-der-daten",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#einlesen-der-daten",
    "title": "ADA Bayern",
    "section": "3 Einlesen der Daten",
    "text": "3 Einlesen der Daten\nIn diesem Schritt lesen wir die Daten in R ein5.\n\npfad &lt;- file.path(\".\")\ncol_types &lt;- c(\"ccccnccccccccccccccccccTTcnTccnTcTcccc\")\n\nforumstar_daten &lt;- read_csv(file.path(pfad, \"230817_Abfrage_Januar-Dezember.csv\"), col_types = col_types)\nkarte_amtsgerichte &lt;- readRDS(file.path(pfad, \"230911_Karte_Amtsgerichtsbezirke.RDS\"))"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "title": "ADA Bayern",
    "section": "4 Daten für die Stichprobenziehung vorbereiten",
    "text": "4 Daten für die Stichprobenziehung vorbereiten\nFür die Stichprobenziehung benötigen wir nicht alle Spalten des Datensatzes. Auch brauchen wir nur einen einzigen Eintrag pro Akte. Die beteiligten Personen interessieren uns hier zunächst nicht. Daher bereiten wir die Daten zunächst vor.\nMit dem folgenden Code aggregieren wir die Daten auf einen Eintrag pro Aktenzeichen6.\n\nakten &lt;- forumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`,\n    `Streitwert in EURO`,\n    `Gesamtstreitgegenstand`,\n    `Erledigungsgrund`,\n    `Dauer des Verfahrens in Tagen`,\n    `Archivstatus`,\n    `Anbietungsgrund (manuell erfasst)`,\n    `Anbietungsgrund`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nUm unsere visuelle Darstellung auf Karten zu ermöglichen generieren wir außerdem die Spalte Bezirk und fügen sie unserem Datensatz hinzu.\n\ntmp_Gericht &lt;- sub(\"Amtsgericht \", \"\", akten$Gericht)\ntmp_Gericht &lt;- sub(\" Zweigstelle.*\", \"\", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.d. \", \" i. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" a.d. \", \" a. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" am \", \" a. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.OB\", \" i. OB\", tmp_Gericht)\n\nakten &lt;- akten %&gt;%\n  mutate(`Bezirk` = tmp_Gericht)"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziehung-einer-einfachen-zufallsstichprobe",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziehung-einer-einfachen-zufallsstichprobe",
    "title": "ADA Bayern",
    "section": "5 Ziehung einer einfachen Zufallsstichprobe",
    "text": "5 Ziehung einer einfachen Zufallsstichprobe\nFür die Ziehung einer einfachen Zufallsstichprobe benötigen wir eine wohl definierte Grundgesamtheit. In unserem Fall bilden alle Akten aus dem Jahr 2018 (alle bayerischen Amtsgerichte, nur Registerzeichen C) die Grundgesamtheit. Der Umfang der Grundgesamtheit, hier die Anzahl aller Akten in einem Jahr, wird mit \\(N\\) bezeichnet.\nAußerdem benötigen wir die Stichprobengröße, also die Anzahl der Akten die ausgewählt werden sollen. Die Stichprobengröße wird mit \\(n\\) bezeichnet.\nIm folgenden Code lesen wir zunächst die Anzahl der Akten aus7.\n\nN &lt;- akten %&gt;% \n  nrow()\n\nN\n\nDann legen wir den Stichprobenumfang \\(n\\) auf 200 fest8.\n\nn &lt;- 200\nn\n\nAls Auswahlwahrscheinlichkeit bezeichnet man die Wahrscheinlichkeit, mit der eine Einheit (hier eine Akte, technisch eine Zeile) in die Stichprobe gelangt9.\n\n100 * n/N\n\nMit welcher Wahrscheinlichkeit (in Prozent) wird eine Akte ausgewählt?\nNun können wir eine einfache Zufallsstichprobe ziehen10. Als Ergebnis erhalten wir \\(n=200\\) zufällig gewählte Werte aus der Spalte Index.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\nWenn wir den gleichen Code noch einmal ausführen, bekommen wir eine neue Zufallsstichprobe.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\n\n\n5.1 Exkurs: Startwerte für den Zufallszahlengenerator setzen\nUm die Ziehung reproduzierbar zu machen, können wir auch einen Startwert für den Zufallszahlengenerator setzen11.\n\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\nWenn wir den selben Startwert setzen, erhalten wir jetzt jedes Mal dieselbe Stichprobe.\n\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\n\nMit den Indizes in der Stichprobe können wir nun die entsprechenden Zeilen in unserem Datensatz akten auswählen. Das machen wir über einen einfachen Abgleich der Indizes12.\n\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\n\n\n\n5.2 Wie groß sollte unsere Stichprobe sein?\nAls einfache Regel gilt, je größer die Stichprobe, desto genauer können wir die Grundgesamtheit abbilden.\nWas bedeutet “genau”? Jede Wiederholung der Stichprobenziehung ergibt eine neue Stichprobe. Das heißt, die Akten, die Teil der Stichprobe sind, unterscheiden sich von Wiederholung zu Wiederholung. Berechnen wir nun, zum Beispiel, einen Mittelwert auf Grundlage der Stichprobe, unterscheiden sich die Ergebnisse der Berechnung. Je größer die Stichprobe ist, desto kleiner werden die Unterschiede und desto näher wird (in der Regel) das Stichprobenmittel dem Mittelwert aus der Grundgesamtheit kommen.\n\n\n5.3 Beispiel zur Stichprobengröße\nHier berechnen wir den mittleren Streitwert (auch Median genannt), um den in unserer Grundgesamtheit üblicherweise gestritten wird 13.\n\nround(median(akten$`Streitwert in EURO`, na.rm = TRUE))\n\nDies ist der wahre Wert aus der Grundgesamtheit, an dem üblicherweise Interesse besteht. Idealerweise könnte man alle Akten aufbewahren und so diesen Mittelwert (oder beliebige andere Maße) jederzeit neu berechnen.\nAllerdings kann nur eine Stichprobe gezogen/archiviert werden. Der mittlere Wert in der Stichprobe wird nur ungefähr dem Wert in der Grundgesamtheit entsprechen14.\n\nn &lt;- 200\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\nMit einer anderen Stichprobe erhalten wir einen anderen Wert.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\nWenn wir dies nun sehr oft (zum Beispiel 1000 mal) durchführen, können wir betrachten, wie genau wir den mittleren Wert aus unseren Stichproben schätzen15.\n\nverteilung_200 &lt;- sapply(1:1000, function(x) {\n  Index_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\n  akten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe,]\n  round(median(akten_stichprobe$`Streitwert in EURO`, na.rm = TRUE))\n})\n\nDiese Verteilung können wir dann visuell in einem sogenannten Histogram darstellen16.\n\nggplot(data.frame(Median = verteilung_200), aes(x = Median)) + \n  geom_histogram() + \n  theme_minimal()\n\nZum Vergleich ziehen wir nun zunächst zwei einfache Zufallsstichproben mit dem Stichprobenumfang \\(n = 1000\\).\n\nn &lt;- 1000\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\nDann wiederholen wir den Prozess wieder 1000 mal.\n\nverteilung_1000 &lt;- sapply(1:1000, function(x) {\n  Index_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\n  akten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe,]\n  round(median(akten_stichprobe$`Streitwert in EURO`, na.rm = TRUE))\n})\n\nDiese Verteilung können wir dann visuell in einem sogenannten Histogram darstellen.\n\nggplot(data.frame(Median = verteilung_1000), aes(x = Median)) + \n  geom_histogram() + \n  theme_minimal()\n\nWie erwartet sind die geschätzten Werte in den größeren Stichproben deutlich näher am Median der Grundgesamtheit (1134) als in den kleineren Stichproben. Eine größere Stichprobe führt zu verbesserter Genauigkeit.\n\n\n5.4 Graphische Darstellung\nNun wollen wir die einfache Zufallsstichprobe vergleichen, mit dem was aktuell angeboten wird. Dazu betrachten wir den Anteil angebotener Akten pro Gericht und stellen dies dar. Berechne zunächst die Anteile.\n\nanteil_anzubietend &lt;- akten %&gt;%\n  mutate(AkteAnzubieten = case_when(\n    `Archivstatus` == \"anzubieten\" ~ \"Ja\",\n    is.na(`Archivstatus`) ~ \"Nein\",\n    TRUE ~ \"Nein\")) %&gt;%\n  group_by(`Bezirk`, `AkteAnzubieten`, .drop = FALSE) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(anteil = n / sum(n)) %&gt;%\n  filter(`AkteAnzubieten` == \"Ja\")\n\nanteil_anzubietend\n\nNun stellen wir die Anteile auf einer Karte dar.\n\n# In Karte übernehmen\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anteil_anzubietend$anteil[match(karte_amtsgerichte$court, anteil_anzubietend$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\n# und Plotten\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nSchweinfurt und Kitzingen bieten einen besonders hohen Anteil an, mehr als 3 Prozent! Von vielen Gerichten wird aber auch keine einzige Akte angeboten.\nWoran liegt diese ungleiche Verteilung?\nNun sollen \\(n = 372\\) Akten (so viele wie 2018 angeboten wurden) zufällig gezogen werden. In dem Indikator inSample in akten_kopie wird abgespeichert, ob die Akte gezogen wurde oder nicht.\n\nn &lt;- 372\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\n\nakten_kopie &lt;- akten\nakten_kopie$inSample[akten$Index %in% Index_stichprobe] &lt;- \"Ja\"\n\nBerechne die Anteile der zufällig ausgewählten Akten in jedem Bezirk.\n\nanteil_anzubietend_zufaellig &lt;- akten_kopie %&gt;%\n  mutate(AkteAnzubieten = case_when(\n    `inSample` == \"Ja\" ~ \"Ja\",\n    is.na(`inSample`) ~ \"Nein\",\n    TRUE ~ \"Nein\")) %&gt;%\n  group_by(`Bezirk`, `AkteAnzubieten`, .drop = FALSE) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(anteil = n / sum(n)) %&gt;%\n  filter(`AkteAnzubieten` == \"Ja\")\n\nanteil_anzubietend_zufaellig\n\nAuf einer Karte können wir nun erneut darstellen, wie viele Akten angeboten werden wenn dies rein zufällig erfolgen würde.\n\n# in Karte übernehmen\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anteil_anzubietend_zufaellig$anteil[match(karte_amtsgerichte$court, anteil_anzubietend_zufaellig$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\n# und plotten\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nDie meisten Gerichte würden bei zufälliger Auswahl mindestens eine Akte anbieten. Man beachte aber auch die Skala: Es ist höchst unwahrscheinlich, dass ein Gericht auch nur 1% seiner Akten zur Archivierung anbieten würde."
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-einfachen-zufallsstichprobe",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-einfachen-zufallsstichprobe",
    "title": "ADA Bayern",
    "section": "6 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer einfachen Zufallsstichprobe?",
    "text": "6 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer einfachen Zufallsstichprobe?\n\nWelche Faktoren beeinflussen, wie groß die Stichprobe sein kann?\nAls Grundgesamtheit wurde hier “alle C-Verfahren in einem Jahr in Bayern” verwendet. Ist dies so sinnvoll und praktikabel oder gibt es bessere Alternativen?\nIst die einfache Zufallsstichprobe zur Auswahl von Akten geeignet? Welche Vor- und Nachteile hat das Verfahren?\n\n\n6.0.1 Vorteile\n\nkeine systematische Auswahl von Akten (z.B. anhand von Nachnamen)\nRepräsentative Stichprobe\nAlle Akten haben dieselbe Auswahlwahrscheinlichkeit\n\n\n\n6.0.2 Nachteile\n\nNicht alle Gerichte/Landkreise/Regionen tauchen immer in der Stichprobe auf\nAlle Akten haben dieselbe Auswahlwahrscheinlichkeit"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#footnotes",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#footnotes",
    "title": "ADA Bayern",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWickham H, Bryan J (2022). readxl: Read Excel Files. R package version 1.4.0, https://CRAN.R-project.org/package=readxl.↩︎\nLumley T (2010). Complex Surveys: A Guide to Analysis Using R: A Guide to Analysis Using R. John Wiley and Sons.↩︎\nValliant, R., Dever, J., Kreuter, F. (2018). Practical Tools for Designing and Weighting Survey Samples, 2nd edition. New York: Springer.↩︎\nFalls Sie auf Ihrem eigenen Rechner ein neues Paket installieren wollen, können Sie das mit der Funktion install.packages(). Im Funktionsaufruf müssen Sie dann nur den Paketnamen angeben. Wenn Sie also, zum Beispiel, das Paket PracTools installieren wollen, lautet der Funktionsaufruf: install.packages(\"PracTools\").↩︎\nDie Forumstar Daten liegen uns in einer .csv Datei vor. Diese können wir mit der Funktion read_csv einlesen. Zunächst definieren wir den Pfad zum Ordner mit den Daten im Objekt pfad, dafür nutzen wir die Funktion file.path(), dort geben wir Schritt für Schritt den Weg zum Ordner mit den Daten an. Dann definieren wir dafür die verschiedenen Datentypen in jeder Spalte. Ein c steht für character, also Textdaten, wie zum Beispiel in Namensfeldern. n steht für numerische Werte, T steht für eine Datumsangabe mit Uhrzeit (von “Time”). Diese Informationen speichern wir im Objekt col_types. Anschließend lesen wir den Datensatz mit der Funktion read_csv() direkt aus dem Ordner in unserer sicheren Datenumgebung ein. Wir können die Daten dann später über das Objekt forumstar_daten aufrufen.\nWir lesen die Kartendaten ebenfalls gleich ein.↩︎\nDazu verwenden wir die group_by Funktion. Zusätzlich erstellen wir mithilfe der Funktion summarise()eine neue Spalte Anzahl Beteiligte. Anschließend fügen wir noch eine eindeutige Index Spalte Index für jede Akte hinzu.↩︎\nWir nutzen dafür nrow() Funktion um die Anzahl (n) der Zeilen (row) im Datensatz akten zählen.↩︎\nWir generieren ein neues Objekt n dem wir den Wert 200 zuweisen.↩︎\nDafür berechnen wir den Quotienten \\(n\\)/\\(N\\) und multiplizieren in mit 100 um Prozentangaben zu bekommen.↩︎\nDafür verwenden wir die Funktion sample() (das englische “to draw a sample” bedeutet “eine Stichprobe ziehen”). Wir nehmen die Einträge in der Index Spalte und ziehen eine Zufallsstichprobe mit der Stichprobengröße \\(n\\) (size = n) ohne Zurücklegen (replace = FALSE).↩︎\nDafür nutzen wir die Funktion set.seed().↩︎\nMit akten$Index %in% Index_stichprobe wird einzeln für jede Zeile (akten$Index) überprüft, ob diese Zeile in (Operator %in%) der gezogenen Stichprobe (Index_stichprobe) ist. Mithilfe der eckigen Klammern (akten[c(TRUE, FALSE, …, FALSE),]) werden nur diejenigen Zeilen aus der akten-Tabelle zurückgegeben, wo dies der Fall ist. Das Ergebnis der Auswahl sind die 200 Akten in unserer Stichprobe.↩︎\nWir berechnen den Median mit der Funktion median() . na.rm = TRUE gibt an, dass Akten bei der Berechnung ignoriert werden, wenn dort kein Wert angegeben ist. Andernfalls können wir keine Berechnung durchführen.↩︎\nDer hier verwendete Code wurde im Detail bereits oben vorgestellt.↩︎\nMit der Funktion sapply() wenden wir eine Funktion wiederholt, hier über den Vektor der Zahlen von 1 bis 1000 (1:1000) an. Die Funktion die wir anwenden sind die drei Schritte aus den vorherigen Beispielen um eine Stichprobe mit Umfang \\(n\\) = 200 zu ziehen.↩︎\nDafür nutzen wir die Funktion ggplot. Zunächst müssen wir unsere Verteilung als data.frame an die Funktion übergeben, dabei benennen wir die Spalte Median. Diese Spalte lassen wir uns dann anzeigen. Mit geom_histogram() definieren wir, dass wir ein Histogram erstellen möchten. theme_minimal() verändert das Aussehen der visuellen Darstellung. (Sie können gerne ausprobieren, was ohne theme_minimal() passiert.)↩︎"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir zunächst Quarto und R kennen. Anschließend führen wir erste Analysen mit den forumSTAR Daten durch. Das Ziel dabei ist es, insbesondere, die Daten gut zu verstehen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#ziel-des-notebooks",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#ziel-des-notebooks",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir zunächst Quarto und R kennen. Anschließend führen wir erste Analysen mit den forumSTAR Daten durch. Das Ziel dabei ist es, insbesondere, die Daten gut zu verstehen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quarto-und-r",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quarto-und-r",
    "title": "ADA Bayern",
    "section": "2 Quarto und R",
    "text": "2 Quarto und R\nDieses Dokument ist ein sogennantes Quarto Dokument. Wir können Quarto Dokumente an der Dateiendung .qmd erkennen und direkt in RStudio öffnen.\nJedes Quarto Dokument besteht im Wesentlichen aus vier Bausteinen. Header, Text, Code Eingabe und Code Ausgabe.\n\n2.1 Header\nDer Header steht immer am Anfang eines Quarto Dokuments. Hier definieren wir den Titel und Untertitel unseres Dokuments. Außerdem können wir hier einstellen wie aus dem vorliegenden .qmd Dokument zum Beispiel ein pdf oder docx Dokument generiert werden soll. Mehr dazu lernen wir in Modul 3.\n\n\n2.2 Text\nDas hier ist ganz normaler Text. Wie in anderen Editoren (zum Beispiel Word), können wir den Text nach unserem Geschmack formatieren.\n\n\n2.3 Code Eingabe\nMit einem Klick auf den grünen Button (C mit kleinem +-Symbol) oben in der Leiste erstellen wir eine R Code Zelle. In dieser können wir ausführbaren R Code schreiben. Mit einem Klick auf das grüne Dreieck rechts an der Code Zelle führen wir den Code aus1.\n\n1 + 1\n\nDabei ist Quarto nicht auf die Programmiersprache R beschränkt. Wir könnten, zum Beispiel, auch ausführbaren Python Code in ein Quarto Dokument integrieren.\n\n\n2.4 Code Ausgabe\nSobald wir Code ausführen, erscheint das Ergebnis (wie oben nach einem Klick auf das grüne Dreieck) ebenfalls direkt im Quarto Dokument."
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#software-pakete-laden",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#software-pakete-laden",
    "title": "ADA Bayern",
    "section": "3 Software-Pakete laden",
    "text": "3 Software-Pakete laden\nZunächst laden wir einige R-Pakete, die uns bei der Datenanalyse helfen.\nR-Pakete sind Erweiterungen, Sammlungen von nützlichem Code die im CRAN (Comprehensive R Archive Network) zur Verfügung gestellt werden.\nIn diesem Modul nutzen wir das sogenannte tidyverse2, das viele nützliche und einfach verständliche (tidy!) Funktionen zum Arbeiten mit Datensätzen bereitstellt.\nAußerdem nutzen wir das Paket sf3 Paket um die Daten auf Karten zu visualisieren4.\n\nlibrary(\"tidyverse\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#daten-einlesen",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#daten-einlesen",
    "title": "ADA Bayern",
    "section": "4 Daten einlesen",
    "text": "4 Daten einlesen\nUm unsere Daten in R nutzen zu können, müssen wir diese zunächst einlesen5.\n\npfad &lt;- file.path(\".\")\ncol_types &lt;- c(\"ccccnccccccccccccccccccTTcnTccnTcTcccc\")\n\nforumstar_daten &lt;- read_csv(file.path(pfad, \"230817_Abfrage_Januar-Dezember.csv\"), col_types = col_types)\n\nZusätzlich laden wir eine Datei mit einer Karte der Amtsgerichtsbezirke. Diese verwenden wir später für erste Analysen.\n\nkarte_amtsgerichte &lt;- readRDS(file.path(pfad, \"230911_Karte_Amtsgerichtsbezirke.RDS\"))"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#überblick-über-die-daten",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#überblick-über-die-daten",
    "title": "ADA Bayern",
    "section": "5 Überblick über die Daten",
    "text": "5 Überblick über die Daten\nUm die Daten besser zu verstehen, macht es Sinn diese zu betrachten. Aber wie machen wir das in R? Es gibt viele Möglichkeiten, wovon wir hier eine sehr gute Möglichkeit beispielhaft zeigen6.\n\nforumstar_daten %&gt;% \n  glimpse()\n\nWas sind die Namen der Spalten des Datensatzes?\nIm Datensatz sind vier Spalten mit Zahlenwerten enthalten. Welche sind das?\nAußerdem können wir uns die ersten Zeilen des Datensatzes ansehen. Mit einem Klick auf das schwarze Dreieck rechts über der Tabelle können wir die weiteren Spalten sehen7.\n\nforumstar_daten %&gt;% \n  head()\n\nFür die Analysen ist es wichtig zu verstehen, was eine Zeile im Datensatz abbildet. Was bildet eine Zeile in unserem Datensatz ab? Betrachten Sie dazu die Spalten Aktenzeichen, Verfahrensbeteiligungsart und Name8.\n\nforumstar_daten %&gt;% \n  select(`Aktenzeichen`, `Verfahrensbeteiligungsart`, `Name`)\n\nAktenzeichen tauchen mehrmalig auf. Was bedeutet das?"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#erste-analysen",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#erste-analysen",
    "title": "ADA Bayern",
    "section": "6 Erste Analysen",
    "text": "6 Erste Analysen\n\n6.1 Welche Gerichte sind im Datensatz?\nMöchten wir wissen, welche und wie viele Gerichte in unserem Datensatz vertreten sind, können wir dies auch erfahren9.\n\nforumstar_daten %&gt;%\n  select(`Gericht`) %&gt;%\n  unique()\n\n\n\n6.2 Wie viele Akten haben wir pro Gericht?\nWir möchten nun wissen, mit wie vielen Akten jedes Gericht im Datensatz vertreten ist. Wir benötigen hierfür einen Datensatz, der nur eine Zeile pro Akte beinhaltet (statt zuvor einer Zeile pro Prozessbeteiligten) beinhaltet. Mit dem folgenden Code aggregieren wir die Daten auf einen Eintrag pro Gericht und Aktenzeichen10.\n\nforumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nZusätzlich wollen wir noch weitere Informationen behalten, die für jede Akte vorliegen, aber nicht personenspezifisch sind. Die entsprechenden Spaltennamen nehmen wir in group_by() auf. Das Ergebnis speichern wir in einem neuen Datensatz akten zur weiteren Analyse ab.\n\nakten &lt;- forumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`,\n    `Streitwert in EURO`,\n    `Gesamtstreitgegenstand`,\n    `Erledigungsgrund`,\n    `Dauer des Verfahrens in Tagen`,\n    `Archivstatus`,\n    `Anbietungsgrund (manuell erfasst)`,\n    `Anbietungsgrund`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nNun können wir die Anzahl der Akten pro Amtsgericht (bzw. Zweigstelle) zählen11.\n\nakten %&gt;% \n  group_by(Gericht) %&gt;% \n  summarise(Anzahl = n())\n\nDas gleiche können wir auch visuell machen12.\n\nggplot(akten, aes(Gericht)) + \n  geom_bar() + \n  scale_x_discrete(guide = guide_axis(angle = 90))\n\nDatenprobleme erkennen: Macht es Sinn die Zweigstellen gesondert aufzuführen?\nWir können uns auch eine numerische Zusammenfassung des Datensatzes anzeigen lassen13.\n\nakten %&gt;% \n  summary()\n\nDatenprobleme erkennen: Ist es möglich, dass die tatsächliche Dauer des Verfahrens in Tagen negativ ist? Wie viele Jahre dauert ein Verfahren mit 17164 Tagen? Ist das ein realistischer Zeitraum?"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#die-daten-visuell-kennenlernen",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#die-daten-visuell-kennenlernen",
    "title": "ADA Bayern",
    "section": "7 Die Daten visuell kennenlernen",
    "text": "7 Die Daten visuell kennenlernen\nWir können die Daten auch auf einer Karte darstellen. Wir haben dazu eine Karte mit den bayerischen Amtsgerichtsbezirken vorbereitet.\nDie Karte ist zunächst ein zusätzlicher Datensatz, den wir oben in R eingelesen haben. Wie bei den forumSTAR Daten können wir uns zunächst einen Überblick verschaffen14.\n\nkarte_amtsgerichte %&gt;%\n  glimpse()\n\nIn diesem Datensatz haben wir drei Variablen. court ist der Name des Amtsgerichtsbezirks, geometry definiert die Grenzen der einzelnen Amtsgerichtsbezirke und tile_map ist eine alternative Darstellung der Amtsgerichtsbezirke als gleich große Kacheln (sogenannte tiles).\nBei einer Karte macht es natürlich besonders Sinn die Daten zu visualisieren15.\n\nggplot(data = karte_amtsgerichte, aes(fill = court)) +\n  geom_sf(aes(geometry = geometry), color = \"white\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    color = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) + scale_fill_viridis_d(option = \"viridis\") +\n  guides(fill=\"none\") +\n  theme_void()\n\nAufgabe: Visualisieren Sie die Amtsgerichtsbezirke mit den gleich großen Kacheln (tile_map) anstatt der geographischen Grenzen (geometry) . Als Starthilfe nehmen wir den gleichen Code wie für die Karte oben. Sie müssen die Spalte geometry an zwei Stellen austauschen.\n\nggplot(data = karte_amtsgerichte, aes(fill = court)) +\n  geom_sf(aes(geometry = tile_map), color = \"white\") +\n  geom_sf_text(\n    aes(geometry = tile_map, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) + scale_fill_viridis_d(option = \"viridis\") +\n  guides(fill=\"none\") +\n  theme_void()\n\nNun wollen wir die Anzahl der Akten pro Gericht auf der Karte darstellen. Dazu müssen wir die Gerichte in unserem Datensatz akten den Amtsgerichtsbezirken im Datensatz karte_amtsgerichte eindeutig zuordnen.\nWenn wir die Namen der Gerichte in den beiden Datensätzen betrachten, fällt auf, dass die Namen unterschiedlich sind.\nZunächst betrachten wir die Namen in unserem Datensatz akten 16.\n\nakten %&gt;% \n  select(`Gericht`) %&gt;%\n  unique()\n\nDann betrachten wir die Namen der Amtsgerichtsbezirke im Datensatz karte_amtsgerichte 17.\n\nkarte_amtsgerichte$court\n\nDatenprobleme erkennen: Wie können wir die verschiedenen Namen der Gerichte automatisiert zusammenführen?\nZunächst löschen wir das “Amtsgericht” vor dem Namen des Amtsgerichtsbezirks18.\n\ntmp_Gericht &lt;- sub(\"Amtsgericht \", \"\", akten$Gericht)\n\ntmp_Gericht %&gt;% \n  unique()\n\nGenauso können wir das Wort “Zweigstelle” (und alle folgenden Buchstaben) aus dem Namen entfernen19.\n\ntmp_Gericht &lt;- sub(\" Zweigstelle.*\", \"\", tmp_Gericht)\n\ntmp_Gericht %&gt;% \n  unique()\n\nNun müssen wir noch leicht unterschiedliche Schreibweisen anpassen. In forumstar_daten steht zum Beispiel die Abkürzung “i.d”, in den karten_amtsgerichte Daten dagegen “i. d.”. Für uns ist trotz der leicht unterschiedlichen Schreibweise eine eindeutige Zuordnung möglich. Für den Computer ist das hier schwieriger. Das “i.d.” können wir auch einfach durch ein “i. d.” austauschen. Wir wiederholen das dann für alle anderen Abkürzungen mit unterschiedlichen Schreibweisen20.\n\ntmp_Gericht &lt;- sub(\" i.d. \", \" i. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" a.d. \", \" a. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" am \", \" a. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.OB\", \" i. OB\", tmp_Gericht)\n\ntmp_Gericht %&gt;% \n  unique()\n\nNun können wir die neuen Namen der Gerichte als eine neue Spalte mit dem Namen Bezirk zu unserem Datensatz akten hinzufügen21.\n\nakten &lt;- akten %&gt;%\n  mutate(`Bezirk` = tmp_Gericht)\n\nDie neue Spalte Bezirk` können wir nutzen um die Daten auf der Ebene der Amtsgerichtsbezirke zu aggregieren. Zweigstellen sind nun ihrem Bezirk zugeordnet22.\n\nfallzahl_pro_bezirk &lt;- akten %&gt;% \n  group_by(Bezirk) %&gt;% \n  summarise(Anzahl = n()) %&gt;%\n  as.data.frame()\n\nDie aggregierten Daten stellen fügen wir dann als Information zu unserem Datensatz karte_amtsgerichte hinzu23.\n\nkarte_amtsgerichte$`Anzahl der Fälle` &lt;- fallzahl_pro_bezirk$Anzahl[match(karte_amtsgerichte$court, fallzahl_pro_bezirk$Bezirk)]\n\nJetzt visualisieren wir die Anzahl der Fälle pro Bezirk auf einer Karte24.\n\nggplot(data = karte_amtsgerichte, aes(fill = `Anzahl der Fälle`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nIm Amtsgerichtsbezirk München ist wirklich einiges los! Sogar so viel mehr, dass wir bei den meisten Gerichten mit weniger als 5000 Akten auf der Karte gar keinen Unterschied sehen können.\nFür so “schiefe Daten” bietet sich eine logarithmische Transformation der Farbskala an25.\n\nggplot(data = karte_amtsgerichte, aes(fill = `Anzahl der Fälle`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(trans = \"log\", option = \"viridis\") +\n  theme_void()\n\n\n7.1 Welche Akten sind anzubieten?\nEine der zentralen Fragen in unserem Projekt ist: welche Verfahrensakten sollen den Archiven angeboten werden und warum? Rechtspfleger:innen geben manchmal eine Begründung ein, wenn sie entscheiden, dass ein Fall archiviert werden sollte. Um diese Begründungen anzuzeigen, erstellen wir zunächste einen kleineren Datensatz mit nur denjenigen Akten, die von den Rechtspfleger:innen als anzubieten gekennzeichnet wurden26.\n\nanzahl_anzubietend &lt;- akten %&gt;% \n  group_by(`Bezirk`, .drop = FALSE) %&gt;%\n  filter(`Archivstatus` == \"anzubieten\") %&gt;%\n  count(name = \"Anzahl\") %&gt;%\n  as.data.frame()\n  \nanzahl_anzubietend\n\nWir können die Gesamtzahl der anzubietenden Akten durch das aufsummieren der anzubietenden Akten pro Gericht herausfinden.\n\nanzahl_anzubietend %&gt;%\n  select(Anzahl) %&gt;%\n  sum()\n\nUm die Daten besser zu verstehen, betrachten wir diese wieder in einer grafischen Darstellung.\n\nggplot(anzahl_anzubietend, aes(Bezirk, Anzahl)) + \n  geom_col() + \n  scale_x_discrete(guide = guide_axis(angle = 90))\n\nUnd können die Daten auch auf einer Karte darstellen.\n\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anzahl_anzubietend$Anzahl[match(karte_amtsgerichte$court, anzahl_anzubietend$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nWas fällt auf?\nNur 48 der 73 Amtsgerichtsbezirke haben in unserem Datensatz Akten mit Archivstatus “anzubieten”. Die Anzahl der anzubietenden Akten scheint nicht mit der Anzahl der Akten zu korrelieren.\nWoran liegt das?\nWarum sollen diese Akten angeboten werden? In zwei Spalten in unserem Datensatz haben wir Informationen zum Anbietungsgrund. Es gibt die Spalte “Anbietungsgrund”, sowie die Spalte “Anbietungsgrund (manuell erfasst)”. Wir erstellen uns je eine Tabelle, die uns sagt, welcher Grund wie oft angegeben wurde27.\n\nakten %&gt;% \n  filter(`Archivstatus` == \"anzubieten\") %&gt;%\n  group_by(Anbietungsgrund) %&gt;% \n  summarise(Anzahl = n())\n\nDas gleiche machen wir mit der Spalte Anbietungsgrund (manuell erfasst) 28.\n\nakten %&gt;% \n  filter(`Archivstatus` == \"anzubieten\") %&gt;%\n  group_by(`Anbietungsgrund (manuell erfasst)`) %&gt;% \n  summarise(Anzahl = n())\n\nNA steht hier für “not available” (nicht verfügbar), d. h. für alle Zeilen, bei denen kein Grund angegeben wurde."
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quantitative-informationen-darstellen",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quantitative-informationen-darstellen",
    "title": "ADA Bayern",
    "section": "8 Quantitative Informationen darstellen",
    "text": "8 Quantitative Informationen darstellen\nWir können auch die quantitativen Informationen visuell darstellen und so die Daten besser verstehen. Zunächst betrachten wir den Streitwert in EURO und die Anzahl Beteiligte 29.\n\nggplot(akten, aes(x=`Streitwert in EURO`, y=`Anzahl Beteiligte`)) + \n  geom_point()\n\nGenauso können wir die Dauer des Verfahrens in Tagen und die Anzahl Beteiligte betrachten. Tauschen Sie dazu eine Spalte im Code für die visuelle Darstellung aus30.\n\nggplot(akten, aes(x=`Dauer des Verfahrens in Tagen`, y=`Anzahl Beteiligte`)) + \n  geom_point()"
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#aufgaben",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#aufgaben",
    "title": "ADA Bayern",
    "section": "9 Aufgaben",
    "text": "9 Aufgaben\nWelche Spalten im Datensatz finden Sie besonders spannend? Welche Spalten könnten sich gut zur Auswahl archivwürdiger Akten eignen?\nSuchen Sie sich gemeinsam mit ihrem Team mindestens zwei Spalten aus und stellen Sie diese visuell dar."
  },
  {
    "objectID": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#footnotes",
    "href": "workshops/ws2/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#footnotes",
    "title": "ADA Bayern",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDas {r} definiert, das die Code Eingabe R Code erwartet. Im Code addieren wir dann 1 und 1.↩︎\nWickham H, Bryan J (2022). readxl: Read Excel Files. R package version 1.4.0, https://CRAN.R-project.org/package=readxl.↩︎\nPebesma, E., & Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016↩︎\nIm Code rufen wir zunächst mit der Funktion install.packages(), ein R Paket zu installieren, bedeutet, dass wir die Funktionen im Paket auf unserem Computer verfügbar machen. install.packages() müssen wir nur ausführen, wenn wir ein R Paket erstmalig installieren oder updaten möchten. Anschließend müssen wir die Funktionen noch für unser spezifisches Dokument verfügbar machen. Dazu rufen wir die Funktion library() für jedes Paket auf. library() führen wir immer aus, wenn wir eine neue R Session starten.↩︎\nDie Forumstar Daten liegen uns in einer .csv Datei vor. Diese können wir mit der Funktion read_csv einlesen. Zunächst definieren wir den Pfad zum Ordner mit den Daten im Objekt pfad, dafür nutzen wir die Funktion file.path(), dort geben wir Schritt für Schritt den Weg zum Ordner mit den Daten an. Dann definieren wir dafür die verschiedenen Datentypen in jeder Spalte. Ein c steht für character, also Textdaten, wie zum Beispiel in Namensfeldern. n steht für numerische Werte, T steht für eine Datumsangabe mit Uhrzeit (von “Time”). Diese Informationen speichern wir im Objekt col_types. Anschließend lesen wir den Datensatz mit der Funktion read_csv() direkt aus dem Ordner in unserer sicheren Datenumgebung ein. Wir können die Daten dann später über das Objekt forumstar_daten aufrufen.↩︎\nMit glimpse() (“Glimpse” bedeutet auf Deutsch “kurzer Blick”.) erhält man die Anzahl an Zeilen (Rows) und Spalten (Columns) der Daten und kann die Spalten (Variablen) besser verstehen. In den folgenden Zeilen werden die Spaltennamen aufgelistet. Dahinter sehen wir den Datentyp jeder Spalte (&lt;chr&gt; für “character” also Text, &lt;dbl&gt; für Zahlen im “double-precision flouting-point format” und &lt;dttm&gt; für “date-time” also Datumsangaben) und einen ersten Überblick über einige der ersten Zellen jeder Spalte. NA bedeutet dabei, dass eine Zelle in unserem Datensatz leer ist.↩︎\nMit der Funktion head() können wir uns die ersten sechs Zeilen des Datensatzes ansehen. Analog könnten wir mit der Funktion tail() die letzten sechs Zeilen des Datensatzes ansehen.↩︎\nWir nutzen die Funktion select() um Spalten aus unserem Datensatz auszuwählen (auswählen auf Englisch “to select”). Hier wählen wir also die Spalten Aktenzeichen, Verfahrensbeteiligungsart und Name aus.↩︎\nWir nutzen die Funktion select() um die Spalte Gericht auszuwählen. Damit wir nicht alle Zeilen erhalten, sondern jedes Gericht nur einmal, können wir die Funktion unique() (“unique” auf Englisch bedeutet einmalig).↩︎\nMithilfe der group_by Funktion werden die nachfolgenden Schritte (hier nur ein Schritt: summarise) getrennt für jede Kombination aus Gericht und Aktenzeichen ausgeführt. Das heißt in summarise() werden alle Zeilen, die dasselbe Gericht und Aktenzeichen haben, in einer einzigen Zeile zusammengefasst. Zusätzlich zählen wir die Anzahl Zeilen n(). Diese Zahl wird in einer neuen Spalte Anzahl Beteiligte gespeichert.\nAnschließend fügen wir noch mit mutate() eine eindeutige Index Spalte Index anhand der Zeilennummer für jede Akte hinzu. Dafür muss zunächst mit as.data.frame() die Gruppierung aufgehoben werden.↩︎\nDafür gruppieren wir unsere Daten nach der Spalte Gericht und nutzen dann die summarise() Funktion innerhalb jeder Gruppe. Mit der Funktion n() zählen wir so die Anzahl der Zeilen pro Gruppe.↩︎\nggplot() ist eine Funktion zur visuellen Darstellung von Daten (auf Englisch “to plot”), bei der mit +-Symbolen die Details der Darstellung bestimmt werden. geom_bar() erstellt einen Bar-Plot; mit scale_x_discrete(guide = guide_axis(angle = 90)) drehen wir die Namen der Gerichte um 90 Grad, damit diese sich nicht überlappen. Sie können ausprobieren, was passiert wenn Sie den letzten Befehl sowie das letzte +-Symbol weglöschen.↩︎\nDazu nutzen wir die Funktion summary(). Diese Funktion fasst numerische Spalten mit einfachen statistischen Werten zusammen. So können wir das Minimum, das erste Quartil, den Median, den Mittelwert (auf Englisch “mean”), das dritte Quartil, das Maximum der Daten, sowie die Anzahl fehlender Werte (NA’s) auf einen Blick sehen.↩︎\nWir nutzen dafür wieder die Funktion glimpse().↩︎\nZur visuellen Darstellung Nutzen wir wieder die ggplot() Funktion. Hier geben wir als Ausgangsdaten (data) den Datensatz mit der Karte karte_amtsgerichte an, mit aes(fill = court) geben wir an, dass wir jeden Gerichtsbezirk unterschiedlich farbig füllen (auf Englisch “fill”) möchten. aes steht dabei für “aesthetics”. Nun nutzen wir geom_sf() um die Karte anzuzeigen, dafür geben wir an, dass die Informationen zur Geometrie der Bezirke (Option geometry) in der Spalte geometry unseres Datensatzes liegen. Grenzen zwischen Bezirken zeigen wir in der Farbe weiß an (color = \"white\"). Zusätzlich zeigen wir mit geom_sf_text() noch die Namen der Amtsgerichtsbezirke auf der Karte an. Um diese an der richtigen Stelle zu platzieren nutzen wir wieder die Informationen in der Spalte geometry. Für die Beschriftung verwenden wir die Spalte court. Mit der Option size = 1.5 geben wir die Größe der Schrift an und mit der Option color = \"black\" geben wir an, dass wir die Beschriftung in schwarz darstellen. Um nun die Labels richtig zu platzieren, berechnen wir den Mittelpunkt der Amtsgerichtsbezirke mit der Funktion st_centroid(). Diese Funktion wird auf Basis der Information in der Spalte geometry berechnet. Anschließend definieren wir mit sclae_fill_viridis_d(option = \"viridis\") die Farbskala mit der wir die verschiedenen Amtsgerichtsbezirke einfärben möchten (mehr dazu in Modul 3). Mit guides(fill=\"none\") unterdrücken wir die Erstellung einer Farblegende. Diese würde in diesem Fall keine Information hinzufügen, dabei aber sehr viel Platz einnehmen. Zuletzt definieren wir mit theme_void(), dass wir die Karte ohne Koordinatensystem und Hintergrund abbilden möchten.↩︎\nDazu wählen wir zunächst mit der Funktion select() die Spalte Gericht aus und zeigen dann mit der Funktion unique() die einmaligen Werte an.↩︎\nHier nutzen wir den $ Operator, mit dem wir Spalten in einem Datensatz anwählen können. Die Namen der Amtsgerichtsbezirke sind in der Spalte court angeführt.↩︎\nDazu nutzen wir die Funktion sub(). Zunächst geben wir an, welche Zeichenfolge wir suchen und ersetzen möchten, hier die Zeichenfolge “Amtsgericht”. Dann geben wir an, durch welche Zeichenfolge wir die erste Zeichenfolge ersetzen möchten, hier durch eine leere Zeichenfolge ““. Als letztes geben an, wir welche Werte wir verändern möchten, hier möchten wir direkt die Einträge in der Spalte Gericht verändern. Mit dem $ können wir die Spalte in unserem Datensatz akten auswählen. Wir speichern die angepassten Namen im Objekt tmp_Gericht.\nDanach betrachten wir das Ergebnis, indem wir mit der Funktion unique() jeden Namen einmal anzeigen.↩︎\nDafür nutzen wir wieder die Funktion sub(). Hier wollen wir die Zeichenfolge “Zweigstelle.*” durch die leere Zeichenfolge “” ersetzen. Das .* gibt an, dass der Zeichenfolge “Zweigstelle” beliebige Zeichen folgen können, die trotzdem mit ausgetauscht werden.↩︎\nDafür nutzen wir wieder die Funktion sub(). Hier tauschen wir zum Beispiel die Zeichenfolge ” i.d. ” durch ” i. d. ” aus. Die Leerzeichen vor und nach den Buchstaben und Punkten sind wichtig! Leerzeichen gehören auch zur Zeichenfolge.↩︎\nDazu nutzen wir die Funktion mutate` mit der wir neue Spalten zu unserem Datensatz hinzufügen können. Wir nennen die Spalte Bezirk` und ordnen unsere aufgeräumten Namen von oben zu.↩︎\nWir erstellen hier ein neues Objekt fallzahl_pro_bezirk . Dazu gruppieren wir zunächst unseren Datensatz nach der Spalte Bezirk und zählen dann mit den Funktionen summarise() und n() die Anzahl der Akten pro Bezirk. Mit as.data.frame() speichern wir diese Information als einen neuen Datensatz.↩︎\nHier erstellen wir die neue Spalte Anzahl der Fälle mit dem $ Operator. Dafür nutzen wir unseren gerade erstellten Datensatz fallzahl_pro_bezirk und die Spalte Anzahl . Mit der Funktion match() stellen wir sicher, dass die Daten in beiden Datensätzen richtig sortiert sind.↩︎\nZur visuellen Darstellung Nutzen wir wieder die ggplot() Funktion. Hier geben wir als Ausgangsdaten (data) den Datensatz mit der Karte karte_amtsgerichte an, mit aes(fill = Anzahl der Fälle) geben wir an, dass wir Farbabstufungen nach der Anzahl der Fälle (auf Englisch “fill”) möchten. aes steht dabei für “aesthetics”. Nun nutzen wir geom_sf() um die Karte anzuzeigen, dafür geben wir an, dass die Informationen zur Geometrie der Bezirke (Option geometry) in der Spalte geometry unseres Datensatzes liegen. Grenzen zwischen Bezirken zeigen wir nicht an (color = \"transparent\"). Zusätzlich zeigen wir mit geom_sf_text() noch die Namen der Amtsgerichtsbezirke auf der Karte an. Um diese an der richtigen Stelle zu platzieren nutzen wir wieder die Informationen in der Spalte geometry. Für die Beschriftung verwenden wir die Spalte court. Mit der Option size = 1.5 geben wir die Größe der Schrift an und mit der Option color = \"black\" geben wir an, dass wir die Beschriftung in schwarz darstellen. Um nun die Labels richtig zu platzieren, berechnen wir den Mittelpunkt der Amtsgerichtsbezirke mit der Funktion st_centroid(). Diese Funktion wird auf Basis der Information in der Spalte geometry berechnet. Anschließend definieren wir mit sclae_fill_viridis_d(option = \"viridis\") die Farbskala mit der wir die verschiedenen Amtsgerichtsbezirke einfärben möchten (mehr dazu in Modul 3). Zuletzt definieren wir mit theme_void(), dass wir die Karte ohne Koordinatensystem und Hintergrund abbilden möchten.↩︎\nZur visuellen Darstellung Nutzen wir wieder die ggplot() Funktion. Hier geben wir als Ausgangsdaten (data) den Datensatz mit der Karte karte_amtsgerichte an, mit aes(fill = Anzahl der Fälle) geben wir an, dass wir Farbabstufungen nach der Anzahl der Fälle (auf Englisch “fill”) möchten. aes steht dabei für “aesthetics”. Nun nutzen wir geom_sf() um die Karte anzuzeigen, dafür geben wir an, dass die Informationen zur Geometrie der Bezirke (Option geometry) in der Spalte geometry unseres Datensatzes liegen. Grenzen zwischen Bezirken zeigen wir nicht an (color = \"transparent\"). Zusätzlich zeigen wir mit geom_sf_text() noch die Namen der Amtsgerichtsbezirke auf der Karte an. Um diese an der richtigen Stelle zu platzieren nutzen wir wieder die Informationen in der Spalte geometry. Für die Beschriftung verwenden wir die Spalte court. Mit der Option size = 1.5 geben wir die Größe der Schrift an und mit der Option color = \"black\" geben wir an, dass wir die Beschriftung in schwarz darstellen. Um nun die Labels richtig zu platzieren, berechnen wir den Mittelpunkt der Amtsgerichtsbezirke mit der Funktion st_centroid(). Diese Funktion wird auf Basis der Information in der Spalte geometry berechnet. Anschließend definieren wir mit sclae_fill_viridis_d(trans = \"log\", option = \"viridis\") die Farbskala mit der wir die verschiedenen Amtsgerichtsbezirke einfärben möchten (mehr dazu in Modul 3). Mit trans = \"log\" geben wir hier an, dass wir die Werte in der Spalte Anzahl der Fälle logarithmisch transformieren um die Grenzen zwischen den verschiedenen Farben neu zu definieren. Zuletzt definieren wir mit theme_void(), dass wir die Karte ohne Koordinatensystem und Hintergrund abbilden möchten.↩︎\nMit filter() erstellen wir den Datensatz akten_anzubieten mit allen Akten, bei denen der Archvstatus anzubieten lautet.\n.drop = FALSE wurde hier verwendet, damit auch Bezirke mit 0 Akten im resultierenden Dataframe erhalten bleiben.↩︎\nWir nutzen hier unseren Datensatz akten . Mit der Funktion filter(Archivstatus == \"anzubieten\") wählen wir zunächst nur Zeilen aus die den Status anzubieten haben. Diese Daten gruppieren wir mit der group_by() Funktion nach den Gründen die in der Spalte Anbietungsgrund angeführt sind. Mit der Funktion summarise(Anzahl = n() zählen wir dann die Anzahl der Zeilen pro Gruppe.↩︎\nWir nutzen hier unseren Datensatz akten . Mit der Funktion filter(Archivstatus == \"anzubieten\") wählen wir zunächst nur Zeilen aus die den Status anzubieten haben. Diese Daten gruppieren wir mit der group_by() Funktion nach den Gründen die in der Spalte Anbietungsgrund (maniell erfasst) angeführt sind. Mit der Funktion summarise(Anzahl = n() zählen wir dann die Anzahl der Zeilen pro Gruppe.↩︎\nZur visuellen Darstellung der Daten nutzen wir wieder die Funktion ggplot(). Als Daten nutzen wir den Datensatz akten. In der Option aes() definieren wir, welche Spalte auf der x-Achse und welche Spalte auf der y-Achse angezeigt werden soll. Hier wählen wir die Spalte Streitwert in EURO für die x-Achse und die Spalte Anzahl Beteiligte für die y-Achse. Mit geom_point() geben wir an, dass die Daten mit Punkten im Koordinatensystem abgetragen werden sollen.↩︎\nZur visuellen Darstellung der Daten nutzen wir wieder die Funktion ggplot() als Daten nutzen wir den Datensatz akten in der Option aes() definieren wir, welche Spalte auf der x-Achse und welche Spalte auf der y-Achse angezeigt werden soll. Hier wählen wir die Spalte Dauer des Verfahrens in Tagen für die x-Achse und die Spalte Anzahl Beteiligte für die y-Achse. Mit geom_point() geben wir an, dass die Daten mit Punkten im Koordinatensystem abgetragen werden sollen.↩︎"
  },
  {
    "objectID": "workshops/ws1/WS1_Modul_2.html",
    "href": "workshops/ws1/WS1_Modul_2.html",
    "title": "Modul 2: Cloud-Computing und Datenanalyse",
    "section": "",
    "text": "19.09.2023, 10:00 - 15:00 Uhr\n\nThemen\nIn Modul 2 beginnen wir mit der Einführung in die benötigte Software und Arbeitsumgebung. Es werden keine Vorkenntnisse in der Datenanalyse erwartet. Sie können sich auf folgende Themen freuen:\n\nErste Schritte im Cloud-Computing\nÜberblick über die vorliegenden Daten\nErste Analysen mit der Software R\nUmgang mit Daten(problemen)\nStichprobenziehung als Strategie\n\n\n\n\n\nVisualisierung einer Zufallsstichprobe\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\nPotentiale erkennen\n\n… Cloud-Computing als Möglichkeit für sichere Datenanalyse einschätzen.\n… in der Cloud-Arbeitsumgebung arbeiten.\n\nDie Daten analysieren (I)\n\n… die Daten vorbereiten und verstehen.\n… erste Datenanalysen durchführung und die Ergebnisse interpretieren.\n… eine Entscheidung über eine gute Strategie für die Stichprobenziehung fällen.\n\n\n\n\n\nMaterial\n\nSlides\nNotebook 2.1 - Erste Analysen\nNotebook 2.2 - Einfache Zufallsstichprobe\nNotebook 2.3 - Stratifizierte Zufallsstichprobe\n\n\n\n\n\n\n\nAnleitung Cloud-Umgebung\n\n\n\n\n\n\nLogin\n\nNavigation zu https://adrf.okta.com\nUsername (endet auf @adrf.net) und Passwort eingeben, Sign In klicken\nSend me the code klicken und dann den Code eingeben, Verify klicken\nADA Bayern auswählen\nDesktop auswählen\nPasswort eingeben, Sign in klicken\nEine Windows-Oberfläche öffnet sich mit einem Pop-Up Fenster. Dort I Acknowledge auswählen und die Quizfrage beantworten. Dann Submit klicken.\nDas offene Browserfenster kann geschlossen werden.\n\n\n\nErste Nutzung\nVor der ersten Nutzung müssen wir die Notebooks in Ihren persönlichen Arbeitsordner kopieren.\n\nFile Explorer öffnen.\nZu projects-Laufwerk (P:) navigieren, darin liegt ein Ordner pr-ada-bayern\nOrdner pr-ada-bayern öffnen, darin liegt eine zip-Datei notebooks_utf8_vx.zip\nzip-Datei notebooks_utf8_vx.zip kopieren (z.B. Recktsklick -&gt; Copy)\nZu users-Laufwerk (U:) navigieren, darin liegt ein Ordner mit ihrem Namen (vorname.nachname.id, dies ist ihr Arbeitsordner)\nOrdner mit ihrem Namen (vorname.nachname.id) öffnen und dort die zip-Datei einfügen (z.B. Rechtsklick -&gt; Paste)\nDoppelklick auf die zip-Datei öffnet ein Programm zum ent-zippen\nExtract To auswählen und im sich öffnenden Fenster OK\n\nNun ist ein Ordner in Ihrem Arbeitstsordner der die Notebooks enthält.\n\n\nNotebooks öffnen\n\nFile Explorer öffnen.\nZu users-Laufwerk (U:) navigieren, darin liegt ein Ordner mit ihrem Namen (vorname.nachname.id, dies ist ihr Arbeitsordner)\nOrdner mit ihrem Namen (vorname.nachname.id) öffnen und den Ordner darin (notebooks_utf8_vx, siehe oben) öffnen\nDoppelklick auf Notebooks.Rproj öffnet die Notebooks in RStudio\n\n\n\nArbeit mit den Notebooks\n\nWählen Sie das gewünschte Notebook über die Tabs in RStudio aus\nSie können beliebig Text sowie R-Code ändern\nCode können Sie über den grünen Pfeil am Code-Block ausführen\n\n\nSollte etwas nicht funktionieren, wenden Sie sich bitte an data-analytics@stat.uni-muenchen.de.",
    "crumbs": [
      "Module",
      "Modul 2: Cloud-Computing und Datenanalyse"
    ]
  },
  {
    "objectID": "workshops/ws3/WS3.html#daten-basierte-archivierung-von-gerichtsakten",
    "href": "workshops/ws3/WS3.html#daten-basierte-archivierung-von-gerichtsakten",
    "title": "Workshop Serie 3",
    "section": "",
    "text": "21.03. - 30.04.2024\n\n\n\n\n\nVeranstaltung\nDatum\nZeit\nOrt\n\n\n\n\nKick-Off 1*\n21.03.\n14:30 - 16:00\nOnline (Zoom)\n\n\nKick-Off 2*\n08.04.\n15:00 - 16:30\nOnline (Zoom)\n\n\nModul 1\n09.04.\n10:00 - 15:30\nLMU\n\n\nModul 2\n10.04.\n10:00 - 15:30\nLMU\n\n\nModul 3\n11.04.\n10:00 - 15:30\nLMU\n\n\nGruppenarbeit 1\n17.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nGruppenarbeit 2\n24.04.\n12:00 - 13:30\nOnline (Zoom)\n\n\nAbschluss-Vorbereitung\n30.04.\n10:00 - 12:00\nLMU\n\n\nAbschlusstermin\n30.04.\n13:30 - 15:30\nLMU / Hybrid (Zoom)\n\n\n\n*Kick-Off: es muss nur ein Kick-Off-Termin besucht werden.\n\n\n\n\nZoom: Der Zoom-Link wird den Teilnehmenden per Mail mitgeteilt. Sollten Sie keine E-Mail erhalten haben, melden Sie sich bitte bei us (siehe Kontakt).\nLMU: Institut für Statistik der LMU München, Ludwigstr. 33, 4. Stock\n\n\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\nDiese Workshop-Serie ist eine inhaltliche Wiederholung der Workshop-Serie 1.\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\n\n\n\n\n\nKick-Off\n\n\n\nAm Ende des Kick-Offs können Sie…\n\n… das Thema der Workshop-Serie beschreiben.\n… den Kontext des Projekts erklären.\n… auf die Cloud-Plattform zugreifen.\n\nSlides",
    "crumbs": [
      "Workshop Serie 3"
    ]
  },
  {
    "objectID": "workshops/ws1/WS1.html#daten-basierte-archivierung-von-gerichtsakten",
    "href": "workshops/ws1/WS1.html#daten-basierte-archivierung-von-gerichtsakten",
    "title": "Workshop Serie 1",
    "section": "",
    "text": "19.09. - 13.10.2023\n\n\nIn dieser Workshop-Serie betrachten wir alle Bestandteile eines klassischen Daten-Analyse-Projekts. Dabei geht es vor allem darum, Berührungsängste abzubauen und Kompetenzen aufzubauen, um als Team solche Projekte anzugehen. Denn Datenarbeit ist Teamarbeit und diverse Expertisen sind gefragt.\n\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieser Workshop-Serie, können Sie…\n\n… einschätzen, welche Fragen in Ihrem Umfeld mit Daten beanwortet werden können.\n… in Projekten mit Daten ihre Fachexpertise einbringen, ohne Berührungsängste zu haben.\n… mit Data Scientists und IT-Personal über Cloud-Computing und Datenanalysen sprechen.\n\n\n\n\n\n\nHier können Sie sich für die Workshop-Serie anmelden.\n\n\n\n\n\n\nWorkshop-Serie ausgebucht, Warteliste offen\n\n\n\nWir freuen uns über die vielen Anmeldungen, die wir bereits erhalten haben.\nLeider gibt es für diese Workshop-Serie ein Teilnahme-Limit. Wir haben eine Warteliste eingerichtet. Nachrücker erhalten Bescheid, falls sich jemand abmeldet.\nGerne sprechen wir mit Ihnen über Möglichkeiten, weitere Workshops zu organisieren. Schreiben Sie uns gerne eine E-Mail oder sprechen Sie uns persönlich an.",
    "crumbs": [
      "Workshop Serie 1"
    ]
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine stratifizierte oder auch geschichtete Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen. Wir wollen interessante Spalten für die Stratifizierung identifizieren und den Code in diesem Notebook für das Projekt anpassen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziel-des-notebooks",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziel-des-notebooks",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine stratifizierte oder auch geschichtete Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen. Wir wollen interessante Spalten für die Stratifizierung identifizieren und den Code in diesem Notebook für das Projekt anpassen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#software-pakete-laden",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#software-pakete-laden",
    "title": "ADA Bayern",
    "section": "2 Software-Pakete laden",
    "text": "2 Software-Pakete laden\nWir laden erneut die gleichen Pakete, die wir auch schon zum Ziehen der einfachen Zufallstichprobe benötigt haben.\n\nlibrary(\"tidyverse\")\nlibrary(\"survey\")\nlibrary(\"PracTools\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#einlesen-der-daten",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#einlesen-der-daten",
    "title": "ADA Bayern",
    "section": "3 Einlesen der Daten",
    "text": "3 Einlesen der Daten\nIn diesem Schritt lesen wir die Daten in R ein. Der Code ist genau wie zuvor.\n\npfad &lt;- file.path(\".\")\ncol_types &lt;- c(\"ccccnccccccccccccccccccTTcnTccnTcTcccc\")\n\nforumstar_daten &lt;- read_csv(file.path(pfad, \"230817_Abfrage_Januar-Dezember.csv\"), col_types = col_types)\nkarte_amtsgerichte &lt;- readRDS(file.path(pfad, \"230911_Karte_Amtsgerichtsbezirke.RDS\"))"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "title": "ADA Bayern",
    "section": "4 Daten für die Stichprobenziehung vorbereiten",
    "text": "4 Daten für die Stichprobenziehung vorbereiten\nFür die Stichprobenziehung benötigen wir nicht alle Spalten des Datensatzes. Daher bereiten wir die Daten zunächst vor. Für eine einfache Stichprobenziehung reicht ein Eintrag pro Akte aus. Die beteiligten Personen interessieren uns hier zunächst nicht. Mit dem folgenden Code aggregieren wir die Daten auf einen Eintrag pro Aktenzeichen1.\n\nakten &lt;- forumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`,\n    `Streitwert in EURO`,\n    `Gesamtstreitgegenstand`,\n    `Erledigungsgrund`,\n    `Dauer des Verfahrens in Tagen`,\n    `Archivstatus`,\n    `Anbietungsgrund (manuell erfasst)`,\n    `Anbietungsgrund`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nUm unsere visuelle Darstellung auf Karten zu ermöglichen generieren wir außerdem die Spalte Bezirk und fügen sie unserem Datensatz hinzu.\n\ntmp_Gericht &lt;- sub(\"Amtsgericht \", \"\", akten$Gericht)\ntmp_Gericht &lt;- sub(\" Zweigstelle.*\", \"\", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.d. \", \" i. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" a.d. \", \" a. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" am \", \" a. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.OB\", \" i. OB\", tmp_Gericht)\n\nakten &lt;- akten %&gt;%\n  mutate(`Bezirk` = tmp_Gericht)"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-zufallsstichprobe-stratifiziert-nach-amtsgerichtsbezirken",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-zufallsstichprobe-stratifiziert-nach-amtsgerichtsbezirken",
    "title": "ADA Bayern",
    "section": "5 Ziehung einer stratifizierten Zufallsstichprobe, stratifiziert nach Amtsgerichtsbezirken",
    "text": "5 Ziehung einer stratifizierten Zufallsstichprobe, stratifiziert nach Amtsgerichtsbezirken\nBei einer stratifizierten Stichprobe werden zusätzlich verfügbare Information aus der Grundgesamtheit zur Stichprobenziehung verwendet.\nVielleicht wollen wir sicherstellen, dass Akten aus allen Amtsgerichtsbezirken archiviert werden? Dann könnten Historiker in Zukunft verschiedenen Regionen Bayerns untereinander vergleichen. Mit diesem Gedanken sollen nun zufällig zehn Akten pro Bezirk gezogen werden.\nVon jedem Amtsgerichtsbezirk (jeder Bezirk bildet hier ein sogenanntes Stratum) benötigen wir die Gesamtzahl der zur Verfügung stehenden Akten.\n\nbeschreibung_stichprobe &lt;- akten %&gt;% \n  group_by(`Bezirk`) %&gt;%\n  summarise(`Anzahl Akten` = n()) %&gt;%\n  arrange(`Bezirk`)\n\nNun sollen aus jedem Bezirk zehn Akten zufällig ausgewählt werden.\n\nbeschreibung_stichprobe &lt;- beschreibung_stichprobe %&gt;% \n  mutate(`Stichprobengröße` = 10)\n\nZur Vorbereitung der Stichprobenziehung werden die Infos aus beschreibung_stichprobe an die Grundgesamtheit herangespielt.\n\nakten_fuer_stratifizierung_nach_gericht &lt;- akten %&gt;%\n  left_join(beschreibung_stichprobe, by = \"Bezirk\")\n\nNun erfolgt die Stichprobenziehung2.\n\nset.seed(20230919)\nstratifizierte_stichprobe_nach_gericht &lt;- akten_fuer_stratifizierung_nach_gericht %&gt;%\n  group_by(Bezirk) %&gt;%\n  mutate(samp = sample(n())) %&gt;%\n  filter(samp &lt;= `Stichprobengröße`) %&gt;%\n  ungroup()\n\nstratifizierte_stichprobe_nach_gericht$samp &lt;- NULL\n\nAlle nötigen Infos zur Beschreibung der Stichprobe sind in der folgenden Tabelle vorhanden:\n\nbeschreibung_stichprobe\n\nKurze Prüfung. Haben wir tatsächlich zehn Akten pro Gericht gezogen?\n\nstratifizierte_stichprobe_nach_gericht %&gt;%\n  group_by(`Bezirk`) %&gt;%\n  summarise(n())\n\nWie viele Akten liegen insgesamt in der Stichprobe vor?\n\nnrow(stratifizierte_stichprobe_nach_gericht)\n\n\n5.1 Ist die stratifizierte Stichprobe repräsentativ?\nAkten aus kleinen Amtsgerichtsbezirken hatten in dieser Stichprobe eine deutlich höhere Wahrscheinlichkeit, in die Stichprobe zu gelangen, als zum Beispiel aus dem Amtsgericht München. Wenn man dies unberücksichtigt lässt, können keine statistisch validen Aussagen über die Grundgesamtheit getroffen werden.\nFür repräsentative statistische Aussagen müssen Gewichte verwendet werden. Akten mit geringer Auswahlwahrscheinlichkeit stehen für eine größere Anzahl von Akten und erhalten damit ein größeres Gewicht.\nDie Auswahlwahrscheinlichkeit berechnet sich in jedem Stratum getrennt als\n\nAuswahlwahrscheinlichkeit = Anzahl (Stichprobe) / Anzahl (Grundgesamtheit)\n\nDas Gewicht (= Hochrechnungsfaktor) ergibt sich als\n\nHochrechnungsfaktor = 1 / Auswahlwahrscheinlichkeit\n\nEine Akte mit einem Hochrechnungsfaktor von z.B. 20 wurde also mit einer Wahrscheinlichkeit von 5% in die Stichprobe gezogen und steht nun repräsentativ für 20 Akten, die alternativ hätten gezogen werden können.\nAuswahlwahrscheinlichkeit und Hochrechnungsfaktor lassen sich allein aus den Angaben zur Beschreibung der Stichprobe wie folgt berechnen.\n\nstratifizierte_stichprobe_nach_gericht &lt;- stratifizierte_stichprobe_nach_gericht %&gt;%\n  mutate(Auswahlwahrscheinlichkeit = `Stichprobengröße` / `Anzahl Akten`) %&gt;%\n  mutate(Hochrechnungsfaktor = 1 / Auswahlwahrscheinlichkeit)\n\nHier zeigt sich also, warum diese Dokumentation der Stichprobe so wichtig ist.\nAls Beispiel soll nun die mittlere Anzahl der Prozessbeteiligten geschätzt werden.\nIn der Grundgesamtheit können wir dies berechnen als\n\\[\n\\bar{y} = \\frac{\\sum_{grundgesamtheit} 1 \\cdot y_i}{N}\n\\]\n\nmean(akten$`Anzahl Beteiligte`)\n\nAnhand der Stichprobe können wir mittels folgender Formel ungefähr den selben Wert erhalten\n\\[\n\\hat{\\bar{y}}=\\frac{\\sum_{stichprobe} hrf_i \\cdot y_i}{\\sum_{stichprobe} hrf_i}\n\\]\n\nsum(stratifizierte_stichprobe_nach_gericht$Hochrechnungsfaktor * stratifizierte_stichprobe_nach_gericht$`Anzahl Beteiligte`) / sum(stratifizierte_stichprobe_nach_gericht$Hochrechnungsfaktor)\n\nZum selben Ergebnis führt auch der folgende Code3.\n\nmy_stratified_design &lt;- svydesign(id = ~0, strata = ~Bezirk, data = stratifizierte_stichprobe_nach_gericht, fpc = ~Auswahlwahrscheinlichkeit)\n\nsvymean(~`Anzahl Beteiligte`, my_stratified_design, na.rm = TRUE)\n\nZusätzlich lässt sich noch das Konfidenzintervall berechnen, welches mit 95%-Wahrscheinlichkeit den wahren Wert aus der Grundgesamtheit überdeckt. Damit kann die Genauigkeit der Schätzung beurteilt werden.\n\nconfint(svymean(~`Anzahl Beteiligte`, my_stratified_design, na.rm = TRUE))\n\nEine naive Schätzung der mittleren Anzahl Prozessbeteiligter berechnet einfach nur den Mittelwert der Stichprobe und lässt dabei die unterschiedlichen Auswahlwahrscheinlichkeiten außer Acht.\n\nmean(stratifizierte_stichprobe_nach_gericht$`Anzahl Beteiligte`)\n\nAnders als die Schätzung oben ist dieser Wert nicht zur Beschreibung der Grundgesamtheit geeignet. Nur wenn Gewichte (=Hochrechnungsfaktoren) verwendet werden, können aus der Stichprobe Ergebnisse abgeleitet werden, die repräsentativ für die Grundgesamtheit sind."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-stichprobe-stratifiziert-nach-verfahrensdauer",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-stichprobe-stratifiziert-nach-verfahrensdauer",
    "title": "ADA Bayern",
    "section": "6 Ziehung einer stratifizierten Stichprobe, stratifiziert nach Verfahrensdauer",
    "text": "6 Ziehung einer stratifizierten Stichprobe, stratifiziert nach Verfahrensdauer\nEine stratifizierte Stichprobe kann noch aus einem anderen Grund sinnvoll sein: In diesem Beispiel sollen Akten mit einer langen Verfahrensdauer besonders häufig archiviert werden, da diese Akten besonders umfangreich und informativ sein könnten.\nZunächst betrachten wir einen kurze Zusammenfassung der Verfahrensdauer.\n\nsummary(akten$`Dauer des Verfahrens in Tagen`)\n\nDie stetige Variable Dauer des Verfahrens in Tagen diskretisieren wir:\n\nkurzes Verfahren, wenn die Verfahrensdauer weniger als 100 Tage betrug\nmittleres Verfahren bei 100-1500 Tagen\nlanges Verfahren ab 1500 Tagen Verfahrensdauer\n\nBei fehlenden Werten wird von einem kurzen Verfahren ausgegangen.\n\nakten &lt;- akten %&gt;% mutate(\n    verfahrensdauer = case_when(\n      `Dauer des Verfahrens in Tagen` &lt; 100 ~ \"kurzes Verfahren\",\n      `Dauer des Verfahrens in Tagen` &gt;= 100 & `Dauer des Verfahrens in Tagen` &lt; 1500 ~ \"mittleres Verfahren\",\n      `Dauer des Verfahrens in Tagen` &gt;= 1500 ~ \"langes Verfahren\",\n      is.na(`Dauer des Verfahrens in Tagen`) ~ \"kurzes Verfahren\",\n      TRUE ~ as.character(`Dauer des Verfahrens in Tagen`)\n    )\n)\n\nDie Variable verfahrensdauer dient zur Stratifizierung. Zur Beschreibung der Stichprobe wird zunächst gezählt, wie viele Akten eine kurze/mittlere/lange Verfahrensdauer haben.\n\nbeschreibung_stichprobe &lt;- akten %&gt;% \n  group_by(verfahrensdauer) %&gt;%\n  summarise(akten_pro_stratum = n()) %&gt;%\n  arrange(verfahrensdauer)\n\nbeschreibung_stichprobe\n\nWir setzen die Stichprobengröße für jedes einzelne Stratum händisch. Dabei sollen alle 178 langen Verfahren archiviert werden.\n\nbeschreibung_stichprobe &lt;- beschreibung_stichprobe %&gt;% mutate(\n     anzahl_akten_pro_schicht = case_when(\n      verfahrensdauer == \"kurzes Verfahren\" ~ 50,\n      verfahrensdauer == \"mittleres Verfahren\" ~ 100,\n      verfahrensdauer == \"langes Verfahren\" ~ 178)\n)\n\nbeschreibung_stichprobe\n\nMit dem bereits oben beschriebenen Code kann hier wieder die Stichprobe gezogen werden.\n\nakten_fuer_stratifizierung_nach_verfahrensdauer &lt;- akten %&gt;%\n  left_join(beschreibung_stichprobe, by = \"verfahrensdauer\")\n\nset.seed(46)\nstratifizierte_stichprobe_nach_verfahrensdauer &lt;- akten_fuer_stratifizierung_nach_verfahrensdauer %&gt;%\n  group_by(verfahrensdauer) %&gt;%\n  mutate(samp = sample(n())) %&gt;%\n  filter(samp &lt;= anzahl_akten_pro_schicht) %&gt;%\n  ungroup()\n\nstratifizierte_stichprobe_nach_gericht$samp &lt;- NULL\n\nEin kurzer Check, ob die Stichprobenziehung erfolgreich war.\n\ntable(stratifizierte_stichprobe_nach_verfahrensdauer$verfahrensdauer)\n\n\n6.1 Wie kann die Stichprobengröße pro Schicht bestimmt werden? (optional)\nHier wurde händisch vorgegeben, wie viele Akten aus jeder Schicht archiviert werden sollen. Alternativ wäre auch möglich:\n\nProportionale Allokation: Proportional zum Umfang der jeweiligen Schicht, stellt Repräsentativität bezüglich des Schichtungsmerkmals sicher\nEqual (gleiche) Allokation: gleiches \\(n\\) in jedem Stratum, so wie es oben bei der Stratifizierung nach Gerichten erfolgt ist\nOptimale Allokation: Optimiere die Genauigkeit der Schätzung für eine Variable. Die Varianz dieser Variablen muss dafür bekannt sein.\n\nDie Stichprobengröße pro Schicht soll nun mit jeder der drei genannten Methoden bestimmt werden. Als Schichtungsvariable verwenden wir erneut die verfahrensdauer (kurz/mittel/lang). Anhand der Grundgesamtheit berechnen wir, wie viele Akten \\(N_h\\) in jeder Schicht \\(h\\) vorhanden sind. Für die optimale Allokation benötigen wir zusätzlich noch die Varianz derjenigen Variablen, für die eine möglichst genaue Schätzung erzielt werden soll.\n\nbeschreibung_stichprobe_allokation &lt;- akten %&gt;% \n  group_by(verfahrensdauer) %&gt;%\n  summarise(akten_pro_stratum = n(),\n            variance_pro_stratum = var(`Dauer des Verfahrens in Tagen`, na.rm = TRUE)) %&gt;%\n  arrange(verfahrensdauer)\n\nZusätzlich geben wir an, dass die Stichprobe insgesamt \\(n = 200\\) Akten enthalten soll. Mithilfe der folgenden Formeln lässt sich nun berechnen, wie viele Akten aus jeder Schicht \\(h\\) gezogen werden:\n\\[n_{h, proportional} = Runden(n \\cdot \\frac{N_h}{N}) = Runden(n \\cdot \\frac{N_h}{\\sum_{h' = 1}^M N_{h'}})\\]\n\\[n_{h, optimal} = Runden(n \\cdot \\frac{N_h \\cdot S_h}{\\sum_{h' = 1}^M N_{h'} \\cdot S_{h'}})\\]\n\\[n_{h, equal} = Runden(n/M)\\]\n\nN &lt;- nrow(akten)\nn &lt;- 200\n\ndenom &lt;- sum(beschreibung_stichprobe_allokation$akten_pro_stratum * beschreibung_stichprobe_allokation$variance_pro_stratum)\n\nbeschreibung_stichprobe_allokation &lt;- beschreibung_stichprobe_allokation %&gt;%\n  mutate(proportional_allocation = round((n / N) * akten_pro_stratum),\n         optimal_allocation = round(n * akten_pro_stratum * variance_pro_stratum / denom),\n         equal_allocation = round(n / nrow(beschreibung_stichprobe_allokation)))\n\nbeschreibung_stichprobe_allokation\n\nDa es nur sehr wenige lange Verfahren gibt, gelangt kein einziges davon bei einer Zuordnung proportional zur Anzahl (proportional allocation) in die Stichprobe.\nBei der optimal allocation wird versucht, möglichst viel über die Dauer des Verfahrens in Tagen (oder ein damit korrelierendes Merkmal) zu erfahren. Bei den kurzen Verfahren wissen wir bereits, dass alle Verfahren weniger als 50 Tage dauern (kleine Varianz). Entsprechend gering ist der Informationsgewinn, wenn ein solches Verfahren in die Stichprobe gelangt. Umgekehrt kann die Länge eines langen Verfahrens (&gt;1500 Tage) stark streuen. Um dennoch möglichst akkurat die mittlere Dauer der langen Verfahren berechnen zu können, müssten hier sehr viele Akten gezogen werden.\nBei der equal allocation gelangt jede Schicht gleich häufig in die Stichprobe."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-stratifizierten-stichprobe",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-stratifizierten-stichprobe",
    "title": "ADA Bayern",
    "section": "7 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer stratifizierten Stichprobe?",
    "text": "7 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer stratifizierten Stichprobe?\n\nIst die stratifizierte Stichprobe zur Auswahl von Akten geeignet? Welche Vor- und Nachteile hat das Verfahren?\nAnhand welcher Variablen definiert man einzelne Strata? Welche Schwellwerte verwendet man bei kontinuierlichen Variablen?\nWie viele Akten werden aus jedem Stratum gezogen? (Proportionale Allokation, Equal Allokation, Optimal Allokation, oder noch was anderes?)\n\nVorteile:\n\nUnterschiedliche Auswahlwahrscheinlichkeiten in jedem Stratum\nKonfidenzintervalle werden schmaler wenn Selektionswahrscheinlichkeiten und Zielvariable korreliert sind.\nGarantierte Mindestzahl an Fällen in jedem Stratum (zum Beispiel zehn Akten pro Gericht)\nJedes Stratum einer stratifizierten Zufallsstichprobe lässt sich auch einzeln auswerten (einfach wie eine einfache Zufallsstichprobe, jedoch ist die Grundgesamtheit nun beschränkt auf das Stratum).\nEinfach verständlich und gut dokumentierbar\n\nNachteile:\n\nUnterschiedliche Auswahlwahrscheinlichkeiten in jedem Stratum\nKein Effizienzgewinn (unter Umstand. sogar -verlust), wenn Selektionswahrscheinlichkeiten und Zielvariablen nicht (bzw. negativ) korreliert sind\nMehr Dokumentationsaufwand"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-wie-wollen-wir-als-gruppe-akten-für-die-archivierung-auswählen",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#diskussion-wie-wollen-wir-als-gruppe-akten-für-die-archivierung-auswählen",
    "title": "ADA Bayern",
    "section": "8 Diskussion: Wie wollen wir als Gruppe Akten für die Archivierung auswählen?",
    "text": "8 Diskussion: Wie wollen wir als Gruppe Akten für die Archivierung auswählen?\n\nEinfache Zufallsstichprobe vs. stratifiziert?\nWenn stratifiziert, wie wählen wir die Strata aus?\nOder gar keine Zufallsstichprobe (zum Beispiel zehn Akten je Gericht mit den meisten Beteiligten?)\nWie viele Akten wählen wir?\nWelche Informationen zum Auswahlprozess müssen wir dokumentieren und kommunizieren?"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#footnotes",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_3_Stratifizierte_Stichprobenziehung_anonymisiert.html#footnotes",
    "title": "ADA Bayern",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDazu verwenden wir die group_by Funktion. Zusätzlich erstellen wir mithilfe der Funktion summarise()eine neue Spalte Anzahl Beteiligte. Anschließend fügen wir noch eine eindeutige Index Spalte Index für jede Akte hinzu.↩︎\nMit set.seed wird zunächst ein Seed gesetzt, so dass immer dieselbe Stichprobe gezogen wird.\nBezirk soll unsere Stratumsvariable sein. Daher werden mittels group_by die nachfolgenden Schritte für jedes Gericht einzeln getrennt voneinander durchgeführt.\nAlle Akten eines jeden Gerichts werden durchnummeriert von 1 bis zur Gesamtzahl der Akten vom jeweiligen Gericht n(), die Nummern werden anschließend mittels sample gemischt und jede Akte bekommt in der Variablen samp eine Nummer zugewiesen. Akten mit einer Nummer kleiner oder gleich 10 (vorgegeben in der Variablen anzahl_akten_pro_gericht) werden herausgefiltert und bleiben in der Stichprobe erhalten.\nAbschließend wird die Hilfsvariable samp gelöscht, indem ihr der Wert NULL zugewiesen wird.↩︎\nDie Funktionen svydesign, svymean und confint kommen aus dem survey-package. Mit svydesign lässt sich das Design der Stichprobe beschreiben. Die anderen Funktionen nutzen dieses Design zur Berechnung vom gewichteten Mittelwert bzw. zur Berechnung des Konfidenzintervalls.↩︎"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook fassen wir die Inhalte der einfachen und stratifizierten Stichprobenziehung nochmal zusammen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen. Wir wollen interessante Spalten für die Stratifizierung identifizieren und den Code in diesem Notebook für das Projekt anpassen."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziel-des-notebooks",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziel-des-notebooks",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook fassen wir die Inhalte der einfachen und stratifizierten Stichprobenziehung nochmal zusammen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen. Wir wollen interessante Spalten für die Stratifizierung identifizieren und den Code in diesem Notebook für das Projekt anpassen."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#software-pakete-laden-und-einlesen-der-daten",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#software-pakete-laden-und-einlesen-der-daten",
    "title": "ADA Bayern",
    "section": "2 Software-Pakete laden und einlesen der Daten",
    "text": "2 Software-Pakete laden und einlesen der Daten\nIn diesem Schritt lesen wir die Daten in R ein. Der Code ist genau wie zuvor. Diesmal verstecken wir den Code in einer anderen Datei, die wir hier aufrufen. So bleibt das Notebook übersichtlicher. Im gleichen Schritt bereiten wir unsere Daten auch schon für die Stichprobenziehung vor1.\n\nsource(\"ADA_daten_vorbereiten.R\")"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziehung-einer-einfachen-zufallsstichprobe",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziehung-einer-einfachen-zufallsstichprobe",
    "title": "ADA Bayern",
    "section": "3 Ziehung einer einfachen Zufallsstichprobe",
    "text": "3 Ziehung einer einfachen Zufallsstichprobe\nFür die Ziehung einer einfachen Zufallsstichprobe benötigen wir eine wohl definierte Grundgesamtheit. In unserem Fall bilden alle Akten aus dem Jahr 2018 (alle bayerischen Amtsgerichte, nur Registerzeichen C) die Grundgesamtheit. Der Umfang der Grundgesamtheit, hier die Anzahl aller Akten in einem Jahr, wird mit \\(N\\) bezeichnet.\nAußerdem benötigen wir die Stichprobengröße, also die Anzahl der Akten die ausgewählt werden sollen. Die Stichprobengröße wird mit \\(n\\) bezeichnet.\nIm folgenden Code lesen wir zunächst die Anzahl der Akten aus2.\n\nN &lt;- akten %&gt;% \n  nrow()\n\nN\n\nDann legen wir den Stichprobenumfang \\(n\\) auf 200 fest3.\n\nn &lt;- 200\nn\n\nAls Auswahlwahrscheinlichkeit bezeichnet man die Wahrscheinlichkeit, mit der eine Einheit (hier eine Akte, technisch eine Zeile) in die Stichprobe gelangt4.\n\n100 * n/N\n\nMit welcher Wahrscheinlichkeit (in Prozent) wird eine Akte ausgewählt?\nNun können wir eine einfache Zufallsstichprobe ziehen5. Als Ergebnis erhalten wir \\(n=200\\) zufällig gewählte Werte aus der Spalte Index.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\nWenn wir den gleichen Code noch einmal ausführen, bekommen wir eine neue Zufallsstichprobe.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\nMit den Indizes in der Stichprobe können wir nun die entsprechenden Zeilen in unserem Datensatz akten auswählen. Das machen wir über einen einfachen Abgleich der Indizes6.\n\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\n\n\n3.1 Wie groß sollte unsere Stichprobe sein?\nAls einfache Regel gilt, je größer die Stichprobe, desto genauer können wir die Grundgesamtheit abbilden.\nWas bedeutet “genau”? Jede Wiederholung der Stichprobenziehung ergibt eine neue Stichprobe. Das heißt, die Akten, die Teil der Stichprobe sind, unterscheiden sich von Wiederholung zu Wiederholung. Berechnen wir nun, zum Beispiel, einen Mittelwert auf Grundlage der Stichprobe, unterscheiden sich die Ergebnisse der Berechnung. Je größer die Stichprobe ist, desto kleiner werden die Unterschiede und desto näher wird (in der Regel) das Stichprobenmittel dem Mittelwert aus der Grundgesamtheit kommen.\n\n\n3.2 Graphische Darstellung\nNun wollen wir die einfache Zufallsstichprobe vergleichen, mit dem was aktuell angeboten wird. Dazu betrachten wir den Anteil angebotener Akten pro Gericht und stellen dies dar. Berechne zunächst die Anteile.\n\nanteil_anzubietend &lt;- akten %&gt;%\n  mutate(AkteAnzubieten = case_when(\n    `Archivstatus` == \"anzubieten\" ~ \"Ja\",\n    is.na(`Archivstatus`) ~ \"Nein\",\n    TRUE ~ \"Nein\")) %&gt;%\n  group_by(`Bezirk`, `AkteAnzubieten`, .drop = FALSE) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(anteil = n / sum(n)) %&gt;%\n  filter(`AkteAnzubieten` == \"Ja\")\n\nanteil_anzubietend\n\nNun stellen wir die Anteile auf einer Karte dar.\n\n# In Karte übernehmen\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anteil_anzubietend$anteil[match(karte_amtsgerichte$court, anteil_anzubietend$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\n# und Plotten\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nSchweinfurt und Kitzingen bieten einen besonders hohen Anteil an, mehr als 3 Prozent! Von vielen Gerichten wird aber auch keine einzige Akte angeboten.\nWoran liegt diese ungleiche Verteilung?\nNun sollen \\(n = 372\\) Akten (so viele wie 2018 angeboten wurden) zufällig gezogen werden. In dem Indikator inSample in akten_kopie wird abgespeichert, ob die Akte gezogen wurde oder nicht.\n\nn &lt;- 372\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\n\nakten_kopie &lt;- akten\nakten_kopie$inSample[akten$Index %in% Index_stichprobe] &lt;- \"Ja\"\n\nBerechne die Anteile der zufällig ausgewählten Akten in jedem Bezirk.\n\nanteil_anzubietend_zufaellig &lt;- akten_kopie %&gt;%\n  mutate(AkteAnzubieten = case_when(\n    `inSample` == \"Ja\" ~ \"Ja\",\n    is.na(`inSample`) ~ \"Nein\",\n    TRUE ~ \"Nein\")) %&gt;%\n  group_by(`Bezirk`, `AkteAnzubieten`, .drop = FALSE) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(anteil = n / sum(n)) %&gt;%\n  filter(`AkteAnzubieten` == \"Ja\")\n\nanteil_anzubietend_zufaellig\n\nAuf einer Karte können wir nun erneut darstellen, wie viele Akten angeboten werden wenn dies rein zufällig erfolgen würde.\n\n# in Karte übernehmen\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anteil_anzubietend_zufaellig$anteil[match(karte_amtsgerichte$court, anteil_anzubietend_zufaellig$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\n# und plotten\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nDie meisten Gerichte würden bei zufälliger Auswahl mindestens eine Akte anbieten. Man beachte aber auch die Skala: Es ist höchst unwahrscheinlich, dass ein Gericht auch nur 1% seiner Akten zur Archivierung anbieten würde."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-einfachen-zufallsstichprobe",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-einfachen-zufallsstichprobe",
    "title": "ADA Bayern",
    "section": "4 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer einfachen Zufallsstichprobe?",
    "text": "4 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer einfachen Zufallsstichprobe?\n\nWelche Faktoren beeinflussen, wie groß die Stichprobe sein kann?\nAls Grundgesamtheit wurde hier “alle C-Verfahren in einem Jahr in Bayern” verwendet. Ist dies so sinnvoll und praktikabel oder gibt es bessere Alternativen?\nIst die einfache Zufallsstichprobe zur Auswahl von Akten geeignet? Welche Vor- und Nachteile hat das Verfahren?\n\n\n4.0.1 Vorteile\n\nkeine systematische Auswahl von Akten (z.B. anhand von Nachnamen)\nRepräsentative Stichprobe\nAlle Akten haben dieselbe Auswahlwahrscheinlichkeit\n\n\n\n4.0.2 Nachteile\n\nNicht alle Gerichte/Landkreise/Regionen tauchen immer in der Stichprobe auf\nAlle Akten haben dieselbe Auswahlwahrscheinlichkeit"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-zufallsstichprobe-stratifiziert-nach-amtsgerichtsbezirken",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-zufallsstichprobe-stratifiziert-nach-amtsgerichtsbezirken",
    "title": "ADA Bayern",
    "section": "5 Ziehung einer stratifizierten Zufallsstichprobe, stratifiziert nach Amtsgerichtsbezirken",
    "text": "5 Ziehung einer stratifizierten Zufallsstichprobe, stratifiziert nach Amtsgerichtsbezirken\nBei einer stratifizierten Stichprobe werden zusätzlich verfügbare Information aus der Grundgesamtheit zur Stichprobenziehung verwendet.\nVielleicht wollen wir sicherstellen, dass Akten aus allen Amtsgerichtsbezirken archiviert werden? Dann könnten Historiker in Zukunft verschiedenen Regionen Bayerns untereinander vergleichen. Mit diesem Gedanken sollen nun zufällig zehn Akten pro Bezirk gezogen werden.\nVon jedem Amtsgerichtsbezirk (jeder Bezirk bildet hier ein sogenanntes Stratum) benötigen wir die Gesamtzahl der zur Verfügung stehenden Akten.\n\nbeschreibung_stichprobe &lt;- akten %&gt;% \n  group_by(`Bezirk`) %&gt;%\n  summarise(`Anzahl Akten` = n()) %&gt;%\n  arrange(`Bezirk`)\n\nNun sollen aus jedem Bezirk zehn Akten zufällig ausgewählt werden.\n\nbeschreibung_stichprobe &lt;- beschreibung_stichprobe %&gt;% \n  mutate(`Stichprobengröße` = 10)\n\nZur Vorbereitung der Stichprobenziehung werden die Infos aus beschreibung_stichprobe an die Grundgesamtheit herangespielt.\n\nakten_fuer_stratifizierung_nach_gericht &lt;- akten %&gt;%\n  left_join(beschreibung_stichprobe, by = \"Bezirk\")\n\nNun erfolgt die Stichprobenziehung7.\n\nset.seed(20230919)\nstratifizierte_stichprobe_nach_gericht &lt;- akten_fuer_stratifizierung_nach_gericht %&gt;%\n  group_by(Bezirk) %&gt;%\n  mutate(samp = sample(n())) %&gt;%\n  filter(samp &lt;= `Stichprobengröße`) %&gt;%\n  ungroup()\n\nstratifizierte_stichprobe_nach_gericht$samp &lt;- NULL\n\nAlle nötigen Infos zur Beschreibung der Stichprobe sind in der folgenden Tabelle vorhanden:\n\nbeschreibung_stichprobe\n\nKurze Prüfung. Haben wir tatsächlich zehn Akten pro Gericht gezogen?\n\nstratifizierte_stichprobe_nach_gericht %&gt;%\n  group_by(`Bezirk`) %&gt;%\n  summarise(n())\n\nWie viele Akten liegen insgesamt in der Stichprobe vor?\n\nnrow(stratifizierte_stichprobe_nach_gericht)\n\n\n5.1 Ist die stratifizierte Stichprobe repräsentativ?\nAkten aus kleinen Amtsgerichtsbezirken hatten in dieser Stichprobe eine deutlich höhere Wahrscheinlichkeit, in die Stichprobe zu gelangen, als zum Beispiel aus dem Amtsgericht München. Wenn man dies unberücksichtigt lässt, können keine statistisch validen Aussagen über die Grundgesamtheit getroffen werden.\nFür repräsentative statistische Aussagen müssen Gewichte verwendet werden. Akten mit geringer Auswahlwahrscheinlichkeit stehen für eine größere Anzahl von Akten und erhalten damit ein größeres Gewicht.\nDie Auswahlwahrscheinlichkeit berechnet sich in jedem Stratum getrennt als\n\nAuswahlwahrscheinlichkeit = Anzahl (Stichprobe) / Anzahl (Grundgesamtheit)\n\nDas Gewicht (= Hochrechnungsfaktor) ergibt sich als\n\nHochrechnungsfaktor = 1 / Auswahlwahrscheinlichkeit\n\nEine Akte mit einem Hochrechnungsfaktor von z.B. 20 wurde also mit einer Wahrscheinlichkeit von 5% in die Stichprobe gezogen und steht nun repräsentativ für 20 Akten, die alternativ hätten gezogen werden können.\nAuswahlwahrscheinlichkeit und Hochrechnungsfaktor lassen sich allein aus den Angaben zur Beschreibung der Stichprobe wie folgt berechnen.\n\nstratifizierte_stichprobe_nach_gericht &lt;- stratifizierte_stichprobe_nach_gericht %&gt;%\n  mutate(Auswahlwahrscheinlichkeit = `Stichprobengröße` / `Anzahl Akten`) %&gt;%\n  mutate(Hochrechnungsfaktor = 1 / Auswahlwahrscheinlichkeit)\n\nHier zeigt sich also, warum diese Dokumentation der Stichprobe so wichtig ist.\nAls Beispiel soll nun die mittlere Anzahl der Prozessbeteiligten geschätzt werden.\nIn der Grundgesamtheit können wir dies berechnen als\n\\[\n\\bar{y} = \\frac{\\sum_{grundgesamtheit} 1 \\cdot y_i}{N}\n\\]\n\nmean(akten$`Anzahl Beteiligte`)\n\nAnhand der Stichprobe können wir mittels folgender Formel ungefähr den selben Wert erhalten\n\\[\n\\hat{\\bar{y}}=\\frac{\\sum_{stichprobe} hrf_i \\cdot y_i}{\\sum_{stichprobe} hrf_i}\n\\]\n\nsum(stratifizierte_stichprobe_nach_gericht$Hochrechnungsfaktor * stratifizierte_stichprobe_nach_gericht$`Anzahl Beteiligte`) / sum(stratifizierte_stichprobe_nach_gericht$Hochrechnungsfaktor)\n\nZum selben Ergebnis führt auch der folgende Code8.\n\nmy_stratified_design &lt;- svydesign(id = ~0, strata = ~Bezirk, data = stratifizierte_stichprobe_nach_gericht, fpc = ~Auswahlwahrscheinlichkeit)\n\nsvymean(~`Anzahl Beteiligte`, my_stratified_design, na.rm = TRUE)\n\nZusätzlich lässt sich noch das Konfidenzintervall berechnen, welches mit 95%-Wahrscheinlichkeit den wahren Wert aus der Grundgesamtheit überdeckt. Damit kann die Genauigkeit der Schätzung beurteilt werden.\n\nconfint(svymean(~`Anzahl Beteiligte`, my_stratified_design, na.rm = TRUE))\n\nEine naive Schätzung der mittleren Anzahl Prozessbeteiligter berechnet einfach nur den Mittelwert der Stichprobe und lässt dabei die unterschiedlichen Auswahlwahrscheinlichkeiten außer Acht.\n\nmean(stratifizierte_stichprobe_nach_gericht$`Anzahl Beteiligte`)\n\nAnders als die Schätzung oben ist dieser Wert nicht zur Beschreibung der Grundgesamtheit geeignet. Nur wenn Gewichte (=Hochrechnungsfaktoren) verwendet werden, können aus der Stichprobe Ergebnisse abgeleitet werden, die repräsentativ für die Grundgesamtheit sind."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-stichprobe-stratifiziert-nach-verfahrensdauer",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#ziehung-einer-stratifizierten-stichprobe-stratifiziert-nach-verfahrensdauer",
    "title": "ADA Bayern",
    "section": "6 Ziehung einer stratifizierten Stichprobe, stratifiziert nach Verfahrensdauer",
    "text": "6 Ziehung einer stratifizierten Stichprobe, stratifiziert nach Verfahrensdauer\nEine stratifizierte Stichprobe kann noch aus einem anderen Grund sinnvoll sein: In diesem Beispiel sollen Akten mit einer langen Verfahrensdauer besonders häufig archiviert werden, da diese Akten besonders umfangreich und informativ sein könnten.\nZunächst betrachten wir einen kurze Zusammenfassung der Verfahrensdauer.\n\nsummary(akten$`Dauer des Verfahrens in Tagen`)\n\nDie stetige Variable Dauer des Verfahrens in Tagen diskretisieren wir:\n\nkurzes Verfahren, wenn die Verfahrensdauer weniger als 100 Tage betrug\nmittleres Verfahren bei 100-1500 Tagen\nlanges Verfahren ab 1500 Tagen Verfahrensdauer\n\nBei fehlenden Werten wird von einem kurzen Verfahren ausgegangen.\n\nakten &lt;- akten %&gt;% mutate(\n    verfahrensdauer = case_when(\n      `Dauer des Verfahrens in Tagen` &lt; 100 ~ \"kurzes Verfahren\",\n      `Dauer des Verfahrens in Tagen` &gt;= 100 & `Dauer des Verfahrens in Tagen` &lt; 1500 ~ \"mittleres Verfahren\",\n      `Dauer des Verfahrens in Tagen` &gt;= 1500 ~ \"langes Verfahren\",\n      is.na(`Dauer des Verfahrens in Tagen`) ~ \"kurzes Verfahren\",\n      TRUE ~ as.character(`Dauer des Verfahrens in Tagen`)\n    )\n)\n\nDie Variable verfahrensdauer dient zur Stratifizierung. Zur Beschreibung der Stichprobe wird zunächst gezählt, wie viele Akten eine kurze/mittlere/lange Verfahrensdauer haben.\n\nbeschreibung_stichprobe &lt;- akten %&gt;% \n  group_by(verfahrensdauer) %&gt;%\n  summarise(akten_pro_stratum = n()) %&gt;%\n  arrange(verfahrensdauer)\n\nbeschreibung_stichprobe\n\nWir setzen die Stichprobengröße für jedes einzelne Stratum händisch. Dabei sollen alle 178 langen Verfahren archiviert werden.\n\nbeschreibung_stichprobe &lt;- beschreibung_stichprobe %&gt;% mutate(\n     anzahl_akten_pro_schicht = case_when(\n      verfahrensdauer == \"kurzes Verfahren\" ~ 50,\n      verfahrensdauer == \"mittleres Verfahren\" ~ 100,\n      verfahrensdauer == \"langes Verfahren\" ~ 178)\n)\n\nbeschreibung_stichprobe\n\nMit dem bereits oben beschriebenen Code kann hier wieder die Stichprobe gezogen werden.\n\nakten_fuer_stratifizierung_nach_verfahrensdauer &lt;- akten %&gt;%\n  left_join(beschreibung_stichprobe, by = \"verfahrensdauer\")\n\nset.seed(46)\nstratifizierte_stichprobe_nach_verfahrensdauer &lt;- akten_fuer_stratifizierung_nach_verfahrensdauer %&gt;%\n  group_by(verfahrensdauer) %&gt;%\n  mutate(samp = sample(n())) %&gt;%\n  filter(samp &lt;= anzahl_akten_pro_schicht) %&gt;%\n  ungroup()\n\nstratifizierte_stichprobe_nach_gericht$samp &lt;- NULL\n\nEin kurzer Check, ob die Stichprobenziehung erfolgreich war.\n\ntable(stratifizierte_stichprobe_nach_verfahrensdauer$verfahrensdauer)"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-stratifizierten-stichprobe",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-stratifizierten-stichprobe",
    "title": "ADA Bayern",
    "section": "7 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer stratifizierten Stichprobe?",
    "text": "7 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer stratifizierten Stichprobe?\n\nIst die stratifizierte Stichprobe zur Auswahl von Akten geeignet? Welche Vor- und Nachteile hat das Verfahren?\nAnhand welcher Variablen definiert man einzelne Strata? Welche Schwellwerte verwendet man bei kontinuierlichen Variablen?\nWie viele Akten werden aus jedem Stratum gezogen? (Proportionale Allokation, Equal Allokation, Optimal Allokation, oder noch was anderes?)\n\nVorteile:\n\nUnterschiedliche Auswahlwahrscheinlichkeiten in jedem Stratum\nKonfidenzintervalle werden schmaler wenn Selektionswahrscheinlichkeiten und Zielvariable korreliert sind.\nGarantierte Mindestzahl an Fällen in jedem Stratum (zum Beispiel zehn Akten pro Gericht)\nJedes Stratum einer stratifizierten Zufallsstichprobe lässt sich auch einzeln auswerten (einfach wie eine einfache Zufallsstichprobe, jedoch ist die Grundgesamtheit nun beschränkt auf das Stratum).\nEinfach verständlich und gut dokumentierbar\n\nNachteile:\n\nUnterschiedliche Auswahlwahrscheinlichkeiten in jedem Stratum\nKein Effizienzgewinn (unter Umstand. sogar -verlust), wenn Selektionswahrscheinlichkeiten und Zielvariablen nicht (bzw. negativ) korreliert sind\nMehr Dokumentationsaufwand"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#diskussion-wie-wollen-wir-als-gruppe-akten-für-die-archivierung-auswählen",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#diskussion-wie-wollen-wir-als-gruppe-akten-für-die-archivierung-auswählen",
    "title": "ADA Bayern",
    "section": "8 Diskussion: Wie wollen wir als Gruppe Akten für die Archivierung auswählen?",
    "text": "8 Diskussion: Wie wollen wir als Gruppe Akten für die Archivierung auswählen?\n\nEinfache Zufallsstichprobe vs. stratifiziert?\nWenn stratifiziert, wie wählen wir die Strata aus?\nOder gar keine Zufallsstichprobe (zum Beispiel zehn Akten je Gericht mit den meisten Beteiligten?)\nWie viele Akten wählen wir?\nWelche Informationen zum Auswahlprozess müssen wir dokumentieren und kommunizieren?"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#footnotes",
    "href": "workshops/ws1/notebooks/ADA_Modul_3_Best_of_Stichprobenziehung_anonymisiert.html#footnotes",
    "title": "ADA Bayern",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMit der Funktion source() können wir R Code aus anderen Dateien ausführen. Hier liegt der Code in der Datei ADA_daten_vorbereiten.R . In der Datei ist der Code den wir in Modul 2 zum Laden der Software Pakete sowie zur Vobereitung der Daten verwendet haben.↩︎\nWir nutzen dafür nrow() Funktion um die Anzahl (n) der Zeilen (row) im Datensatz akten zählen.↩︎\nWir generieren ein neues Objekt n dem wir den Wert 200 zuweisen.↩︎\nDafür berechnen wir den Quotienten \\(n\\)/\\(N\\) und multiplizieren in mit 100 um Prozentangaben zu bekommen.↩︎\nDafür verwenden wir die Funktion sample() (das englische “to draw a sample” bedeutet “eine Stichprobe ziehen”). Wir nehmen die Einträge in der Index Spalte und ziehen eine Zufallsstichprobe mit der Stichprobengröße \\(n\\) (size = n) ohne Zurücklegen (replace = FALSE).↩︎\nMit akten$Index %in% Index_stichprobe wird einzeln für jede Zeile (akten$Index) überprüft, ob diese Zeile in (Operator %in%) der gezogenen Stichprobe (Index_stichprobe) ist. Mithilfe der eckigen Klammern (akten[c(TRUE, FALSE, …, FALSE),]) werden nur diejenigen Zeilen aus der akten-Tabelle zurückgegeben, wo dies der Fall ist. Das Ergebnis der Auswahl sind die 200 Akten in unserer Stichprobe.↩︎\nMit set.seed wird zunächst ein Seed gesetzt, so dass immer dieselbe Stichprobe gezogen wird.\nBezirk soll unsere Stratumsvariable sein. Daher werden mittels group_by die nachfolgenden Schritte für jedes Gericht einzeln getrennt voneinander durchgeführt.\nAlle Akten eines jeden Gerichts werden durchnummeriert von 1 bis zur Gesamtzahl der Akten vom jeweiligen Gericht n(), die Nummern werden anschließend mittels sample gemischt und jede Akte bekommt in der Variablen samp eine Nummer zugewiesen. Akten mit einer Nummer kleiner oder gleich 10 (vorgegeben in der Variablen anzahl_akten_pro_gericht) werden herausgefiltert und bleiben in der Stichprobe erhalten.\nAbschließend wird die Hilfsvariable samp gelöscht, indem ihr der Wert NULL zugewiesen wird.↩︎\nDie Funktionen svydesign, svymean und confint kommen aus dem survey-package. Mit svydesign lässt sich das Design der Stichprobe beschreiben. Die anderen Funktionen nutzen dieses Design zur Berechnung vom gewichteten Mittelwert bzw. zur Berechnung des Konfidenzintervalls.↩︎"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine einfache Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziel-des-notebooks",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziel-des-notebooks",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir eine einfache Zufallsstichprobe zu ziehen. Das Ziel ist es dieses Wissen für das Projekt zu nutzen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#software-pakete-laden",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#software-pakete-laden",
    "title": "ADA Bayern",
    "section": "2 Software-Pakete laden",
    "text": "2 Software-Pakete laden\nZunächst laden wir einige R-Pakete, die uns bei der Analyse helfen.\nR-Pakete sind Erweiterungen, Sammlungen von nützlichem Code die im CRAN (Comprehensive R Archive Network) zur Verfügung gestellt werden.\nIn diesem Modul nutzen wir das sogenannte tidyverse1, das viele nützliche und einfach verständliche (tidy!) Funktionen zum Arbeiten mit Datensätzen bereitstellt.\nDas survey2 Paket hilft uns später bei der Stichprobenziehung. Das Paket stellt viele nützliche Funktionen für die Analyse von Daten aus Stichproben bereit.\nDas PracTools3 Paket beeinhaltet weitere nützliche Funktionen zur Analyse komplexerer Stichprobendesigns.\nWir haben die Pakete bereits alle in der Arbeitsumgebung installiert4. Installierte Pakete können dann mit der Funktion library() für unser Notebook verfügbar gemacht werden.\n\nlibrary(\"tidyverse\")\nlibrary(\"survey\")\nlibrary(\"PracTools\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#einlesen-der-daten",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#einlesen-der-daten",
    "title": "ADA Bayern",
    "section": "3 Einlesen der Daten",
    "text": "3 Einlesen der Daten\nIn diesem Schritt lesen wir die Daten in R ein5.\n\npfad &lt;- file.path(\".\")\ncol_types &lt;- c(\"ccccnccccccccccccccccccTTcnTccnTcTcccc\")\n\nforumstar_daten &lt;- read_csv(file.path(pfad, \"230817_Abfrage_Januar-Dezember.csv\"), col_types = col_types)\nkarte_amtsgerichte &lt;- readRDS(file.path(pfad, \"230911_Karte_Amtsgerichtsbezirke.RDS\"))"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#daten-für-die-stichprobenziehung-vorbereiten",
    "title": "ADA Bayern",
    "section": "4 Daten für die Stichprobenziehung vorbereiten",
    "text": "4 Daten für die Stichprobenziehung vorbereiten\nFür die Stichprobenziehung benötigen wir nicht alle Spalten des Datensatzes. Auch brauchen wir nur einen einzigen Eintrag pro Akte. Die beteiligten Personen interessieren uns hier zunächst nicht. Daher bereiten wir die Daten zunächst vor.\nMit dem folgenden Code aggregieren wir die Daten auf einen Eintrag pro Aktenzeichen6.\n\nakten &lt;- forumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`,\n    `Streitwert in EURO`,\n    `Gesamtstreitgegenstand`,\n    `Erledigungsgrund`,\n    `Dauer des Verfahrens in Tagen`,\n    `Archivstatus`,\n    `Anbietungsgrund (manuell erfasst)`,\n    `Anbietungsgrund`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nUm unsere visuelle Darstellung auf Karten zu ermöglichen generieren wir außerdem die Spalte Bezirk und fügen sie unserem Datensatz hinzu.\n\ntmp_Gericht &lt;- sub(\"Amtsgericht \", \"\", akten$Gericht)\ntmp_Gericht &lt;- sub(\" Zweigstelle.*\", \"\", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.d. \", \" i. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" a.d. \", \" a. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" am \", \" a. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.OB\", \" i. OB\", tmp_Gericht)\n\nakten &lt;- akten %&gt;%\n  mutate(`Bezirk` = tmp_Gericht)"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziehung-einer-einfachen-zufallsstichprobe",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#ziehung-einer-einfachen-zufallsstichprobe",
    "title": "ADA Bayern",
    "section": "5 Ziehung einer einfachen Zufallsstichprobe",
    "text": "5 Ziehung einer einfachen Zufallsstichprobe\nFür die Ziehung einer einfachen Zufallsstichprobe benötigen wir eine wohl definierte Grundgesamtheit. In unserem Fall bilden alle Akten aus dem Jahr 2018 (alle bayerischen Amtsgerichte, nur Registerzeichen C) die Grundgesamtheit. Der Umfang der Grundgesamtheit, hier die Anzahl aller Akten in einem Jahr, wird mit \\(N\\) bezeichnet.\nAußerdem benötigen wir die Stichprobengröße, also die Anzahl der Akten die ausgewählt werden sollen. Die Stichprobengröße wird mit \\(n\\) bezeichnet.\nIm folgenden Code lesen wir zunächst die Anzahl der Akten aus7.\n\nN &lt;- akten %&gt;% \n  nrow()\n\nN\n\nDann legen wir den Stichprobenumfang \\(n\\) auf 200 fest8.\n\nn &lt;- 200\nn\n\nAls Auswahlwahrscheinlichkeit bezeichnet man die Wahrscheinlichkeit, mit der eine Einheit (hier eine Akte, technisch eine Zeile) in die Stichprobe gelangt9.\n\n100 * n/N\n\nMit welcher Wahrscheinlichkeit (in Prozent) wird eine Akte ausgewählt?\nNun können wir eine einfache Zufallsstichprobe ziehen10. Als Ergebnis erhalten wir \\(n=200\\) zufällig gewählte Werte aus der Spalte Index.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\nWenn wir den gleichen Code noch einmal ausführen, bekommen wir eine neue Zufallsstichprobe.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\n\n\n5.1 Exkurs: Startwerte für den Zufallszahlengenerator setzen\nUm die Ziehung reproduzierbar zu machen, können wir auch einen Startwert für den Zufallszahlengenerator setzen11.\n\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\nWenn wir den selben Startwert setzen, erhalten wir jetzt jedes Mal dieselbe Stichprobe.\n\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nIndex_stichprobe\n\n\nMit den Indizes in der Stichprobe können wir nun die entsprechenden Zeilen in unserem Datensatz akten auswählen. Das machen wir über einen einfachen Abgleich der Indizes12.\n\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\n\n\n\n5.2 Wie groß sollte unsere Stichprobe sein?\nAls einfache Regel gilt, je größer die Stichprobe, desto genauer können wir die Grundgesamtheit abbilden.\nWas bedeutet “genau”? Jede Wiederholung der Stichprobenziehung ergibt eine neue Stichprobe. Das heißt, die Akten, die Teil der Stichprobe sind, unterscheiden sich von Wiederholung zu Wiederholung. Berechnen wir nun, zum Beispiel, einen Mittelwert auf Grundlage der Stichprobe, unterscheiden sich die Ergebnisse der Berechnung. Je größer die Stichprobe ist, desto kleiner werden die Unterschiede und desto näher wird (in der Regel) das Stichprobenmittel dem Mittelwert aus der Grundgesamtheit kommen.\n\n\n5.3 Beispiel zur Stichprobengröße\nHier berechnen wir den mittleren Streitwert (auch Median genannt), um den in unserer Grundgesamtheit üblicherweise gestritten wird 13.\n\nround(median(akten$`Streitwert in EURO`, na.rm = TRUE))\n\nDies ist der wahre Wert aus der Grundgesamtheit, an dem üblicherweise Interesse besteht. Idealerweise könnte man alle Akten aufbewahren und so diesen Mittelwert (oder beliebige andere Maße) jederzeit neu berechnen.\nAllerdings kann nur eine Stichprobe gezogen/archiviert werden. Der mittlere Wert in der Stichprobe wird nur ungefähr dem Wert in der Grundgesamtheit entsprechen14.\n\nn &lt;- 200\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\nMit einer anderen Stichprobe erhalten wir einen anderen Wert.\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\nWenn wir dies nun sehr oft (zum Beispiel 1000 mal) durchführen, können wir betrachten, wie genau wir den mittleren Wert aus unseren Stichproben schätzen15.\n\nverteilung_200 &lt;- sapply(1:1000, function(x) {\n  Index_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\n  akten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe,]\n  round(median(akten_stichprobe$`Streitwert in EURO`, na.rm = TRUE))\n})\n\nDiese Verteilung können wir dann visuell in einem sogenannten Histogram darstellen16.\n\nggplot(data.frame(Median = verteilung_200), aes(x = Median)) + \n  geom_histogram() + \n  theme_minimal()\n\nZum Vergleich ziehen wir nun zunächst zwei einfache Zufallsstichproben mit dem Stichprobenumfang \\(n = 1000\\).\n\nn &lt;- 1000\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\n\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\nakten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe, ]\nround(median(akten_stichprobe$`Streitwert in EURO`, na.rm=TRUE))\n\nDann wiederholen wir den Prozess wieder 1000 mal.\n\nverteilung_1000 &lt;- sapply(1:1000, function(x) {\n  Index_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\n  akten_stichprobe &lt;- akten[akten$Index %in% Index_stichprobe,]\n  round(median(akten_stichprobe$`Streitwert in EURO`, na.rm = TRUE))\n})\n\nDiese Verteilung können wir dann visuell in einem sogenannten Histogram darstellen.\n\nggplot(data.frame(Median = verteilung_1000), aes(x = Median)) + \n  geom_histogram() + \n  theme_minimal()\n\nWie erwartet sind die geschätzten Werte in den größeren Stichproben deutlich näher am Median der Grundgesamtheit (1134) als in den kleineren Stichproben. Eine größere Stichprobe führt zu verbesserter Genauigkeit.\n\n\n5.4 Graphische Darstellung\nNun wollen wir die einfache Zufallsstichprobe vergleichen, mit dem was aktuell angeboten wird. Dazu betrachten wir den Anteil angebotener Akten pro Gericht und stellen dies dar. Berechne zunächst die Anteile.\n\nanteil_anzubietend &lt;- akten %&gt;%\n  mutate(AkteAnzubieten = case_when(\n    `Archivstatus` == \"anzubieten\" ~ \"Ja\",\n    is.na(`Archivstatus`) ~ \"Nein\",\n    TRUE ~ \"Nein\")) %&gt;%\n  group_by(`Bezirk`, `AkteAnzubieten`, .drop = FALSE) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(anteil = n / sum(n)) %&gt;%\n  filter(`AkteAnzubieten` == \"Ja\")\n\nanteil_anzubietend\n\nNun stellen wir die Anteile auf einer Karte dar.\n\n# In Karte übernehmen\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anteil_anzubietend$anteil[match(karte_amtsgerichte$court, anteil_anzubietend$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\n# und Plotten\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nSchweinfurt und Kitzingen bieten einen besonders hohen Anteil an, mehr als 3 Prozent! Von vielen Gerichten wird aber auch keine einzige Akte angeboten.\nWoran liegt diese ungleiche Verteilung?\nNun sollen \\(n = 372\\) Akten (so viele wie 2018 angeboten wurden) zufällig gezogen werden. In dem Indikator inSample in akten_kopie wird abgespeichert, ob die Akte gezogen wurde oder nicht.\n\nn &lt;- 372\nset.seed(20230919)\nIndex_stichprobe &lt;- sample(akten$Index, size = n, replace = FALSE)\n\nakten_kopie &lt;- akten\nakten_kopie$inSample[akten$Index %in% Index_stichprobe] &lt;- \"Ja\"\n\nBerechne die Anteile der zufällig ausgewählten Akten in jedem Bezirk.\n\nanteil_anzubietend_zufaellig &lt;- akten_kopie %&gt;%\n  mutate(AkteAnzubieten = case_when(\n    `inSample` == \"Ja\" ~ \"Ja\",\n    is.na(`inSample`) ~ \"Nein\",\n    TRUE ~ \"Nein\")) %&gt;%\n  group_by(`Bezirk`, `AkteAnzubieten`, .drop = FALSE) %&gt;%\n  summarise(n = n()) %&gt;%\n  mutate(anteil = n / sum(n)) %&gt;%\n  filter(`AkteAnzubieten` == \"Ja\")\n\nanteil_anzubietend_zufaellig\n\nAuf einer Karte können wir nun erneut darstellen, wie viele Akten angeboten werden wenn dies rein zufällig erfolgen würde.\n\n# in Karte übernehmen\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anteil_anzubietend_zufaellig$anteil[match(karte_amtsgerichte$court, anteil_anzubietend_zufaellig$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\n# und plotten\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nDie meisten Gerichte würden bei zufälliger Auswahl mindestens eine Akte anbieten. Man beachte aber auch die Skala: Es ist höchst unwahrscheinlich, dass ein Gericht auch nur 1% seiner Akten zur Archivierung anbieten würde."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-einfachen-zufallsstichprobe",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#diskussion-was-muss-berücksichtigt-werden-was-sind-vor--und-nachteile-einer-einfachen-zufallsstichprobe",
    "title": "ADA Bayern",
    "section": "6 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer einfachen Zufallsstichprobe?",
    "text": "6 Diskussion: Was muss berücksichtigt werden? Was sind Vor- und Nachteile einer einfachen Zufallsstichprobe?\n\nWelche Faktoren beeinflussen, wie groß die Stichprobe sein kann?\nAls Grundgesamtheit wurde hier “alle C-Verfahren in einem Jahr in Bayern” verwendet. Ist dies so sinnvoll und praktikabel oder gibt es bessere Alternativen?\nIst die einfache Zufallsstichprobe zur Auswahl von Akten geeignet? Welche Vor- und Nachteile hat das Verfahren?\n\n\n6.0.1 Vorteile\n\nkeine systematische Auswahl von Akten (z.B. anhand von Nachnamen)\nRepräsentative Stichprobe\nAlle Akten haben dieselbe Auswahlwahrscheinlichkeit\n\n\n\n6.0.2 Nachteile\n\nNicht alle Gerichte/Landkreise/Regionen tauchen immer in der Stichprobe auf\nAlle Akten haben dieselbe Auswahlwahrscheinlichkeit"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#footnotes",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_2_Einfache_Zufallsstichprobe_anonymisiert.html#footnotes",
    "title": "ADA Bayern",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWickham H, Bryan J (2022). readxl: Read Excel Files. R package version 1.4.0, https://CRAN.R-project.org/package=readxl.↩︎\nLumley T (2010). Complex Surveys: A Guide to Analysis Using R: A Guide to Analysis Using R. John Wiley and Sons.↩︎\nValliant, R., Dever, J., Kreuter, F. (2018). Practical Tools for Designing and Weighting Survey Samples, 2nd edition. New York: Springer.↩︎\nFalls Sie auf Ihrem eigenen Rechner ein neues Paket installieren wollen, können Sie das mit der Funktion install.packages(). Im Funktionsaufruf müssen Sie dann nur den Paketnamen angeben. Wenn Sie also, zum Beispiel, das Paket PracTools installieren wollen, lautet der Funktionsaufruf: install.packages(\"PracTools\").↩︎\nDie Forumstar Daten liegen uns in einer .csv Datei vor. Diese können wir mit der Funktion read_csv einlesen. Zunächst definieren wir den Pfad zum Ordner mit den Daten im Objekt pfad, dafür nutzen wir die Funktion file.path(), dort geben wir Schritt für Schritt den Weg zum Ordner mit den Daten an. Dann definieren wir dafür die verschiedenen Datentypen in jeder Spalte. Ein c steht für character, also Textdaten, wie zum Beispiel in Namensfeldern. n steht für numerische Werte, T steht für eine Datumsangabe mit Uhrzeit (von “Time”). Diese Informationen speichern wir im Objekt col_types. Anschließend lesen wir den Datensatz mit der Funktion read_csv() direkt aus dem Ordner in unserer sicheren Datenumgebung ein. Wir können die Daten dann später über das Objekt forumstar_daten aufrufen.\nWir lesen die Kartendaten ebenfalls gleich ein.↩︎\nDazu verwenden wir die group_by Funktion. Zusätzlich erstellen wir mithilfe der Funktion summarise()eine neue Spalte Anzahl Beteiligte. Anschließend fügen wir noch eine eindeutige Index Spalte Index für jede Akte hinzu.↩︎\nWir nutzen dafür nrow() Funktion um die Anzahl (n) der Zeilen (row) im Datensatz akten zählen.↩︎\nWir generieren ein neues Objekt n dem wir den Wert 200 zuweisen.↩︎\nDafür berechnen wir den Quotienten \\(n\\)/\\(N\\) und multiplizieren in mit 100 um Prozentangaben zu bekommen.↩︎\nDafür verwenden wir die Funktion sample() (das englische “to draw a sample” bedeutet “eine Stichprobe ziehen”). Wir nehmen die Einträge in der Index Spalte und ziehen eine Zufallsstichprobe mit der Stichprobengröße \\(n\\) (size = n) ohne Zurücklegen (replace = FALSE).↩︎\nDafür nutzen wir die Funktion set.seed().↩︎\nMit akten$Index %in% Index_stichprobe wird einzeln für jede Zeile (akten$Index) überprüft, ob diese Zeile in (Operator %in%) der gezogenen Stichprobe (Index_stichprobe) ist. Mithilfe der eckigen Klammern (akten[c(TRUE, FALSE, …, FALSE),]) werden nur diejenigen Zeilen aus der akten-Tabelle zurückgegeben, wo dies der Fall ist. Das Ergebnis der Auswahl sind die 200 Akten in unserer Stichprobe.↩︎\nWir berechnen den Median mit der Funktion median() . na.rm = TRUE gibt an, dass Akten bei der Berechnung ignoriert werden, wenn dort kein Wert angegeben ist. Andernfalls können wir keine Berechnung durchführen.↩︎\nDer hier verwendete Code wurde im Detail bereits oben vorgestellt.↩︎\nMit der Funktion sapply() wenden wir eine Funktion wiederholt, hier über den Vektor der Zahlen von 1 bis 1000 (1:1000) an. Die Funktion die wir anwenden sind die drei Schritte aus den vorherigen Beispielen um eine Stichprobe mit Umfang \\(n\\) = 200 zu ziehen.↩︎\nDafür nutzen wir die Funktion ggplot. Zunächst müssen wir unsere Verteilung als data.frame an die Funktion übergeben, dabei benennen wir die Spalte Median. Diese Spalte lassen wir uns dann anzeigen. Mit geom_histogram() definieren wir, dass wir ein Histogram erstellen möchten. theme_minimal() verändert das Aussehen der visuellen Darstellung. (Sie können gerne ausprobieren, was ohne theme_minimal() passiert.)↩︎"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir zunächst Quarto und R kennen. Anschließend führen wir erste Analysen mit den forumSTAR Daten durch. Das Ziel dabei ist es, insbesondere, die Daten gut zu verstehen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#ziel-des-notebooks",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#ziel-des-notebooks",
    "title": "ADA Bayern",
    "section": "",
    "text": "In diesem Notebook lernen wir zunächst Quarto und R kennen. Anschließend führen wir erste Analysen mit den forumSTAR Daten durch. Das Ziel dabei ist es, insbesondere, die Daten gut zu verstehen.\nZur Anonymisierung wurde dieses File nachträglich verändert!"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quarto-und-r",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quarto-und-r",
    "title": "ADA Bayern",
    "section": "2 Quarto und R",
    "text": "2 Quarto und R\nDieses Dokument ist ein sogennantes Quarto Dokument. Wir können Quarto Dokumente an der Dateiendung .qmd erkennen und direkt in RStudio öffnen.\nJedes Quarto Dokument besteht im Wesentlichen aus vier Bausteinen. Header, Text, Code Eingabe und Code Ausgabe.\n\n2.1 Header\nDer Header steht immer am Anfang eines Quarto Dokuments. Hier definieren wir den Titel und Untertitel unseres Dokuments. Außerdem können wir hier einstellen wie aus dem vorliegenden .qmd Dokument zum Beispiel ein pdf oder docx Dokument generiert werden soll. Mehr dazu lernen wir in Modul 3.\n\n\n2.2 Text\nDas hier ist ganz normaler Text. Wie in anderen Editoren (zum Beispiel Word), können wir den Text nach unserem Geschmack formatieren.\n\n\n2.3 Code Eingabe\nMit einem Klick auf den grünen Button (C mit kleinem +-Symbol) oben in der Leiste erstellen wir eine R Code Zelle. In dieser können wir ausführbaren R Code schreiben. Mit einem Klick auf das grüne Dreieck rechts an der Code Zelle führen wir den Code aus1.\n\n1 + 1\n\nDabei ist Quarto nicht auf die Programmiersprache R beschränkt. Wir könnten, zum Beispiel, auch ausführbaren Python Code in ein Quarto Dokument integrieren.\n\n\n2.4 Code Ausgabe\nSobald wir Code ausführen, erscheint das Ergebnis (wie oben nach einem Klick auf das grüne Dreieck) ebenfalls direkt im Quarto Dokument."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#software-pakete-laden",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#software-pakete-laden",
    "title": "ADA Bayern",
    "section": "3 Software-Pakete laden",
    "text": "3 Software-Pakete laden\nZunächst laden wir einige R-Pakete, die uns bei der Datenanalyse helfen.\nR-Pakete sind Erweiterungen, Sammlungen von nützlichem Code die im CRAN (Comprehensive R Archive Network) zur Verfügung gestellt werden.\nIn diesem Modul nutzen wir das sogenannte tidyverse2, das viele nützliche und einfach verständliche (tidy!) Funktionen zum Arbeiten mit Datensätzen bereitstellt.\nAußerdem nutzen wir das Paket sf3 Paket um die Daten auf Karten zu visualisieren4.\n\nlibrary(\"tidyverse\")\nlibrary(\"sf\")"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#daten-einlesen",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#daten-einlesen",
    "title": "ADA Bayern",
    "section": "4 Daten einlesen",
    "text": "4 Daten einlesen\nUm unsere Daten in R nutzen zu können, müssen wir diese zunächst einlesen5.\n\npfad &lt;- file.path(\".\")\ncol_types &lt;- c(\"ccccnccccccccccccccccccTTcnTccnTcTcccc\")\n\nforumstar_daten &lt;- read_csv(file.path(pfad, \"230817_Abfrage_Januar-Dezember.csv\"), col_types = col_types)\n\nZusätzlich laden wir eine Datei mit einer Karte der Amtsgerichtsbezirke. Diese verwenden wir später für erste Analysen.\n\nkarte_amtsgerichte &lt;- readRDS(file.path(pfad, \"230911_Karte_Amtsgerichtsbezirke.RDS\"))"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#überblick-über-die-daten",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#überblick-über-die-daten",
    "title": "ADA Bayern",
    "section": "5 Überblick über die Daten",
    "text": "5 Überblick über die Daten\nUm die Daten besser zu verstehen, macht es Sinn diese zu betrachten. Aber wie machen wir das in R? Es gibt viele Möglichkeiten, wovon wir hier eine sehr gute Möglichkeit beispielhaft zeigen6.\n\nforumstar_daten %&gt;% \n  glimpse()\n\nWas sind die Namen der Spalten des Datensatzes?\nIm Datensatz sind vier Spalten mit Zahlenwerten enthalten. Welche sind das?\nAußerdem können wir uns die ersten Zeilen des Datensatzes ansehen. Mit einem Klick auf das schwarze Dreieck rechts über der Tabelle können wir die weiteren Spalten sehen7.\n\nforumstar_daten %&gt;% \n  head()\n\nFür die Analysen ist es wichtig zu verstehen, was eine Zeile im Datensatz abbildet. Was bildet eine Zeile in unserem Datensatz ab? Betrachten Sie dazu die Spalten Aktenzeichen, Verfahrensbeteiligungsart und Name8.\n\nforumstar_daten %&gt;% \n  select(`Aktenzeichen`, `Verfahrensbeteiligungsart`, `Name`)\n\nAktenzeichen tauchen mehrmalig auf. Was bedeutet das?"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#erste-analysen",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#erste-analysen",
    "title": "ADA Bayern",
    "section": "6 Erste Analysen",
    "text": "6 Erste Analysen\n\n6.1 Welche Gerichte sind im Datensatz?\nMöchten wir wissen, welche und wie viele Gerichte in unserem Datensatz vertreten sind, können wir dies auch erfahren9.\n\nforumstar_daten %&gt;%\n  select(`Gericht`) %&gt;%\n  unique()\n\n\n\n6.2 Wie viele Akten haben wir pro Gericht?\nWir möchten nun wissen, mit wie vielen Akten jedes Gericht im Datensatz vertreten ist. Wir benötigen hierfür einen Datensatz, der nur eine Zeile pro Akte beinhaltet (statt zuvor einer Zeile pro Prozessbeteiligten) beinhaltet. Mit dem folgenden Code aggregieren wir die Daten auf einen Eintrag pro Gericht und Aktenzeichen10.\n\nforumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nZusätzlich wollen wir noch weitere Informationen behalten, die für jede Akte vorliegen, aber nicht personenspezifisch sind. Die entsprechenden Spaltennamen nehmen wir in group_by() auf. Das Ergebnis speichern wir in einem neuen Datensatz akten zur weiteren Analyse ab.\n\nakten &lt;- forumstar_daten %&gt;%\n  group_by(\n    `Gericht`,\n    `Aktenzeichen`,\n    `Streitwert in EURO`,\n    `Gesamtstreitgegenstand`,\n    `Erledigungsgrund`,\n    `Dauer des Verfahrens in Tagen`,\n    `Archivstatus`,\n    `Anbietungsgrund (manuell erfasst)`,\n    `Anbietungsgrund`\n  ) %&gt;%\n  summarise(`Anzahl Beteiligte` = n()) %&gt;%\n  as.data.frame() %&gt;%\n  mutate(Index = row_number())\n\nNun können wir die Anzahl der Akten pro Amtsgericht (bzw. Zweigstelle) zählen11.\n\nakten %&gt;% \n  group_by(Gericht) %&gt;% \n  summarise(Anzahl = n())\n\nDas gleiche können wir auch visuell machen12.\n\nggplot(akten, aes(Gericht)) + \n  geom_bar() + \n  scale_x_discrete(guide = guide_axis(angle = 90))\n\nDatenprobleme erkennen: Macht es Sinn die Zweigstellen gesondert aufzuführen?\nWir können uns auch eine numerische Zusammenfassung des Datensatzes anzeigen lassen13.\n\nakten %&gt;% \n  summary()\n\nDatenprobleme erkennen: Ist es möglich, dass die tatsächliche Dauer des Verfahrens in Tagen negativ ist? Wie viele Jahre dauert ein Verfahren mit 17164 Tagen? Ist das ein realistischer Zeitraum?"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#die-daten-visuell-kennenlernen",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#die-daten-visuell-kennenlernen",
    "title": "ADA Bayern",
    "section": "7 Die Daten visuell kennenlernen",
    "text": "7 Die Daten visuell kennenlernen\nWir können die Daten auch auf einer Karte darstellen. Wir haben dazu eine Karte mit den bayerischen Amtsgerichtsbezirken vorbereitet.\nDie Karte ist zunächst ein zusätzlicher Datensatz, den wir oben in R eingelesen haben. Wie bei den forumSTAR Daten können wir uns zunächst einen Überblick verschaffen14.\n\nkarte_amtsgerichte %&gt;%\n  glimpse()\n\nIn diesem Datensatz haben wir drei Variablen. court ist der Name des Amtsgerichtsbezirks, geometry definiert die Grenzen der einzelnen Amtsgerichtsbezirke und tile_map ist eine alternative Darstellung der Amtsgerichtsbezirke als gleich große Kacheln (sogenannte tiles).\nBei einer Karte macht es natürlich besonders Sinn die Daten zu visualisieren15.\n\nggplot(data = karte_amtsgerichte, aes(fill = court)) +\n  geom_sf(aes(geometry = geometry), color = \"white\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    color = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) + scale_fill_viridis_d(option = \"viridis\") +\n  guides(fill=\"none\") +\n  theme_void()\n\nAufgabe: Visualisieren Sie die Amtsgerichtsbezirke mit den gleich großen Kacheln (tile_map) anstatt der geographischen Grenzen (geometry) . Als Starthilfe nehmen wir den gleichen Code wie für die Karte oben. Sie müssen die Spalte geometry an zwei Stellen austauschen.\n\nggplot(data = karte_amtsgerichte, aes(fill = court)) +\n  geom_sf(aes(geometry = tile_map), color = \"white\") +\n  geom_sf_text(\n    aes(geometry = tile_map, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) + scale_fill_viridis_d(option = \"viridis\") +\n  guides(fill=\"none\") +\n  theme_void()\n\nNun wollen wir die Anzahl der Akten pro Gericht auf der Karte darstellen. Dazu müssen wir die Gerichte in unserem Datensatz akten den Amtsgerichtsbezirken im Datensatz karte_amtsgerichte eindeutig zuordnen.\nWenn wir die Namen der Gerichte in den beiden Datensätzen betrachten, fällt auf, dass die Namen unterschiedlich sind.\nZunächst betrachten wir die Namen in unserem Datensatz akten 16.\n\nakten %&gt;% \n  select(`Gericht`) %&gt;%\n  unique()\n\nDann betrachten wir die Namen der Amtsgerichtsbezirke im Datensatz karte_amtsgerichte 17.\n\nkarte_amtsgerichte$court\n\nDatenprobleme erkennen: Wie können wir die verschiedenen Namen der Gerichte automatisiert zusammenführen?\nZunächst löschen wir das “Amtsgericht” vor dem Namen des Amtsgerichtsbezirks18.\n\ntmp_Gericht &lt;- sub(\"Amtsgericht \", \"\", akten$Gericht)\n\ntmp_Gericht %&gt;% \n  unique()\n\nGenauso können wir das Wort “Zweigstelle” (und alle folgenden Buchstaben) aus dem Namen entfernen19.\n\ntmp_Gericht &lt;- sub(\" Zweigstelle.*\", \"\", tmp_Gericht)\n\ntmp_Gericht %&gt;% \n  unique()\n\nNun müssen wir noch leicht unterschiedliche Schreibweisen anpassen. In forumstar_daten steht zum Beispiel die Abkürzung “i.d”, in den karten_amtsgerichte Daten dagegen “i. d.”. Für uns ist trotz der leicht unterschiedlichen Schreibweise eine eindeutige Zuordnung möglich. Für den Computer ist das hier schwieriger. Das “i.d.” können wir auch einfach durch ein “i. d.” austauschen. Wir wiederholen das dann für alle anderen Abkürzungen mit unterschiedlichen Schreibweisen20.\n\ntmp_Gericht &lt;- sub(\" i.d. \", \" i. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" a.d. \", \" a. d. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" am \", \" a. \", tmp_Gericht)\ntmp_Gericht &lt;- sub(\" i.OB\", \" i. OB\", tmp_Gericht)\n\ntmp_Gericht %&gt;% \n  unique()\n\nNun können wir die neuen Namen der Gerichte als eine neue Spalte mit dem Namen Bezirk zu unserem Datensatz akten hinzufügen21.\n\nakten &lt;- akten %&gt;%\n  mutate(`Bezirk` = tmp_Gericht)\n\nDie neue Spalte Bezirk` können wir nutzen um die Daten auf der Ebene der Amtsgerichtsbezirke zu aggregieren. Zweigstellen sind nun ihrem Bezirk zugeordnet22.\n\nfallzahl_pro_bezirk &lt;- akten %&gt;% \n  group_by(Bezirk) %&gt;% \n  summarise(Anzahl = n()) %&gt;%\n  as.data.frame()\n\nDie aggregierten Daten stellen fügen wir dann als Information zu unserem Datensatz karte_amtsgerichte hinzu23.\n\nkarte_amtsgerichte$`Anzahl der Fälle` &lt;- fallzahl_pro_bezirk$Anzahl[match(karte_amtsgerichte$court, fallzahl_pro_bezirk$Bezirk)]\n\nJetzt visualisieren wir die Anzahl der Fälle pro Bezirk auf einer Karte24.\n\nggplot(data = karte_amtsgerichte, aes(fill = `Anzahl der Fälle`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nIm Amtsgerichtsbezirk München ist wirklich einiges los! Sogar so viel mehr, dass wir bei den meisten Gerichten mit weniger als 5000 Akten auf der Karte gar keinen Unterschied sehen können.\nFür so “schiefe Daten” bietet sich eine logarithmische Transformation der Farbskala an25.\n\nggplot(data = karte_amtsgerichte, aes(fill = `Anzahl der Fälle`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(trans = \"log\", option = \"viridis\") +\n  theme_void()\n\n\n7.1 Welche Akten sind anzubieten?\nEine der zentralen Fragen in unserem Projekt ist: welche Verfahrensakten sollen den Archiven angeboten werden und warum? Rechtspfleger:innen geben manchmal eine Begründung ein, wenn sie entscheiden, dass ein Fall archiviert werden sollte. Um diese Begründungen anzuzeigen, erstellen wir zunächste einen kleineren Datensatz mit nur denjenigen Akten, die von den Rechtspfleger:innen als anzubieten gekennzeichnet wurden26.\n\nanzahl_anzubietend &lt;- akten %&gt;% \n  group_by(`Bezirk`, .drop = FALSE) %&gt;%\n  filter(`Archivstatus` == \"anzubieten\") %&gt;%\n  count(name = \"Anzahl\") %&gt;%\n  as.data.frame()\n  \nanzahl_anzubietend\n\nWir können die Gesamtzahl der anzubietenden Akten durch das aufsummieren der anzubietenden Akten pro Gericht herausfinden.\n\nanzahl_anzubietend %&gt;%\n  select(Anzahl) %&gt;%\n  sum()\n\nUm die Daten besser zu verstehen, betrachten wir diese wieder in einer grafischen Darstellung.\n\nggplot(anzahl_anzubietend, aes(Bezirk, Anzahl)) + \n  geom_col() + \n  scale_x_discrete(guide = guide_axis(angle = 90))\n\nUnd können die Daten auch auf einer Karte darstellen.\n\nkarte_amtsgerichte$`Anzubietende Akten` &lt;- anzahl_anzubietend$Anzahl[match(karte_amtsgerichte$court, anzahl_anzubietend$Bezirk)]\nkarte_amtsgerichte$`Anzubietende Akten`[karte_amtsgerichte$`Anzubietende Akten` == 0] &lt;- NA\n\nggplot(data = karte_amtsgerichte, aes(fill = `Anzubietende Akten`)) +\n  geom_sf(aes(geometry = geometry), color = \"transparent\") +\n  geom_sf_text(\n    aes(geometry = geometry, label = court),\n    size = 1.5,\n    col = \"black\",\n    fun.geometry = function(x)\n      st_centroid(x)\n  ) +\n  scale_fill_viridis_b(option = \"viridis\") +\n  theme_void()\n\nWas fällt auf?\nNur 48 der 73 Amtsgerichtsbezirke haben in unserem Datensatz Akten mit Archivstatus “anzubieten”. Die Anzahl der anzubietenden Akten scheint nicht mit der Anzahl der Akten zu korrelieren.\nWoran liegt das?\nWarum sollen diese Akten angeboten werden? In zwei Spalten in unserem Datensatz haben wir Informationen zum Anbietungsgrund. Es gibt die Spalte “Anbietungsgrund”, sowie die Spalte “Anbietungsgrund (manuell erfasst)”. Wir erstellen uns je eine Tabelle, die uns sagt, welcher Grund wie oft angegeben wurde27.\n\nakten %&gt;% \n  filter(`Archivstatus` == \"anzubieten\") %&gt;%\n  group_by(Anbietungsgrund) %&gt;% \n  summarise(Anzahl = n())\n\nDas gleiche machen wir mit der Spalte Anbietungsgrund (manuell erfasst) 28.\n\nakten %&gt;% \n  filter(`Archivstatus` == \"anzubieten\") %&gt;%\n  group_by(`Anbietungsgrund (manuell erfasst)`) %&gt;% \n  summarise(Anzahl = n())\n\nNA steht hier für “not available” (nicht verfügbar), d. h. für alle Zeilen, bei denen kein Grund angegeben wurde."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quantitative-informationen-darstellen",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#quantitative-informationen-darstellen",
    "title": "ADA Bayern",
    "section": "8 Quantitative Informationen darstellen",
    "text": "8 Quantitative Informationen darstellen\nWir können auch die quantitativen Informationen visuell darstellen und so die Daten besser verstehen. Zunächst betrachten wir den Streitwert in EURO und die Anzahl Beteiligte 29.\n\nggplot(akten, aes(x=`Streitwert in EURO`, y=`Anzahl Beteiligte`)) + \n  geom_point()\n\nGenauso können wir die Dauer des Verfahrens in Tagen und die Anzahl Beteiligte betrachten. Tauschen Sie dazu eine Spalte im Code für die visuelle Darstellung aus30.\n\nggplot(akten, aes(x=`Dauer des Verfahrens in Tagen`, y=`Anzahl Beteiligte`)) + \n  geom_point()"
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#aufgaben",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#aufgaben",
    "title": "ADA Bayern",
    "section": "9 Aufgaben",
    "text": "9 Aufgaben\nWelche Spalten im Datensatz finden Sie besonders spannend? Welche Spalten könnten sich gut zur Auswahl archivwürdiger Akten eignen?\nSuchen Sie sich gemeinsam mit ihrem Team mindestens zwei Spalten aus und stellen Sie diese visuell dar."
  },
  {
    "objectID": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#footnotes",
    "href": "workshops/ws1/notebooks/ADA_Modul_2_1_Erste-Analysen_anonymisiert.html#footnotes",
    "title": "ADA Bayern",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDas {r} definiert, das die Code Eingabe R Code erwartet. Im Code addieren wir dann 1 und 1.↩︎\nWickham H, Bryan J (2022). readxl: Read Excel Files. R package version 1.4.0, https://CRAN.R-project.org/package=readxl.↩︎\nPebesma, E., & Bivand, R. (2023). Spatial Data Science: With Applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016↩︎\nIm Code rufen wir zunächst mit der Funktion install.packages(), ein R Paket zu installieren, bedeutet, dass wir die Funktionen im Paket auf unserem Computer verfügbar machen. install.packages() müssen wir nur ausführen, wenn wir ein R Paket erstmalig installieren oder updaten möchten. Anschließend müssen wir die Funktionen noch für unser spezifisches Dokument verfügbar machen. Dazu rufen wir die Funktion library() für jedes Paket auf. library() führen wir immer aus, wenn wir eine neue R Session starten.↩︎\nDie Forumstar Daten liegen uns in einer .csv Datei vor. Diese können wir mit der Funktion read_csv einlesen. Zunächst definieren wir den Pfad zum Ordner mit den Daten im Objekt pfad, dafür nutzen wir die Funktion file.path(), dort geben wir Schritt für Schritt den Weg zum Ordner mit den Daten an. Dann definieren wir dafür die verschiedenen Datentypen in jeder Spalte. Ein c steht für character, also Textdaten, wie zum Beispiel in Namensfeldern. n steht für numerische Werte, T steht für eine Datumsangabe mit Uhrzeit (von “Time”). Diese Informationen speichern wir im Objekt col_types. Anschließend lesen wir den Datensatz mit der Funktion read_csv() direkt aus dem Ordner in unserer sicheren Datenumgebung ein. Wir können die Daten dann später über das Objekt forumstar_daten aufrufen.↩︎\nMit glimpse() (“Glimpse” bedeutet auf Deutsch “kurzer Blick”.) erhält man die Anzahl an Zeilen (Rows) und Spalten (Columns) der Daten und kann die Spalten (Variablen) besser verstehen. In den folgenden Zeilen werden die Spaltennamen aufgelistet. Dahinter sehen wir den Datentyp jeder Spalte (&lt;chr&gt; für “character” also Text, &lt;dbl&gt; für Zahlen im “double-precision flouting-point format” und &lt;dttm&gt; für “date-time” also Datumsangaben) und einen ersten Überblick über einige der ersten Zellen jeder Spalte. NA bedeutet dabei, dass eine Zelle in unserem Datensatz leer ist.↩︎\nMit der Funktion head() können wir uns die ersten sechs Zeilen des Datensatzes ansehen. Analog könnten wir mit der Funktion tail() die letzten sechs Zeilen des Datensatzes ansehen.↩︎\nWir nutzen die Funktion select() um Spalten aus unserem Datensatz auszuwählen (auswählen auf Englisch “to select”). Hier wählen wir also die Spalten Aktenzeichen, Verfahrensbeteiligungsart und Name aus.↩︎\nWir nutzen die Funktion select() um die Spalte Gericht auszuwählen. Damit wir nicht alle Zeilen erhalten, sondern jedes Gericht nur einmal, können wir die Funktion unique() (“unique” auf Englisch bedeutet einmalig).↩︎\nMithilfe der group_by Funktion werden die nachfolgenden Schritte (hier nur ein Schritt: summarise) getrennt für jede Kombination aus Gericht und Aktenzeichen ausgeführt. Das heißt in summarise() werden alle Zeilen, die dasselbe Gericht und Aktenzeichen haben, in einer einzigen Zeile zusammengefasst. Zusätzlich zählen wir die Anzahl Zeilen n(). Diese Zahl wird in einer neuen Spalte Anzahl Beteiligte gespeichert.\nAnschließend fügen wir noch mit mutate() eine eindeutige Index Spalte Index anhand der Zeilennummer für jede Akte hinzu. Dafür muss zunächst mit as.data.frame() die Gruppierung aufgehoben werden.↩︎\nDafür gruppieren wir unsere Daten nach der Spalte Gericht und nutzen dann die summarise() Funktion innerhalb jeder Gruppe. Mit der Funktion n() zählen wir so die Anzahl der Zeilen pro Gruppe.↩︎\nggplot() ist eine Funktion zur visuellen Darstellung von Daten (auf Englisch “to plot”), bei der mit +-Symbolen die Details der Darstellung bestimmt werden. geom_bar() erstellt einen Bar-Plot; mit scale_x_discrete(guide = guide_axis(angle = 90)) drehen wir die Namen der Gerichte um 90 Grad, damit diese sich nicht überlappen. Sie können ausprobieren, was passiert wenn Sie den letzten Befehl sowie das letzte +-Symbol weglöschen.↩︎\nDazu nutzen wir die Funktion summary(). Diese Funktion fasst numerische Spalten mit einfachen statistischen Werten zusammen. So können wir das Minimum, das erste Quartil, den Median, den Mittelwert (auf Englisch “mean”), das dritte Quartil, das Maximum der Daten, sowie die Anzahl fehlender Werte (NA’s) auf einen Blick sehen.↩︎\nWir nutzen dafür wieder die Funktion glimpse().↩︎\nZur visuellen Darstellung Nutzen wir wieder die ggplot() Funktion. Hier geben wir als Ausgangsdaten (data) den Datensatz mit der Karte karte_amtsgerichte an, mit aes(fill = court) geben wir an, dass wir jeden Gerichtsbezirk unterschiedlich farbig füllen (auf Englisch “fill”) möchten. aes steht dabei für “aesthetics”. Nun nutzen wir geom_sf() um die Karte anzuzeigen, dafür geben wir an, dass die Informationen zur Geometrie der Bezirke (Option geometry) in der Spalte geometry unseres Datensatzes liegen. Grenzen zwischen Bezirken zeigen wir in der Farbe weiß an (color = \"white\"). Zusätzlich zeigen wir mit geom_sf_text() noch die Namen der Amtsgerichtsbezirke auf der Karte an. Um diese an der richtigen Stelle zu platzieren nutzen wir wieder die Informationen in der Spalte geometry. Für die Beschriftung verwenden wir die Spalte court. Mit der Option size = 1.5 geben wir die Größe der Schrift an und mit der Option color = \"black\" geben wir an, dass wir die Beschriftung in schwarz darstellen. Um nun die Labels richtig zu platzieren, berechnen wir den Mittelpunkt der Amtsgerichtsbezirke mit der Funktion st_centroid(). Diese Funktion wird auf Basis der Information in der Spalte geometry berechnet. Anschließend definieren wir mit sclae_fill_viridis_d(option = \"viridis\") die Farbskala mit der wir die verschiedenen Amtsgerichtsbezirke einfärben möchten (mehr dazu in Modul 3). Mit guides(fill=\"none\") unterdrücken wir die Erstellung einer Farblegende. Diese würde in diesem Fall keine Information hinzufügen, dabei aber sehr viel Platz einnehmen. Zuletzt definieren wir mit theme_void(), dass wir die Karte ohne Koordinatensystem und Hintergrund abbilden möchten.↩︎\nDazu wählen wir zunächst mit der Funktion select() die Spalte Gericht aus und zeigen dann mit der Funktion unique() die einmaligen Werte an.↩︎\nHier nutzen wir den $ Operator, mit dem wir Spalten in einem Datensatz anwählen können. Die Namen der Amtsgerichtsbezirke sind in der Spalte court angeführt.↩︎\nDazu nutzen wir die Funktion sub(). Zunächst geben wir an, welche Zeichenfolge wir suchen und ersetzen möchten, hier die Zeichenfolge “Amtsgericht”. Dann geben wir an, durch welche Zeichenfolge wir die erste Zeichenfolge ersetzen möchten, hier durch eine leere Zeichenfolge ““. Als letztes geben an, wir welche Werte wir verändern möchten, hier möchten wir direkt die Einträge in der Spalte Gericht verändern. Mit dem $ können wir die Spalte in unserem Datensatz akten auswählen. Wir speichern die angepassten Namen im Objekt tmp_Gericht.\nDanach betrachten wir das Ergebnis, indem wir mit der Funktion unique() jeden Namen einmal anzeigen.↩︎\nDafür nutzen wir wieder die Funktion sub(). Hier wollen wir die Zeichenfolge “Zweigstelle.*” durch die leere Zeichenfolge “” ersetzen. Das .* gibt an, dass der Zeichenfolge “Zweigstelle” beliebige Zeichen folgen können, die trotzdem mit ausgetauscht werden.↩︎\nDafür nutzen wir wieder die Funktion sub(). Hier tauschen wir zum Beispiel die Zeichenfolge ” i.d. ” durch ” i. d. ” aus. Die Leerzeichen vor und nach den Buchstaben und Punkten sind wichtig! Leerzeichen gehören auch zur Zeichenfolge.↩︎\nDazu nutzen wir die Funktion mutate` mit der wir neue Spalten zu unserem Datensatz hinzufügen können. Wir nennen die Spalte Bezirk` und ordnen unsere aufgeräumten Namen von oben zu.↩︎\nWir erstellen hier ein neues Objekt fallzahl_pro_bezirk . Dazu gruppieren wir zunächst unseren Datensatz nach der Spalte Bezirk und zählen dann mit den Funktionen summarise() und n() die Anzahl der Akten pro Bezirk. Mit as.data.frame() speichern wir diese Information als einen neuen Datensatz.↩︎\nHier erstellen wir die neue Spalte Anzahl der Fälle mit dem $ Operator. Dafür nutzen wir unseren gerade erstellten Datensatz fallzahl_pro_bezirk und die Spalte Anzahl . Mit der Funktion match() stellen wir sicher, dass die Daten in beiden Datensätzen richtig sortiert sind.↩︎\nZur visuellen Darstellung Nutzen wir wieder die ggplot() Funktion. Hier geben wir als Ausgangsdaten (data) den Datensatz mit der Karte karte_amtsgerichte an, mit aes(fill = Anzahl der Fälle) geben wir an, dass wir Farbabstufungen nach der Anzahl der Fälle (auf Englisch “fill”) möchten. aes steht dabei für “aesthetics”. Nun nutzen wir geom_sf() um die Karte anzuzeigen, dafür geben wir an, dass die Informationen zur Geometrie der Bezirke (Option geometry) in der Spalte geometry unseres Datensatzes liegen. Grenzen zwischen Bezirken zeigen wir nicht an (color = \"transparent\"). Zusätzlich zeigen wir mit geom_sf_text() noch die Namen der Amtsgerichtsbezirke auf der Karte an. Um diese an der richtigen Stelle zu platzieren nutzen wir wieder die Informationen in der Spalte geometry. Für die Beschriftung verwenden wir die Spalte court. Mit der Option size = 1.5 geben wir die Größe der Schrift an und mit der Option color = \"black\" geben wir an, dass wir die Beschriftung in schwarz darstellen. Um nun die Labels richtig zu platzieren, berechnen wir den Mittelpunkt der Amtsgerichtsbezirke mit der Funktion st_centroid(). Diese Funktion wird auf Basis der Information in der Spalte geometry berechnet. Anschließend definieren wir mit sclae_fill_viridis_d(option = \"viridis\") die Farbskala mit der wir die verschiedenen Amtsgerichtsbezirke einfärben möchten (mehr dazu in Modul 3). Zuletzt definieren wir mit theme_void(), dass wir die Karte ohne Koordinatensystem und Hintergrund abbilden möchten.↩︎\nZur visuellen Darstellung Nutzen wir wieder die ggplot() Funktion. Hier geben wir als Ausgangsdaten (data) den Datensatz mit der Karte karte_amtsgerichte an, mit aes(fill = Anzahl der Fälle) geben wir an, dass wir Farbabstufungen nach der Anzahl der Fälle (auf Englisch “fill”) möchten. aes steht dabei für “aesthetics”. Nun nutzen wir geom_sf() um die Karte anzuzeigen, dafür geben wir an, dass die Informationen zur Geometrie der Bezirke (Option geometry) in der Spalte geometry unseres Datensatzes liegen. Grenzen zwischen Bezirken zeigen wir nicht an (color = \"transparent\"). Zusätzlich zeigen wir mit geom_sf_text() noch die Namen der Amtsgerichtsbezirke auf der Karte an. Um diese an der richtigen Stelle zu platzieren nutzen wir wieder die Informationen in der Spalte geometry. Für die Beschriftung verwenden wir die Spalte court. Mit der Option size = 1.5 geben wir die Größe der Schrift an und mit der Option color = \"black\" geben wir an, dass wir die Beschriftung in schwarz darstellen. Um nun die Labels richtig zu platzieren, berechnen wir den Mittelpunkt der Amtsgerichtsbezirke mit der Funktion st_centroid(). Diese Funktion wird auf Basis der Information in der Spalte geometry berechnet. Anschließend definieren wir mit sclae_fill_viridis_d(trans = \"log\", option = \"viridis\") die Farbskala mit der wir die verschiedenen Amtsgerichtsbezirke einfärben möchten (mehr dazu in Modul 3). Mit trans = \"log\" geben wir hier an, dass wir die Werte in der Spalte Anzahl der Fälle logarithmisch transformieren um die Grenzen zwischen den verschiedenen Farben neu zu definieren. Zuletzt definieren wir mit theme_void(), dass wir die Karte ohne Koordinatensystem und Hintergrund abbilden möchten.↩︎\nMit filter() erstellen wir den Datensatz akten_anzubieten mit allen Akten, bei denen der Archvstatus anzubieten lautet.\n.drop = FALSE wurde hier verwendet, damit auch Bezirke mit 0 Akten im resultierenden Dataframe erhalten bleiben.↩︎\nWir nutzen hier unseren Datensatz akten . Mit der Funktion filter(Archivstatus == \"anzubieten\") wählen wir zunächst nur Zeilen aus die den Status anzubieten haben. Diese Daten gruppieren wir mit der group_by() Funktion nach den Gründen die in der Spalte Anbietungsgrund angeführt sind. Mit der Funktion summarise(Anzahl = n() zählen wir dann die Anzahl der Zeilen pro Gruppe.↩︎\nWir nutzen hier unseren Datensatz akten . Mit der Funktion filter(Archivstatus == \"anzubieten\") wählen wir zunächst nur Zeilen aus die den Status anzubieten haben. Diese Daten gruppieren wir mit der group_by() Funktion nach den Gründen die in der Spalte Anbietungsgrund (maniell erfasst) angeführt sind. Mit der Funktion summarise(Anzahl = n() zählen wir dann die Anzahl der Zeilen pro Gruppe.↩︎\nZur visuellen Darstellung der Daten nutzen wir wieder die Funktion ggplot(). Als Daten nutzen wir den Datensatz akten. In der Option aes() definieren wir, welche Spalte auf der x-Achse und welche Spalte auf der y-Achse angezeigt werden soll. Hier wählen wir die Spalte Streitwert in EURO für die x-Achse und die Spalte Anzahl Beteiligte für die y-Achse. Mit geom_point() geben wir an, dass die Daten mit Punkten im Koordinatensystem abgetragen werden sollen.↩︎\nZur visuellen Darstellung der Daten nutzen wir wieder die Funktion ggplot() als Daten nutzen wir den Datensatz akten in der Option aes() definieren wir, welche Spalte auf der x-Achse und welche Spalte auf der y-Achse angezeigt werden soll. Hier wählen wir die Spalte Dauer des Verfahrens in Tagen für die x-Achse und die Spalte Anzahl Beteiligte für die y-Achse. Mit geom_point() geben wir an, dass die Daten mit Punkten im Koordinatensystem abgetragen werden sollen.↩︎"
  },
  {
    "objectID": "workshops/ws1/WS1_Modul_4.html",
    "href": "workshops/ws1/WS1_Modul_4.html",
    "title": "Modul 4: Ergebnispräsentation und Umsetzung",
    "section": "",
    "text": "13.10.2023, 13:00 - 17:00 Uhr\n\nThemen\nIn Modul 4 besprechen wir Möglichkeiten, wie Ergebnisse in bestehende Prozesse eingebaut werden können. Sie können sich auf folgende Themen freuen:\n\nFallstudie: Identifikation von bekannten Persönlichkeiten\nMachine Learning: Können wir lernen “Was wird vermutlich in Zukunft von Interesse sein?”\nEinblick in den rechtlichen Rahmen von Datenanalyseprojekten\n\n\n\n\n\n\nDes weiteren präsentieren die Teams ihre Ergebnisse und Zukunftsvisionen.\n\n\n\n\n\n\nProjektvorstellung\n\n\n\nJedes Team präsentiert drei Folien:\n\nDie Ergebnisse der Datenanalyse\nEin Plan für die Umsetzung der vorgeschlagenen Analyse in die Praxis.\nEine Zukunftsvision: Was bräuchte man noch für das “perfekte” Archivierungs-System?\n\n\n\n\n\n\n\n\n\n\nLernziele\n\n\n\nAm Ende dieses Moduls können Sie…\n\n… externe Datenquellen hinsichtlich ihres Nutzens beurteilen.\n… die Möglichkeiten und Grenzen der Record Linkage Methode einschätzen.\n… Datenprojekte präsentieren und deren Nutzen einordnen.\n… einschätzen, was rechtlich in einem solchen Projekt beachtet werden muss.\n\n\n\n\n\nMaterial\n\nSlides",
    "crumbs": [
      "Module",
      "Modul 4: Ergebnispräsentation und Umsetzung"
    ]
  },
  {
    "objectID": "workflow.html#datenschutz-in-unseren-projekten",
    "href": "workflow.html#datenschutz-in-unseren-projekten",
    "title": "Arbeitsweise",
    "section": "Datenschutz in unseren Projekten",
    "text": "Datenschutz in unseren Projekten\nInhalt folgt in Kürze"
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "Arbeitsweise",
    "section": "",
    "text": "Inhalt folgt in Kürze"
  },
  {
    "objectID": "workflow.html#die-ada-timeline",
    "href": "workflow.html#die-ada-timeline",
    "title": "Arbeitsweise",
    "section": "",
    "text": "Inhalt folgt in Kürze"
  }
]